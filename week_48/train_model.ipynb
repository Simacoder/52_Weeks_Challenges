{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca067d95",
   "metadata": {},
   "source": [
    "No Libraries, No Shortcuts: LLM from Scratch with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "711034fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59592a",
   "metadata": {},
   "source": [
    "Self -Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14b9cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, attention_dim, bias=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_key = nn.Linear(embed_dim, attention_dim, bias=bias)\n",
    "        self.w_query = nn.Linear(embed_dim, attention_dim, bias=bias)\n",
    "        self.w_value = nn.Linear(embed_dim, attention_dim, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.size()\n",
    "        \"\"\"\n",
    "        [\n",
    "            [write vector],\n",
    "            [me vector],\n",
    "            [a vector],\n",
    "            [poem vector],\n",
    "        ]\n",
    "        \"\"\"\n",
    "\n",
    "        k = self.w_key(x)   # (B, T, A)\n",
    "        q = self.w_query(x) # (B, T, A)\n",
    "        v = self.w_value(x) # (B, T, A)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = (q @ k.transpose(-2, -1)) / (k.size(-1) ** 0.5)  # (B, T, T)\n",
    "\n",
    "        # Causal mask (future positions masked)\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
    "        scores = scores.masked_fill(mask, float('-1e10'))\n",
    "\n",
    "        attn = scores.softmax(dim=-1)  # (B, T, T)\n",
    "\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        return attn @ v  # (B, T, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb330013",
   "metadata": {},
   "source": [
    "multiple attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f77cc992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, attention_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.head_size = attention_dim//num_heads\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(SelfAttention(embed_dim=embed_dim, attention_dim=self.head_size,dropout=dropout))\n",
    "\n",
    "    def forward(self,x):\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head(x)) #B x T x A//num_heads\n",
    "        concatenated = torch.cat(head_outputs, dim = 2)\n",
    "        return concatenated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a5fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,attention_dim):\n",
    "        super().__init__()\n",
    "        self.up = nn.Linear(attention_dim,attention_dim*4)\n",
    "        self.relu = nn.GELU()\n",
    "        self.down = nn.Linear(attention_dim*4,attention_dim)\n",
    "    def forward(self,x):\n",
    "        return self.down(self.relu(self.up(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b83d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,num_heads,embed_dim,attention_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_multihead = MultiHeadAttention(num_heads, embed_dim, attention_dim, dropout)\n",
    "        self.feed_forward = FeedForward(attention_dim)\n",
    "        self.n1 = nn.LayerNorm(attention_dim)\n",
    "        self.n2 = nn.LayerNorm(attention_dim)\n",
    "    def forward(self,x):\n",
    "        e = self.masked_multihead(self.n1(x))\n",
    "        e =  e + x\n",
    "        e = self.feed_forward(self.n2(e))\n",
    "        return e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bc13de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, num_heads, vocab_size, embed_dim, attention_dim, num_blocks, context_length, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, attention_dim)\n",
    "        self.positional_embedding = nn.Embedding(context_length, attention_dim)\n",
    "\n",
    "        self.decoders = nn.ModuleList([\n",
    "            Decoder(num_heads, attention_dim, attention_dim, dropout_rate) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.exit_norm = nn.LayerNorm(attention_dim)\n",
    "        self.linear = nn.Linear(attention_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        embeddings = self.embedding(context)\n",
    "        context_len = context.shape[1]\n",
    "        position = torch.arange(context_len, device=context.device).unsqueeze(0)\n",
    "        position_embeddings = self.positional_embedding(position)\n",
    "\n",
    "        e = embeddings + position_embeddings\n",
    "\n",
    "        for decoder in self.decoders:\n",
    "            e = decoder(e)\n",
    "\n",
    "        return self.linear(self.exit_norm(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fb092c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: '*', 11: '+', 12: ',', 13: '-', 14: '.', 15: '/', 16: '0', 17: '1', 18: '2', 19: '3', 20: '4', 21: '5', 22: '6', 23: '7', 24: '8', 25: '9', 26: ':', 27: ';', 28: '?', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '{', 85: '|', 86: '}', 87: 'à', 88: 'á', 89: 'è', 90: 'é', 91: 'ë', 92: 'ñ', 93: 'ó', 94: 'ú', 95: '\\u2005', 96: '–', 97: '—', 98: '‘', 99: '’', 100: '“', 101: '”', 102: '…', 103: '\\u205f'}\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = float('-inf')\n",
    "    return out\n",
    "\n",
    "\n",
    "def generate_text(model, new_chars, context, context_length, int_to_char, temperature=1.0, top_k=None):\n",
    "    res = []\n",
    "    for _ in range(new_chars):\n",
    "        if context.shape[1] > context_length:\n",
    "            context = context[:, -context_length:]\n",
    "\n",
    "        logits = model(context)  # [B, T, V]\n",
    "        logits = logits[:, -1, :]  # [B, V]\n",
    "        logits = logits / max(temperature, 1e-3)\n",
    "\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "\n",
    "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "            raise ValueError(\"Logits contain NaN or Inf\")\n",
    "\n",
    "        probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "        probabilities = torch.clamp(probabilities, min=1e-9, max=1.0)\n",
    "\n",
    "        next_token = torch.multinomial(probabilities, 1)  # [B, 1]\n",
    "        context = torch.cat((context, next_token), dim=1)\n",
    "        res.append(int_to_char[next_token.item()])\n",
    "\n",
    "    return ''.join(res)\n",
    "\n",
    "def generate(model, max_new_tokens, context, context_length, temperature=1.0, top_k=None):\n",
    "    res = []\n",
    "    for _ in range(max_new_tokens):\n",
    "        if context.shape[1] > context_length:\n",
    "            context = context[:, -context_length:]\n",
    "\n",
    "        logits = model(context)  # [B, T, V]\n",
    "        logits = logits[:, -1, :]  # [B, V]\n",
    "        logits = logits / max(temperature, 1e-3)\n",
    "\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "\n",
    "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "            raise ValueError(\"Logits contain NaN or Inf\")\n",
    "\n",
    "        probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "        probabilities = torch.clamp(probabilities, min=1e-9, max=1.0)\n",
    "\n",
    "        next_token = torch.multinomial(probabilities, 1)  # [B, 1]\n",
    "        context = torch.cat((context, next_token), dim=1)\n",
    "\n",
    "\n",
    "    return context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3639633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = 104             # Number of tokens in your vocabulary\n",
    "# context_length = 128         # Maximum sequence length (tokens)\n",
    "# embed_dim = 256              # Token embedding size\n",
    "# attention_dim = 256          # Attention projection dimension (keep same as embed_dim)\n",
    "# num_heads = 4                # Number of attention heads (must divide attention_dim)\n",
    "# num_blocks = 6               # Number of decoder blocks (layers)\n",
    "# num_words = 5000\n",
    "# dropout_rate=0.1           # Number of new tokens to generate (optional tuning)\n",
    "\n",
    "# # Initial context (starting token or BOS)\n",
    "# context = torch.zeros(1, 1, dtype=torch.int64).to(device)\n",
    "\n",
    "# model = GPT(num_heads,vocab_size,embed_dim,attention_dim,num_blocks,context_length,dropout_rate).to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037b655",
   "metadata": {},
   "source": [
    "Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732a822",
   "metadata": {},
   "source": [
    "Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab4dd762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (4.4.2)\n",
      "Requirement already satisfied: filelock in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from datasets) (2.3.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f18035b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  100k  100  100k    0     0  84542      0  0:00:01  0:00:01 --:--:-- 84690\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_idx</th>\n",
       "      <th>row</th>\n",
       "      <th>truncated_cells</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{'text': 'Ive been reading books of old\n",
       "The le...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{'text': 'Come up to meet you, tell you Im sor...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>{'text': 'I used to rule the world\n",
       "Seas would ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>{'text': 'When you try your best, but you dont...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>{'text': 'Look at the stars\n",
       "Look how they shin...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_idx                                                row truncated_cells\n",
       "0        0  {'text': 'Ive been reading books of old\n",
       "The le...              []\n",
       "1        1  {'text': 'Come up to meet you, tell you Im sor...              []\n",
       "2        2  {'text': 'I used to rule the world\n",
       "Seas would ...              []\n",
       "3        3  {'text': 'When you try your best, but you dont...              []\n",
       "4        4  {'text': 'Look at the stars\n",
       "Look how they shin...              []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!curl -X GET \"https://datasets-server.huggingface.co/rows?dataset=huggingartists%2Fcoldplay&config=default&split=train&offset=0&length=100\" -o coldplay_data.json\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('coldplay_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data['rows'])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5227496c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ive been reading books of old\\nThe legends and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Come up to meet you, tell you Im sorry\\nYou do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I used to rule the world\\nSeas would rise when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>When you try your best, but you dont succeed\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Look at the stars\\nLook how they shine for you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_idx                                               text\n",
       "0        0  Ive been reading books of old\\nThe legends and...\n",
       "1        1  Come up to meet you, tell you Im sorry\\nYou do...\n",
       "2        2  I used to rule the world\\nSeas would rise when...\n",
       "3        3  When you try your best, but you dont succeed\\n...\n",
       "4        4  Look at the stars\\nLook how they shine for you..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.drop(columns=['truncated_cells'])\n",
    "df['text'] = df['row'].apply(lambda x: x['text'])\n",
    "df = df.drop(columns=['row'])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d890377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Select subsets\n",
    "train_subset = df.iloc[:90]\n",
    "test_subset = df.iloc[90:]\n",
    "\n",
    "# Extract and clean lyrics\n",
    "def keep_english_only(text):\n",
    "    return re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "\n",
    "# Process training lyrics\n",
    "train_lyrics = [keep_english_only(row[\"text\"]) for index, row in train_subset.iterrows()]\n",
    "joined_train_lyrics = '\\n'.join(train_lyrics)\n",
    "\n",
    "# Process test lyrics\n",
    "test_lyrics = [keep_english_only(row[\"text\"]) for index, row in test_subset.iterrows()]\n",
    "joined_test_lyrics = '\\n'.join(test_lyrics)\n",
    "\n",
    "full_lyrics = joined_train_lyrics + '\\n' + joined_test_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c317cfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ive been reading books of old\\nThe legends and the myths\\nAchilles and his gold\\nHercules and his gifts\\nSpider-Mans control\\nAnd Batman with his fists\\nAnd clearly I dont see myself upon that list\\nBut she '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_lyrics[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e391f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = list(set(full_lyrics))\n",
    "len(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535bbdda",
   "metadata": {},
   "source": [
    "Stanford IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2c43617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train text length: 13090602 characters\n",
      "Test text length: 12929994 characters\n",
      "\n",
      "Train preview: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really h\n",
      "\n",
      "Test preview: I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, C\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "# Function to keep only English (ASCII) characters\n",
    "def keep_english_only(text):\n",
    "    return re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "\n",
    "# Function to clean and combine a list of texts\n",
    "def combine_and_clean(text_list):\n",
    "    # Keep only English\n",
    "    cleaned_list = [keep_english_only(t) for t in text_list]\n",
    "    # Combine into one string\n",
    "    combined = \" \".join(cleaned_list)\n",
    "    # Remove extra spaces/newlines\n",
    "    combined = re.sub(r'\\s+', ' ', combined).strip()\n",
    "    return combined\n",
    "\n",
    "# Create separate combined strings\n",
    "train_text_data = combine_and_clean(ds['train']['text'][:10000])\n",
    "test_text_data = combine_and_clean(ds['test']['text'][:10000])\n",
    "\n",
    "print(f\"Train text length: {len(train_text_data)} characters\")\n",
    "print(f\"Test text length: {len(test_text_data)} characters\")\n",
    "\n",
    "# Preview first 300 chars from each\n",
    "print(\"\\nTrain preview:\", train_text_data[:300])\n",
    "print(\"\\nTest preview:\", test_text_data[:300])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa4d652",
   "metadata": {},
   "source": [
    "Shakespeare Karpathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9be42250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import re\n",
    "\n",
    "# # Load dataset\n",
    "# ds = load_dataset(\"SamuelYang/bookcorpus\")\n",
    "\n",
    "# # Function to keep only English (ASCII) characters\n",
    "# def keep_english_only(text):\n",
    "#     return re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "\n",
    "# # Function to clean and combine a list of texts\n",
    "# def combine_and_clean(text_list):\n",
    "#     # Keep only English\n",
    "#     cleaned_list = [keep_english_only(t) for t in text_list]\n",
    "#     # Combine into one string\n",
    "#     combined = \" \".join(cleaned_list)\n",
    "#     # Remove extra spaces/newlines\n",
    "#     combined = re.sub(r'\\s+', ' ', combined).strip()\n",
    "#     return combined\n",
    "\n",
    "# # Since bookcorpus has only a train split, create synthetic validation and test splits\n",
    "# # Use 80% for train, 10% for validation, 10% for test\n",
    "# train_size = int(0.8 * len(ds['train']['text']))\n",
    "# val_size = int(0.1 * len(ds['train']['text']))\n",
    "# train_text = ds['train']['text'][:train_size]\n",
    "# validation_text = ds['train']['text'][train_size:train_size + val_size]\n",
    "# test_text = ds['train']['text'][train_size + val_size:train_size + 2 * val_size]\n",
    "\n",
    "# # Create combined strings for train, validation, and test\n",
    "# train_text_data = combine_and_clean(train_text)\n",
    "# validation_text_data = combine_and_clean(validation_text)\n",
    "# test_text_data = combine_and_clean(test_text)\n",
    "\n",
    "# # Print lengths\n",
    "# print(f\"Train text length: {len(train_text_data)} characters\")\n",
    "# print(f\"Validation text length: {len(validation_text_data)} characters\")\n",
    "# print(f\"Test text length: {len(test_text_data)} characters\")\n",
    "\n",
    "# # Preview first 300 chars from each\n",
    "# print(\"\\nTrain preview:\", train_text_data[:300])\n",
    "# print(\"\\nValidation preview:\", validation_text_data[:300])\n",
    "# print(\"\\nTest preview:\", test_text_data[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94b765",
   "metadata": {},
   "source": [
    "Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ffc1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "\n",
    "# # Alternatively:\n",
    "# # from llms_from_scratch.ch04 import generate_text_simple\n",
    "\n",
    "# def text_to_token_ids(text, tokenizer, device):\n",
    "#     encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "#     encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(device, non_blocking=True) # add batch dimension and move to device\n",
    "#     return encoded_tensor\n",
    "\n",
    "# def token_ids_to_text(token_ids, tokenizer):\n",
    "#     flat = token_ids.squeeze(0) # remove batch dimension\n",
    "#     return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "# start_context = \"I want something\"\n",
    "# tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6e959",
   "metadata": {},
   "source": [
    "Custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e5dcc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (0.12.0)\n",
      "Requirement already satisfied: transformers in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (4.57.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: filelock in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61eae432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_training_corpus(batch_size=1000):\n",
    "    \"\"\"\n",
    "    Yields batches of raw IMDB reviews + Coldplay lyrics\n",
    "    for tokenizer training.\n",
    "    \"\"\"\n",
    "    # Combine IMDB train + test + Coldplay lyrics\n",
    "    texts = ds[\"train\"][\"text\"][:] + ds[\"test\"][\"text\"][:] + \\\n",
    "            [joined_train_lyrics, joined_test_lyrics]\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        yield [keep_english_only(t) for t in batch]  # clean before yielding\n",
    "\n",
    "\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", local_files_only=True)\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c043f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def text_to_token_ids(text, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Convert raw text → token IDs tensor (batch size = 1).\n",
    "    \"\"\"\n",
    "    encoded = tokenizer.encode(text, add_special_tokens=False)\n",
    "    encoded_tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0)  # (1, T)\n",
    "    return encoded_tensor.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert token IDs tensor → decoded text.\n",
    "    \"\"\"\n",
    "    flat = token_ids.squeeze(0).tolist()  # remove batch dimension\n",
    "    return tokenizer.decode(flat, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f31dc9",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36b52eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embedding): Embedding(5000, 128)\n",
       "  (positional_embedding): Embedding(128, 128)\n",
       "  (decoders): ModuleList(\n",
       "    (0-3): 4 x Decoder(\n",
       "      (masked_multihead): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-1): 2 x SelfAttention(\n",
       "            (w_key): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (w_query): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (w_value): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (up): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (relu): GELU(approximate='none')\n",
       "        (down): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (n1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (n2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (exit_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear): Linear(in_features=128, out_features=5000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size           # GPT-2 tokenizer vocab size\n",
    "context_length = 128                     # Reduced context length for faster training\n",
    "embed_dim = 128                        # Smaller embedding dimension\n",
    "attention_dim = 128                     # Keep same as embed_dim\n",
    "num_heads = 2                           # Divisible by attention_dim\n",
    "num_blocks = 4                          # Fewer blocks for smaller model\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Initial context (starting token or BOS)\n",
    "context = torch.zeros(1, 1, dtype=torch.int64).to(device, non_blocking=True)\n",
    "\n",
    "model = GPT(num_heads,vocab_size,embed_dim,attention_dim,num_blocks,context_length, dropout_rate).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92544513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Once upon a time in hollywood, among running ann ele revenge industryric Thoughton One\n"
     ]
    }
   ],
   "source": [
    "start_context=\"Once upon a time in hollywood,\"\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    context=text_to_token_ids(start_context, tokenizer, device),\n",
    "    max_new_tokens=10,\n",
    "    context_length=context_length\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407d51c",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "821c7486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embedding): Embedding(5000, 128)\n",
       "  (positional_embedding): Embedding(128, 128)\n",
       "  (decoders): ModuleList(\n",
       "    (0-3): 4 x Decoder(\n",
       "      (masked_multihead): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-1): 2 x SelfAttention(\n",
       "            (w_key): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (w_query): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (w_value): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (up): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (relu): GELU(approximate='none')\n",
       "        (down): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (n1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (n2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (exit_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear): Linear(in_features=128, out_features=5000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        torch.nn.init.ones_(module.weight)\n",
    "        torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "# Apply initialization\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "483e1f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3467525 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 13090602\n",
      "Tokens: 3467525\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(train_text_data)\n",
    "total_tokens = len(tokenizer.encode(train_text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)\n",
    "\n",
    "# Sanity check\n",
    "\n",
    "if total_tokens * (0.95) < context_length:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the context_length or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-0.95) <context_length:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the context_length or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac212cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "\n",
    "\n",
    "        # Use a sliding window to chunk the data into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_encoded_dataloader(txt, tokenizer, batch_size=4, max_length=128,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = CustomDataset(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8968ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(model, new_chars, context, context_length, temperature=1.0, top_k=None):\n",
    "#     res = []\n",
    "#     for _ in range(new_chars):\n",
    "#         if context.shape[1] > context_length:\n",
    "#             context = context[:, -context_length:]\n",
    "\n",
    "#         logits = model(context)  # [B, T, V]\n",
    "#         logits = logits[:, -1, :]  # [B, V]\n",
    "#         logits = logits / max(temperature, 1e-3)\n",
    "\n",
    "#         if top_k is not None:\n",
    "#             logits = top_k_logits(logits, top_k)\n",
    "\n",
    "#         if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "#             raise ValueError(\"Logits contain NaN or Inf\")\n",
    "\n",
    "#         probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "#         probabilities = torch.clamp(probabilities, min=1e-9, max=1.0)\n",
    "\n",
    "#         next_token = torch.multinomial(probabilities, 1)  # [B, 1]\n",
    "#         context = torch.cat((context, next_token), dim=1)\n",
    "#         res.append(next_token.item())\n",
    "\n",
    "#     return tokenizer.decode(res)\n",
    "\n",
    "# # Function to compute gradient norm and max gradient\n",
    "# def compute_gradient_stats(model):\n",
    "#     total_norm = 0.0\n",
    "#     max_grad = 0.0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is not None:\n",
    "#             param_norm = p.grad.data.norm(2).item()\n",
    "#             total_norm += param_norm ** 2\n",
    "#             max_grad = max(max_grad, p.grad.data.abs().max().item())\n",
    "#     total_norm = total_norm ** 0.5\n",
    "#     return total_norm, max_grad\n",
    "\n",
    "# # Loss calculation function for a single batch\n",
    "# def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "#     input_batch = input_batch.to(device, non_blocking=True)\n",
    "#     target_batch = target_batch.to(device, non_blocking=True)\n",
    "\n",
    "#     logits = model(input_batch)\n",
    "#     loss = torch.nn.functional.cross_entropy(\n",
    "#         logits.view(-1, vocab_size), target_batch.view(-1), ignore_index=-100\n",
    "#     )\n",
    "#     return loss\n",
    "\n",
    "# # Training loop for one epoch\n",
    "# def train_epoch(model, dataloader, optimizer, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     total_grad_norm = 0\n",
    "#     total_max_grad = 0\n",
    "#     num_batches = len(dataloader)\n",
    "\n",
    "#     for batch_idx, batch in enumerate(dataloader):\n",
    "#         loss = calc_loss_batch(batch[0], batch[1], model, device)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Compute gradient statistics\n",
    "#         grad_norm, max_grad = compute_gradient_stats(model)\n",
    "#         total_grad_norm += grad_norm\n",
    "#         total_max_grad = max(total_max_grad, max_grad)\n",
    "\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Print batch-level stats every 100 batches for debugging\n",
    "#         if (batch_idx + 1) % 100 == 0:\n",
    "#             print(f\"Batch {batch_idx + 1}/{num_batches}: \"\n",
    "#                   f\"Loss = {loss.item():.4f}, \"\n",
    "#                   f\"Grad Norm = {grad_norm:.4f}, \"\n",
    "#                   f\"Max Grad = {max_grad:.4f}\")\n",
    "\n",
    "#     avg_loss = total_loss / num_batches\n",
    "#     avg_grad_norm = total_grad_norm / num_batches\n",
    "#     avg_max_grad = total_max_grad / num_batches\n",
    "#     return avg_loss, avg_grad_norm, avg_max_grad\n",
    "\n",
    "# # Evaluation loop for one epoch\n",
    "# def eval_epoch(model, dataloader, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     num_batches = len(dataloader)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "#             loss = calc_loss_batch(batch[0], batch[1], model, device)\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#     return total_loss / num_batches\n",
    "\n",
    "# # Main training function with early stopping\n",
    "# def main(settings):\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=settings[\"learning_rate\"], weight_decay=settings[\"weight_decay\"], betas=(0.9, 0.95))\n",
    "\n",
    "#     best_val_loss = float('inf')\n",
    "#     patience_counter = 0\n",
    "#     best_model_path = \"gpt_model_best.pth\"\n",
    "\n",
    "#     for epoch in range(settings[\"num_epochs\"]):\n",
    "#         train_loss, train_grad_norm, train_max_grad = train_epoch(model, train_dataloader, optimizer, device)\n",
    "#         val_loss = eval_epoch(model, test_dataloader, device)\n",
    "\n",
    "#         if (epoch + 1) % settings[\"print_interval\"] == 0:\n",
    "#             print(f\"Epoch {epoch + 1}/{settings['num_epochs']}: \"\n",
    "#                   f\"Train Loss = {train_loss:.4f}, \"\n",
    "#                   f\"Val Loss = {val_loss:.4f}, \"\n",
    "#                   f\"Avg Grad Norm = {train_grad_norm:.4f}, \"\n",
    "#                   f\"Max Grad = {train_max_grad:.4f}, \"\n",
    "#                   f\"Learning Rate = {settings['learning_rate']:.6f}\")\n",
    "\n",
    "#         # Early stopping check\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "#             patience_counter = 0\n",
    "#             torch.save(model.state_dict(), best_model_path)\n",
    "#             print(f\"Saved best model with validation loss {best_val_loss:.4f}\")\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "#             print(f\"No improvement in validation loss. Patience counter: {patience_counter}/{settings['patience']}\")\n",
    "#             if patience_counter >= settings[\"patience\"]:\n",
    "#                 print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "#                 break\n",
    "\n",
    "#         if (epoch + 1) % settings[\"generate_interval\"] == 0:\n",
    "#             start_context = \"This is a test generation: \"\n",
    "#             context = text_to_token_ids(start_context, tokenizer, device)\n",
    "#             generated = generate_text(model, 100, context, context_length, temperature=0.7)\n",
    "#             print(f\"Generated text after epoch {epoch + 1}:\\n{start_context + generated}\\n\")\n",
    "\n",
    "#     # Load the best model\n",
    "#     model.load_state_dict(torch.load(best_model_path))\n",
    "#     print(f\"Training completed. Best model loaded from '{best_model_path}' with validation loss {best_val_loss:.4f}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7237bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# ---------- Loss helpers ----------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device, non_blocking=True)\n",
    "    target_batch = target_batch.to(device, non_blocking=True)\n",
    "\n",
    "    logits = model(input_batch)  # [B, T, V]\n",
    "    B, T, V = logits.shape\n",
    "    loss = criterion(logits.view(B * T, V), target_batch.view(B * T))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(data_loader) if num_batches is None else min(num_batches, len(data_loader))\n",
    "\n",
    "    for i, (inp, tgt) in enumerate(data_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        loss = calc_loss_batch(inp, tgt, model, device)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.train()\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter=1):\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "    val_loss   = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "# ---------- Cosine LR with Warmup ----------\n",
    "class CosineWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, base_lr, min_lr, last_epoch=-1):\n",
    "        self.warmup_steps = max(1, warmup_steps)\n",
    "        self.total_steps = max(self.warmup_steps + 1, total_steps)\n",
    "        self.base_lr = base_lr\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch + 1\n",
    "        lrs = []\n",
    "        for _ in self.base_lrs:\n",
    "            if step <= self.warmup_steps:\n",
    "                lr = self.base_lr * step / self.warmup_steps\n",
    "            else:\n",
    "                progress = (step - self.warmup_steps) / max(1, self.total_steps - self.warmup_steps)\n",
    "                lr = self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + math.cos(math.pi * progress))\n",
    "            lrs.append(lr)\n",
    "        return lrs\n",
    "\n",
    "\n",
    "# ---------- Training Loop ----------\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    settings,\n",
    "    save_path=\"checkpoints/gpt_256_256_8_8.pt\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Pure FP32 training loop — no autocast, no GradScaler.\n",
    "    \"\"\"\n",
    "\n",
    "    # Seeding\n",
    "    torch.manual_seed(123)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(123)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=settings[\"learning_rate\"],\n",
    "        weight_decay=settings[\"weight_decay\"],\n",
    "        betas=(0.9, 0.95),\n",
    "    )\n",
    "\n",
    "    total_steps = settings[\"num_epochs\"] * len(train_loader)\n",
    "    scheduler = CosineWithWarmup(\n",
    "        optimizer,\n",
    "        warmup_steps=settings[\"warmup_steps\"],\n",
    "        total_steps=total_steps,\n",
    "        base_lr=settings[\"max_lr\"],\n",
    "        min_lr=settings[\"min_lr\"],\n",
    "    )\n",
    "\n",
    "    train_losses, val_losses, tokens_seen_track = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    best_val_loss, patience_counter = float(\"inf\"), 0\n",
    "\n",
    "    for epoch in range(settings[\"num_epochs\"]):\n",
    "        model.train()  # ensure training mode at start of each epoch\n",
    "        for step, (inp, tgt) in enumerate(train_loader):\n",
    "            loss = calc_loss_batch(inp, tgt, model, device)\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), settings[\"gradient_clip\"])\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "            tokens_seen += inp.numel()\n",
    "\n",
    "            # evaluation\n",
    "            if global_step % settings[\"eval_freq\"] == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device,\n",
    "                    eval_iter=settings[\"eval_iter\"],\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                tokens_seen_track.append(tokens_seen)\n",
    "                lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "                print(f\"Ep {epoch+1} | step {global_step:06d} | lr {lr_now:.3e} \"\n",
    "                      f\"| train {train_loss:.3f} | val {val_loss:.3f}\")\n",
    "\n",
    "                # early stopping\n",
    "                if val_loss + settings[\"min_improvement\"] < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    os.makedirs(os.path.dirname(save_path) or \".\", exist_ok=True)\n",
    "                    torch.save({\n",
    "                        \"model_state\": model.state_dict(),\n",
    "                        \"optimizer_state\": optimizer.state_dict(),\n",
    "                        \"epoch\": epoch,\n",
    "                        \"global_step\": global_step,\n",
    "                    }, save_path)\n",
    "                    print(f\"[Checkpoint saved at step {global_step}]\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= settings[\"patience\"]:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        return train_losses, val_losses, tokens_seen_track\n",
    "\n",
    "    return train_losses, val_losses, tokens_seen_track\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4c8b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from torch import nn\n",
    "\n",
    "# # ---------- Loss helpers ----------\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "#     input_batch = input_batch.to(device, non_blocking=True)\n",
    "#     target_batch = target_batch.to(device, non_blocking=True)\n",
    "\n",
    "#     logits = model(input_batch)  # [B, T, V]\n",
    "#     B, T, V = logits.shape\n",
    "#     loss = criterion(logits.view(B * T, V), target_batch.view(B * T))\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "#     if len(data_loader) == 0:\n",
    "#         return float(\"nan\")\n",
    "\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     num_batches = len(data_loader) if num_batches is None else min(num_batches, len(data_loader))\n",
    "\n",
    "#     for i, (inp, tgt) in enumerate(data_loader):\n",
    "#         if i >= num_batches:\n",
    "#             break\n",
    "#         loss = calc_loss_batch(inp, tgt, model, device)\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     model.train()\n",
    "#     return total_loss / num_batches\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def evaluate_model(model, train_loader, val_loader, device, eval_iter=1):\n",
    "#     train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "#     val_loss   = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "#     return train_loss, val_loss\n",
    "\n",
    "\n",
    "# # ---------- Training Loop ----------\n",
    "# def train_model(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     device,\n",
    "#     settings,\n",
    "#     save_path=\"checkpoints/gpt_256_256_8_8.pt\",\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Pure FP32 training loop — no autocast, no GradScaler.\n",
    "#     Uses CosineAnnealingWarmRestarts scheduler for periodic LR resets.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Seeding\n",
    "#     torch.manual_seed(123)\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.manual_seed_all(123)\n",
    "\n",
    "#     model.to(device)\n",
    "\n",
    "#     optimizer = torch.optim.AdamW(\n",
    "#         model.parameters(),\n",
    "#         lr=settings[\"max_lr\"],  # start with max LR\n",
    "#         weight_decay=settings[\"weight_decay\"],\n",
    "#         betas=(0.9, 0.95),\n",
    "#     )\n",
    "\n",
    "#     # CosineAnnealingWarmRestarts:\n",
    "#     # T_0 = number of epochs before the first restart\n",
    "#     # T_mult = factor to increase T_i after a restart\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "#         optimizer,\n",
    "#         T_0=settings.get(\"restart_period\", 10),   # default: restart every 10 epochs\n",
    "#         T_mult=settings.get(\"restart_mult\", 2),   # how restart period grows\n",
    "#         eta_min=settings[\"min_lr\"],               # minimum LR\n",
    "#     )\n",
    "\n",
    "#     train_losses, val_losses, tokens_seen_track = [], [], []\n",
    "#     tokens_seen, global_step = 0, -1\n",
    "#     best_val_loss, patience_counter = float(\"inf\"), 0\n",
    "\n",
    "#     for epoch in range(settings[\"num_epochs\"]):\n",
    "#         model.train()  # ensure training mode at start of each epoch\n",
    "#         for step, (inp, tgt) in enumerate(train_loader):\n",
    "#             loss = calc_loss_batch(inp, tgt, model, device)\n",
    "#             loss.backward()\n",
    "\n",
    "#             # gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), settings[\"gradient_clip\"])\n",
    "\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "#             # step scheduler with fractional epoch progress\n",
    "#             scheduler.step(epoch + step / len(train_loader))\n",
    "\n",
    "#             global_step += 1\n",
    "#             tokens_seen += inp.numel()\n",
    "\n",
    "#             # evaluation\n",
    "#             if global_step % settings[\"eval_freq\"] == 0:\n",
    "#                 train_loss, val_loss = evaluate_model(\n",
    "#                     model, train_loader, val_loader, device,\n",
    "#                     eval_iter=settings[\"eval_iter\"],\n",
    "#                 )\n",
    "#                 train_losses.append(train_loss)\n",
    "#                 val_losses.append(val_loss)\n",
    "#                 tokens_seen_track.append(tokens_seen)\n",
    "#                 lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "#                 print(f\"Ep {epoch+1} | step {global_step:06d} | lr {lr_now:.3e} \"\n",
    "#                       f\"| train {train_loss:.3f} | val {val_loss:.3f}\")\n",
    "\n",
    "#                 # early stopping\n",
    "#                 if val_loss + settings[\"min_improvement\"] < best_val_loss:\n",
    "#                     best_val_loss = val_loss\n",
    "#                     patience_counter = 0\n",
    "#                     os.makedirs(os.path.dirname(save_path) or \".\", exist_ok=True)\n",
    "#                     torch.save({\n",
    "#                         \"model_state\": model.state_dict(),\n",
    "#                         \"optimizer_state\": optimizer.state_dict(),\n",
    "#                         \"epoch\": epoch,\n",
    "#                         \"global_step\": global_step,\n",
    "#                     }, save_path)\n",
    "#                     print(f\"[Checkpoint saved at step {global_step}]\")\n",
    "#                 else:\n",
    "#                     patience_counter += 1\n",
    "#                     if patience_counter >= settings[\"patience\"]:\n",
    "#                         print(\"Early stopping triggered.\")\n",
    "#                         return train_losses, val_losses, tokens_seen_track\n",
    "\n",
    "#     return train_losses, val_losses, tokens_seen_track\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f3943b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in training dataloader: 846\n",
      "Number of batches in validation dataloader: 837\n",
      "Train loader:\n",
      "torch.Size([32, 128]) torch.Size([32, 128])\n",
      "\n",
      "test loader:\n",
      "torch.Size([32, 128]) torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "settings = {\n",
    "    \"learning_rate\": 3e-4,          # Slightly higher start (AdamW with warmup will handle it)\n",
    "    \"weight_decay\": 0.1,            # Standard for GPT-style training\n",
    "    \"num_epochs\": 30,               # Enough for IMDb text size (~40M tokens) with small model\n",
    "    \"batch_size\": 32,               # Balanced for GPU memory vs convergence\n",
    "    \"warmup_steps\": 1500,            # Warmup helps avoid divergence early\n",
    "    \"max_lr\": 3e-4,                 # Same as base LR\n",
    "    \"min_lr\": 3e-5,                 # Don't decay too low (keeps learning alive)\n",
    "    \"eval_freq\": 200,               # Every ~200 steps = less overhead\n",
    "    \"eval_iter\": 20,                # Average over more batches for stability\n",
    "    \"gradient_clip\": 1.0,           # Safe norm clipping\n",
    "    \"patience\": 50,                 # Stop early if val loss stagnates\n",
    "    \"min_improvement\": 1e-4,\n",
    "    \"print_interval\": 1,            # Print progress every epoch\n",
    "    \"generate_interval\": 5          # Require meaningful improvement\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create training and validation dataloaders\n",
    "train_dataloader = create_encoded_dataloader(\n",
    "    train_text_data,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=settings[\"batch_size\"],\n",
    "    max_length=context_length,\n",
    "    stride=context_length,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_dataloader = create_encoded_dataloader(\n",
    "    test_text_data,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=settings[\"batch_size\"],\n",
    "    max_length=context_length,\n",
    "    stride=context_length,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in training dataloader: {len(train_dataloader)}\")\n",
    "print(f\"Number of batches in validation dataloader: {len(test_dataloader)}\")\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "\n",
    "print(\"\\ntest loader:\")\n",
    "for x, y in test_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27d3d2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 | step 000000 | lr 4.000e-07 | train 8.343 | val 8.343\n",
      "[Checkpoint saved at step 0]\n",
      "Ep 1 | step 000200 | lr 4.040e-05 | train 7.881 | val 7.884\n",
      "[Checkpoint saved at step 200]\n",
      "Ep 1 | step 000400 | lr 8.040e-05 | train 7.061 | val 7.062\n",
      "[Checkpoint saved at step 400]\n",
      "Ep 1 | step 000600 | lr 1.204e-04 | train 6.606 | val 6.641\n",
      "[Checkpoint saved at step 600]\n",
      "Ep 1 | step 000800 | lr 1.604e-04 | train 6.136 | val 6.201\n",
      "[Checkpoint saved at step 800]\n",
      "Ep 2 | step 001000 | lr 2.004e-04 | train 5.834 | val 5.893\n",
      "[Checkpoint saved at step 1000]\n",
      "Ep 2 | step 001200 | lr 2.404e-04 | train 5.666 | val 5.725\n",
      "[Checkpoint saved at step 1200]\n",
      "Ep 2 | step 001400 | lr 2.804e-04 | train 5.550 | val 5.619\n",
      "[Checkpoint saved at step 1400]\n",
      "Ep 2 | step 001600 | lr 3.000e-04 | train 5.447 | val 5.535\n",
      "[Checkpoint saved at step 1600]\n",
      "Ep 3 | step 001800 | lr 2.999e-04 | train 5.374 | val 5.450\n",
      "[Checkpoint saved at step 1800]\n",
      "Ep 3 | step 002000 | lr 2.997e-04 | train 5.337 | val 5.386\n",
      "[Checkpoint saved at step 2000]\n",
      "Ep 3 | step 002200 | lr 2.994e-04 | train 5.247 | val 5.319\n",
      "[Checkpoint saved at step 2200]\n",
      "Ep 3 | step 002400 | lr 2.991e-04 | train 5.178 | val 5.259\n",
      "[Checkpoint saved at step 2400]\n",
      "Ep 4 | step 002600 | lr 2.986e-04 | train 5.131 | val 5.213\n",
      "[Checkpoint saved at step 2600]\n",
      "Ep 4 | step 002800 | lr 2.980e-04 | train 5.061 | val 5.161\n",
      "[Checkpoint saved at step 2800]\n",
      "Ep 4 | step 003000 | lr 2.974e-04 | train 5.026 | val 5.114\n",
      "[Checkpoint saved at step 3000]\n",
      "Ep 4 | step 003200 | lr 2.966e-04 | train 5.022 | val 5.081\n",
      "[Checkpoint saved at step 3200]\n",
      "Ep 5 | step 003400 | lr 2.958e-04 | train 4.933 | val 5.038\n",
      "[Checkpoint saved at step 3400]\n",
      "Ep 5 | step 003600 | lr 2.949e-04 | train 4.885 | val 5.003\n",
      "[Checkpoint saved at step 3600]\n",
      "Ep 5 | step 003800 | lr 2.939e-04 | train 4.826 | val 4.971\n",
      "[Checkpoint saved at step 3800]\n",
      "Ep 5 | step 004000 | lr 2.928e-04 | train 4.819 | val 4.943\n",
      "[Checkpoint saved at step 4000]\n",
      "Ep 5 | step 004200 | lr 2.916e-04 | train 4.825 | val 4.908\n",
      "[Checkpoint saved at step 4200]\n",
      "Ep 6 | step 004400 | lr 2.903e-04 | train 4.771 | val 4.882\n",
      "[Checkpoint saved at step 4400]\n",
      "Ep 6 | step 004600 | lr 2.889e-04 | train 4.746 | val 4.853\n",
      "[Checkpoint saved at step 4600]\n",
      "Ep 6 | step 004800 | lr 2.875e-04 | train 4.676 | val 4.822\n",
      "[Checkpoint saved at step 4800]\n",
      "Ep 6 | step 005000 | lr 2.859e-04 | train 4.708 | val 4.807\n",
      "[Checkpoint saved at step 5000]\n",
      "Ep 7 | step 005200 | lr 2.843e-04 | train 4.678 | val 4.788\n",
      "[Checkpoint saved at step 5200]\n",
      "Ep 7 | step 005400 | lr 2.826e-04 | train 4.639 | val 4.768\n",
      "[Checkpoint saved at step 5400]\n",
      "Ep 7 | step 005600 | lr 2.808e-04 | train 4.611 | val 4.750\n",
      "[Checkpoint saved at step 5600]\n",
      "Ep 7 | step 005800 | lr 2.789e-04 | train 4.581 | val 4.727\n",
      "[Checkpoint saved at step 5800]\n",
      "Ep 8 | step 006000 | lr 2.770e-04 | train 4.577 | val 4.709\n",
      "[Checkpoint saved at step 6000]\n",
      "Ep 8 | step 006200 | lr 2.750e-04 | train 4.565 | val 4.702\n",
      "[Checkpoint saved at step 6200]\n",
      "Ep 8 | step 006400 | lr 2.729e-04 | train 4.526 | val 4.681\n",
      "[Checkpoint saved at step 6400]\n",
      "Ep 8 | step 006600 | lr 2.707e-04 | train 4.526 | val 4.670\n",
      "[Checkpoint saved at step 6600]\n",
      "Ep 9 | step 006800 | lr 2.685e-04 | train 4.490 | val 4.662\n",
      "[Checkpoint saved at step 6800]\n",
      "Ep 9 | step 007000 | lr 2.662e-04 | train 4.483 | val 4.647\n",
      "[Checkpoint saved at step 7000]\n",
      "Ep 9 | step 007200 | lr 2.638e-04 | train 4.484 | val 4.639\n",
      "[Checkpoint saved at step 7200]\n",
      "Ep 9 | step 007400 | lr 2.613e-04 | train 4.463 | val 4.629\n",
      "[Checkpoint saved at step 7400]\n",
      "Ep 9 | step 007600 | lr 2.588e-04 | train 4.453 | val 4.618\n",
      "[Checkpoint saved at step 7600]\n",
      "Ep 10 | step 007800 | lr 2.562e-04 | train 4.420 | val 4.607\n",
      "[Checkpoint saved at step 7800]\n",
      "Ep 10 | step 008000 | lr 2.536e-04 | train 4.405 | val 4.603\n",
      "[Checkpoint saved at step 8000]\n",
      "Ep 10 | step 008200 | lr 2.508e-04 | train 4.395 | val 4.592\n",
      "[Checkpoint saved at step 8200]\n",
      "Ep 10 | step 008400 | lr 2.481e-04 | train 4.425 | val 4.579\n",
      "[Checkpoint saved at step 8400]\n",
      "Ep 11 | step 008600 | lr 2.452e-04 | train 4.394 | val 4.578\n",
      "[Checkpoint saved at step 8600]\n",
      "Ep 11 | step 008800 | lr 2.424e-04 | train 4.358 | val 4.567\n",
      "[Checkpoint saved at step 8800]\n",
      "Ep 11 | step 009000 | lr 2.394e-04 | train 4.359 | val 4.557\n",
      "[Checkpoint saved at step 9000]\n",
      "Ep 11 | step 009200 | lr 2.364e-04 | train 4.347 | val 4.549\n",
      "[Checkpoint saved at step 9200]\n",
      "Ep 12 | step 009400 | lr 2.334e-04 | train 4.372 | val 4.545\n",
      "[Checkpoint saved at step 9400]\n",
      "Ep 12 | step 009600 | lr 2.303e-04 | train 4.342 | val 4.541\n",
      "[Checkpoint saved at step 9600]\n",
      "Ep 12 | step 009800 | lr 2.272e-04 | train 4.331 | val 4.530\n",
      "[Checkpoint saved at step 9800]\n",
      "Ep 12 | step 010000 | lr 2.240e-04 | train 4.339 | val 4.526\n",
      "[Checkpoint saved at step 10000]\n",
      "Ep 13 | step 010200 | lr 2.208e-04 | train 4.320 | val 4.520\n",
      "[Checkpoint saved at step 10200]\n",
      "Ep 13 | step 010400 | lr 2.175e-04 | train 4.290 | val 4.518\n",
      "[Checkpoint saved at step 10400]\n",
      "Ep 13 | step 010600 | lr 2.142e-04 | train 4.289 | val 4.512\n",
      "[Checkpoint saved at step 10600]\n",
      "Ep 13 | step 010800 | lr 2.109e-04 | train 4.306 | val 4.502\n",
      "[Checkpoint saved at step 10800]\n",
      "Ep 14 | step 011000 | lr 2.076e-04 | train 4.276 | val 4.500\n",
      "[Checkpoint saved at step 11000]\n",
      "Ep 14 | step 011200 | lr 2.042e-04 | train 4.249 | val 4.495\n",
      "[Checkpoint saved at step 11200]\n",
      "Ep 14 | step 011400 | lr 2.008e-04 | train 4.257 | val 4.489\n",
      "[Checkpoint saved at step 11400]\n",
      "Ep 14 | step 011600 | lr 1.973e-04 | train 4.262 | val 4.489\n",
      "[Checkpoint saved at step 11600]\n",
      "Ep 14 | step 011800 | lr 1.939e-04 | train 4.247 | val 4.471\n",
      "[Checkpoint saved at step 11800]\n",
      "Ep 15 | step 012000 | lr 1.904e-04 | train 4.219 | val 4.475\n",
      "Ep 15 | step 012200 | lr 1.869e-04 | train 4.236 | val 4.471\n",
      "[Checkpoint saved at step 12200]\n",
      "Ep 15 | step 012400 | lr 1.834e-04 | train 4.233 | val 4.465\n",
      "[Checkpoint saved at step 12400]\n",
      "Ep 15 | step 012600 | lr 1.799e-04 | train 4.217 | val 4.461\n",
      "[Checkpoint saved at step 12600]\n",
      "Ep 16 | step 012800 | lr 1.763e-04 | train 4.219 | val 4.453\n",
      "[Checkpoint saved at step 12800]\n",
      "Ep 16 | step 013000 | lr 1.728e-04 | train 4.199 | val 4.456\n",
      "Ep 16 | step 013200 | lr 1.692e-04 | train 4.193 | val 4.451\n",
      "[Checkpoint saved at step 13200]\n",
      "Ep 16 | step 013400 | lr 1.657e-04 | train 4.181 | val 4.443\n",
      "[Checkpoint saved at step 13400]\n",
      "Ep 17 | step 013600 | lr 1.621e-04 | train 4.162 | val 4.438\n",
      "[Checkpoint saved at step 13600]\n",
      "Ep 17 | step 013800 | lr 1.586e-04 | train 4.201 | val 4.439\n",
      "Ep 17 | step 014000 | lr 1.550e-04 | train 4.179 | val 4.433\n",
      "[Checkpoint saved at step 14000]\n",
      "Ep 17 | step 014200 | lr 1.515e-04 | train 4.150 | val 4.435\n",
      "Ep 18 | step 014400 | lr 1.480e-04 | train 4.166 | val 4.426\n",
      "[Checkpoint saved at step 14400]\n",
      "Ep 18 | step 014600 | lr 1.444e-04 | train 4.164 | val 4.430\n",
      "Ep 18 | step 014800 | lr 1.409e-04 | train 4.169 | val 4.423\n",
      "[Checkpoint saved at step 14800]\n",
      "Ep 18 | step 015000 | lr 1.375e-04 | train 4.139 | val 4.416\n",
      "[Checkpoint saved at step 15000]\n",
      "Ep 18 | step 015200 | lr 1.340e-04 | train 4.169 | val 4.416\n",
      "Ep 19 | step 015400 | lr 1.305e-04 | train 4.149 | val 4.419\n",
      "Ep 19 | step 015600 | lr 1.271e-04 | train 4.143 | val 4.412\n",
      "[Checkpoint saved at step 15600]\n",
      "Ep 19 | step 015800 | lr 1.237e-04 | train 4.131 | val 4.407\n",
      "[Checkpoint saved at step 15800]\n",
      "Ep 19 | step 016000 | lr 1.204e-04 | train 4.111 | val 4.403\n",
      "[Checkpoint saved at step 16000]\n",
      "Ep 20 | step 016200 | lr 1.170e-04 | train 4.125 | val 4.407\n",
      "Ep 20 | step 016400 | lr 1.137e-04 | train 4.112 | val 4.401\n",
      "[Checkpoint saved at step 16400]\n",
      "Ep 20 | step 016600 | lr 1.104e-04 | train 4.115 | val 4.396\n",
      "[Checkpoint saved at step 16600]\n",
      "Ep 20 | step 016800 | lr 1.072e-04 | train 4.122 | val 4.397\n",
      "Ep 21 | step 017000 | lr 1.040e-04 | train 4.116 | val 4.395\n",
      "[Checkpoint saved at step 17000]\n",
      "Ep 21 | step 017200 | lr 1.009e-04 | train 4.092 | val 4.395\n",
      "[Checkpoint saved at step 17200]\n",
      "Ep 21 | step 017400 | lr 9.778e-05 | train 4.089 | val 4.392\n",
      "[Checkpoint saved at step 17400]\n",
      "Ep 21 | step 017600 | lr 9.472e-05 | train 4.087 | val 4.386\n",
      "[Checkpoint saved at step 17600]\n",
      "Ep 22 | step 017800 | lr 9.171e-05 | train 4.103 | val 4.384\n",
      "[Checkpoint saved at step 17800]\n",
      "Ep 22 | step 018000 | lr 8.875e-05 | train 4.091 | val 4.383\n",
      "[Checkpoint saved at step 18000]\n",
      "Ep 22 | step 018200 | lr 8.585e-05 | train 4.078 | val 4.385\n",
      "Ep 22 | step 018400 | lr 8.300e-05 | train 4.084 | val 4.380\n",
      "[Checkpoint saved at step 18400]\n",
      "Ep 22 | step 018600 | lr 8.021e-05 | train 4.099 | val 4.376\n",
      "[Checkpoint saved at step 18600]\n",
      "Ep 23 | step 018800 | lr 7.747e-05 | train 4.071 | val 4.377\n",
      "Ep 23 | step 019000 | lr 7.480e-05 | train 4.074 | val 4.376\n",
      "Ep 23 | step 019200 | lr 7.219e-05 | train 4.080 | val 4.371\n",
      "[Checkpoint saved at step 19200]\n",
      "Ep 23 | step 019400 | lr 6.964e-05 | train 4.064 | val 4.369\n",
      "[Checkpoint saved at step 19400]\n",
      "Ep 24 | step 019600 | lr 6.716e-05 | train 4.059 | val 4.371\n",
      "Ep 24 | step 019800 | lr 6.475e-05 | train 4.039 | val 4.369\n",
      "[Checkpoint saved at step 19800]\n",
      "Ep 24 | step 020000 | lr 6.240e-05 | train 4.055 | val 4.367\n",
      "[Checkpoint saved at step 20000]\n",
      "Ep 24 | step 020200 | lr 6.013e-05 | train 4.078 | val 4.364\n",
      "[Checkpoint saved at step 20200]\n",
      "Ep 25 | step 020400 | lr 5.793e-05 | train 4.055 | val 4.366\n",
      "Ep 25 | step 020600 | lr 5.580e-05 | train 4.057 | val 4.366\n",
      "Ep 25 | step 020800 | lr 5.375e-05 | train 4.053 | val 4.363\n",
      "[Checkpoint saved at step 20800]\n",
      "Ep 25 | step 021000 | lr 5.178e-05 | train 4.066 | val 4.360\n",
      "[Checkpoint saved at step 21000]\n",
      "Ep 26 | step 021200 | lr 4.988e-05 | train 4.050 | val 4.359\n",
      "[Checkpoint saved at step 21200]\n",
      "Ep 26 | step 021400 | lr 4.807e-05 | train 4.043 | val 4.359\n",
      "[Checkpoint saved at step 21400]\n",
      "Ep 26 | step 021600 | lr 4.633e-05 | train 4.049 | val 4.359\n",
      "Ep 26 | step 021800 | lr 4.468e-05 | train 4.025 | val 4.357\n",
      "[Checkpoint saved at step 21800]\n",
      "Ep 27 | step 022000 | lr 4.311e-05 | train 4.061 | val 4.354\n",
      "[Checkpoint saved at step 22000]\n",
      "Ep 27 | step 022200 | lr 4.163e-05 | train 4.024 | val 4.357\n",
      "Ep 27 | step 022400 | lr 4.023e-05 | train 4.050 | val 4.354\n",
      "[Checkpoint saved at step 22400]\n",
      "Ep 27 | step 022600 | lr 3.892e-05 | train 4.035 | val 4.353\n",
      "[Checkpoint saved at step 22600]\n",
      "Ep 27 | step 022800 | lr 3.769e-05 | train 4.004 | val 4.352\n",
      "[Checkpoint saved at step 22800]\n",
      "Ep 28 | step 023000 | lr 3.655e-05 | train 4.029 | val 4.353\n",
      "Ep 28 | step 023200 | lr 3.550e-05 | train 4.023 | val 4.352\n",
      "[Checkpoint saved at step 23200]\n",
      "Ep 28 | step 023400 | lr 3.455e-05 | train 4.062 | val 4.351\n",
      "[Checkpoint saved at step 23400]\n",
      "Ep 28 | step 023600 | lr 3.368e-05 | train 3.998 | val 4.351\n",
      "[Checkpoint saved at step 23600]\n",
      "Ep 29 | step 023800 | lr 3.290e-05 | train 4.011 | val 4.349\n",
      "[Checkpoint saved at step 23800]\n",
      "Ep 29 | step 024000 | lr 3.221e-05 | train 4.013 | val 4.350\n",
      "Ep 29 | step 024200 | lr 3.162e-05 | train 4.051 | val 4.348\n",
      "[Checkpoint saved at step 24200]\n",
      "Ep 29 | step 024400 | lr 3.112e-05 | train 4.006 | val 4.349\n",
      "Ep 30 | step 024600 | lr 3.071e-05 | train 4.021 | val 4.348\n",
      "[Checkpoint saved at step 24600]\n",
      "Ep 30 | step 024800 | lr 3.039e-05 | train 4.016 | val 4.347\n",
      "[Checkpoint saved at step 24800]\n",
      "Ep 30 | step 025000 | lr 3.017e-05 | train 4.018 | val 4.347\n",
      "[Checkpoint saved at step 25000]\n",
      "Ep 30 | step 025200 | lr 3.004e-05 | train 4.011 | val 4.345\n",
      "[Checkpoint saved at step 25200]\n"
     ]
    }
   ],
   "source": [
    "#main(settings)\n",
    "train_losses, val_losses, tokens_seen = train_model(\n",
    "    model, train_dataloader, test_dataloader, device,\n",
    "    settings,\n",
    "    save_path=\"checkpoints/gpt_128_128_8_8.pt\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc144fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqYAAAHWCAYAAAClsUvDAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAlrJJREFUeJzs3Xd4VFX+x/H3nZnMTNqkEUgCSSChBoIFFBEpKlbEulZ0sa2udd1dXV1dFXQV2/qzrqvu2ttawLJWUFApKopgDBACISSUEEjvZe79/XHJQEhCMiHJzJn5vp5nHsidOzPn5JOBb86cc65mGIaBEEIIIYQQPmbxdQOEEEIIIYQAKUyFEEIIIYSfkMJUCCGEEEL4BSlMhRBCCCGEX5DCVAghhBBC+AUpTIUQQgghhF+QwlQIIYQQQvgFKUyFEEIIIYRfkMJUCCGEEEL4BSlMhRA97tJLL2Xw4MHdeuycOXPQNK1nG6S4JUuWoGkaS5Ys8Rzr6vc4Pz8fTdN46aWXerRNgwcP5tJLL+3R5xRCCClMhQgimqZ16bZvARRsdF3nkUceYdiwYYSGhpKens4111xDdXV1lx4/duxYUlJSONDVnidNmsSAAQNobm7uqWb3iuXLlzNnzhzKy8t93RSPl156CU3T+PHHH33dFCFEL7D5ugFCiL7z6quvtvr6lVdeYeHChW2Ojxo16qBe5/nnn0fX9W499m9/+xu33XbbQb3+wXj88ce55ZZbOPPMM7nlllvYsmULb775JrfeeisRERGdPn7WrFncdtttfPvtt0yZMqXN/fn5+axYsYLrr78em637/wQfzPe4q5YvX87cuXO59NJLiY6ObnVfTk4OFouMbQghepYUpkIEkYsvvrjV19999x0LFy5sc3x/tbW1hIWFdfl1QkJCutU+AJvNdlAF28F66623GD16NPPnz/dMKbj33nu7XARedNFF/PWvf+WNN95otzB98803MQyDWbNmHVQ7D+Z73BMcDodPX18IEZjk110hRCvTpk1jzJgx/PTTT0yZMoWwsDBuv/12AD744ANmzJhBUlISDoeD9PR07r33Xtxud6vn2H/+Y8s8x0ceeYTnnnuO9PR0HA4HRxxxBCtXrmz12PbmmGqaxvXXX8/777/PmDFjcDgcjB49ms8++6xN+5csWcL48eNxOp2kp6fz7LPPejVv1WKxoOt6q/MtFkuXi+Xk5GSmTJnCu+++S1NTU5v733jjDdLT05kwYQJbtmzh2muvZcSIEYSGhhIXF8e5555Lfn5+p6/T3hzT8vJyLr30UqKiooiOjmb27Nntfgz/yy+/cOmll5KWlobT6SQhIYHLL7+ckpISzzlz5szhlltuAWDIkCGeaR4tbWtvjmleXh7nnnsusbGxhIWFcdRRR/Hxxx+3Oqdlvuzbb7/Nfffdx6BBg3A6nRx//PFs3Lix03531c8//8wpp5yCy+UiIiKC448/nu+++67VOU1NTcydO5dhw4bhdDqJi4vjmGOOYeHChZ5zioqKuOyyyxg0aBAOh4PExETOOOOMLmUkhPCejJgKIdooKSnhlFNO4YILLuDiiy9mwIABgDm/LyIigj/96U9ERETw1Vdfcdddd1FZWcnDDz/c6fO+8cYbVFVVcfXVV6NpGg899BBnn302eXl5nY4ALl26lPnz53PttdcSGRnJE088wTnnnENBQQFxcXGAWYycfPLJJCYmMnfuXNxuN/fccw/x8fFd7vtll13G1VdfzbPPPsvVV1/d5cfta9asWVx11VV8/vnnnHbaaZ7jWVlZ/Prrr9x1110ArFy5kuXLl3PBBRcwaNAg8vPzeeaZZ5g2bRpr1671apTaMAzOOOMMli5dyu9//3tGjRrFggULmD17dptzFy5cSF5eHpdddhkJCQlkZ2fz3HPPkZ2dzXfffYemaZx99tls2LCBN998k//7v/+jX79+AB1+L3fu3MnRRx9NbW0tN954I3Fxcbz88sucfvrpvPvuu5x11lmtzn/ggQewWCzcfPPNVFRU8NBDDzFr1iy+//77Lve5I9nZ2UyePBmXy8Vf/vIXQkJCePbZZ5k2bRpff/01EyZMAMzie968eVx55ZUceeSRVFZW8uOPP7Jq1SpOOOEEAM455xyys7O54YYbGDx4MMXFxSxcuJCCgoJuL/ATQhyAIYQIWtddd52x/z8DU6dONQDjX//6V5vza2tr2xy7+uqrjbCwMKO+vt5zbPbs2UZqaqrn682bNxuAERcXZ5SWlnqOf/DBBwZgfPTRR55jd999d5s2AYbdbjc2btzoObZmzRoDMJ588knPsZkzZxphYWHGtm3bPMdyc3MNm83W5jk7cttttxl2u92wWq3G/Pnzu/SY/ZWWlhoOh8O48MIL2zw3YOTk5BiG0f73c8WKFQZgvPLKK55jixcvNgBj8eLFnmP7f4/ff/99AzAeeughz7Hm5mZj8uTJBmC8+OKLnuPtve6bb75pAMY333zjOfbwww8bgLF58+Y256emphqzZ8/2fH3TTTcZgPHtt996jlVVVRlDhgwxBg8ebLjd7lZ9GTVqlNHQ0OA59/HHHzcAIysrq81r7evFF180AGPlypUdnnPmmWcadrvd2LRpk+fY9u3bjcjISGPKlCmeY4cccogxY8aMDp+nrKzMAIyHH374gG0SQvQc+ShfCNGGw+Hgsssua3M8NDTU8/eqqip2797N5MmTqa2tZf369Z0+7/nnn09MTIzn68mTJwPmR8CdmT59Ounp6Z6vx44di8vl8jzW7XazaNEizjzzTJKSkjznDR06lFNOOaXT5wd44oknePTRR1m2bBkXXnghF1xwAV988UWrcxwOB3feeecBnycmJoZTTz2VDz/8kJqaGsAc0XzrrbcYP348w4cPB1p/P5uamigpKWHo0KFER0ezatWqLrW5xSeffILNZuOaa67xHLNardxwww1tzt33devr69m9ezdHHXUUgNevu+/rH3nkkRxzzDGeYxEREVx11VXk5+ezdu3aVudfdtll2O12z9fe/CwciNvt5osvvuDMM88kLS3NczwxMZGLLrqIpUuXUllZCUB0dDTZ2dnk5ua2+1yhoaHY7XaWLFlCWVnZQbVLCNE1UpgKIdoYOHBgq6KhRXZ2NmeddRZRUVG4XC7i4+M9C6cqKio6fd6UlJRWX7cUqV35T3//x7Y8vuWxxcXF1NXVMXTo0DbntXdsf3V1ddx9991ceeWVjB8/nhdffJHjjjuOs846i6VLlwKQm5tLY2Oj56PgA5k1axY1NTV88MEHgLnCPT8/v9Wip7q6Ou666y6Sk5NxOBz069eP+Ph4ysvLu/T93NeWLVtITExss3PAiBEj2pxbWlrKH/7wBwYMGEBoaCjx8fEMGTIE6FqOHb1+e6/VssPDli1bWh0/mJ+FA9m1axe1tbUdtkXXdQoLCwG45557KC8vZ/jw4WRmZnLLLbfwyy+/eM53OBw8+OCDfPrppwwYMIApU6bw0EMPUVRUdFBtFEJ0TApTIUQb+46otSgvL2fq1KmsWbOGe+65h48++oiFCxfy4IMPAnRp1brVam33uHGAPT974rFdsW7dOsrLyz0jhzabjXfffZcxY8YwY8YMVq1axXPPPUf//v098w8P5LTTTiMqKoo33ngDMOfXWq1WLrjgAs85N9xwA/fddx/nnXceb7/9Nl988QULFy4kLi6uV7eCOu+883j++ef5/e9/z/z58/niiy88C8l6ewuqFr2dZ1dMmTKFTZs28cILLzBmzBj+/e9/c/jhh/Pvf//bc85NN93Ehg0bmDdvHk6nkzvvvJNRo0bx888/91k7hQgmsvhJCNElS5YsoaSkhPnz57faBmnz5s0+bNVe/fv3x+l0truyuyurvVtW4beMpgGEh4fzySefcMwxx3DSSSdRX1/P3//+9y5tleRwOPjNb37DK6+8ws6dO3nnnXc47rjjSEhI8Jzz7rvvMnv2bP7xj394jtXX13drQ/vU1FS+/PJLqqurW42a5uTktDqvrKyML7/8krlz53oWYQHtfpztzRW4UlNT27wW4JnikZqa2uXnOhjx8fGEhYV12BaLxUJycrLnWGxsLJdddhmXXXYZ1dXVTJkyhTlz5nDllVd6zklPT+fPf/4zf/7zn8nNzeXQQw/lH//4B6+99lqf9EmIYCIjpkKILmkZ4dp3RKuxsZF//vOfvmpSK1arlenTp/P++++zfft2z/GNGzfy6aefdvr4zMxMBgwYwFNPPUVxcbHneFxcHC+++CK7d++mrq6OmTNndrlNs2bNoqmpiauvvppdu3a12bvUarW2GSF88skn22y/1RWnnnoqzc3NPPPMM55jbrebJ598ss1rQtuRyccee6zNc4aHhwN0qVA+9dRT+eGHH1ixYoXnWE1NDc899xyDBw8mIyOjq105KFarlRNPPJEPPvig1ZZOO3fu5I033uCYY47B5XIBtNoeC8w5sUOHDqWhoQEw9++tr69vdU56ejqRkZGec4QQPUtGTIUQXXL00UcTExPD7NmzufHGG9E0jVdffbVPP3rtzJw5c/jiiy+YNGkS11xzDW63m6eeeooxY8awevXqAz7WZrPx1FNPcf7555OZmcnVV19Namoq69at44UXXiAzM5OtW7dyxhlnsGzZMk9xcyBTp05l0KBBfPDBB4SGhnL22We3uv+0007j1VdfJSoqioyMDFasWMGiRYs82195Y+bMmUyaNInbbruN/Px8MjIymD9/fps5oy6XyzNXsqmpiYEDB/LFF1+0O/I9btw4AO644w4uuOACQkJCmDlzpqdg3ddtt93Gm2++ySmnnMKNN95IbGwsL7/8Mps3b+a9997r8atEvfDCC+3uY/uHP/yBv//97yxcuJBjjjmGa6+9FpvNxrPPPktDQwMPPfSQ59yMjAymTZvGuHHjiI2N5ccff+Tdd9/l+uuvB2DDhg0cf/zxnHfeeWRkZGCz2ViwYAE7d+5sNSVDCNFzpDAVQnRJXFwc//vf//jzn//M3/72N2JiYrj44os5/vjjOemkk3zdPMAspD799FNuvvlm7rzzTpKTk7nnnntYt25dl3YN+M1vfsOSJUu47777ePzxx2loaGDYsGH85S9/4Q9/+ANff/01M2bM4Nxzz+Xjjz/udNN9i8XChRdeyMMPP8zMmTOJjIxsdf/jjz+O1Wrl9ddfp76+nkmTJrFo0aJufT8tFgsffvghN910E6+99hqapnH66afzj3/8g8MOO6zVuW+88QY33HADTz/9NIZhcOKJJ/Lpp5+22s0A4IgjjuDee+/lX//6F5999hm6rrN58+Z2C9MBAwawfPlybr31Vp588knq6+sZO3YsH330ETNmzPC6P53Zd2R4X5deeimjR4/m22+/5a9//Svz5s1D13UmTJjAa6+91mrh2o033siHH37IF198QUNDA6mpqfz973/3XFggOTmZCy+8kC+//JJXX30Vm83GyJEjefvttznnnHN6vE9CCNAMfxruEEKIXnDmmWcecFsgIYQQ/kHmmAohAkpdXV2rr3Nzc/nkk0+YNm2abxokhBCiy2TEVAgRUBITEz3Xgd+yZQvPPPMMDQ0N/PzzzwwbNszXzRNCCHEAMsdUCBFQTj75ZN58802KiopwOBxMnDiR+++/X4pSIYRQgIyYCiGEEEIIvyBzTIUQQgghhF+QwlQIIYQQQvgFpeeY6rrO9u3biYyM9OrSeUIIIYQQom8YhkFVVRVJSUmdXmxD6cJ0+/btra55LIQQQggh/FNhYSGDBg064DlKF6YtV1EpLCzs0uUBD5bb7SY7O5vRo0d7rjct/J/kph7JTD2SmXokM/WomlllZSXJycltrn7XHqUL05aP710uV58VpomJibhcLqV+IIKd5KYeyUw9kpl6JDP1qJ5ZV6ZdKr1dVGVlJVFRUVRUVPRJYSqEEEIIIbzjTb0mq/K9oOs6RUVF6Lru66YIL0hu6pHM1COZqUcyU08wZCaFqRcMw6CoqAiFB5mDkuSmHslMPZKZeiQz9QRDZkrPMRVCCCFE9xiGgWEY1NfXKzlfMRi53W6/zMxqtWKz2Xpk604pTIUQQogg09jYyPbt29E0jS1btshe4IowDAOLxeKXmYWFhZGYmIjdbj+o55HC1AuaphEbG+t3PwziwCQ39Uhm6pHM1KHrOps3b8ZqtTJo0CDCwsI63fRc+AfDMGhqaiIkJMRv3muGYdDY2MiuXbvYvHkzw4YNO6ifJylMvWCxWEhJSfF1M4SXJDf1SGbqkczU0djYiK7rJCcnExYW5uvmCC+Fhob6uglthIaGEhISwpYtW2hsbMTpdHb7ueRXJC/ouk5BQUFAr4YLRJKbeiQz9Uhm6tE0jYaGhoBeSBNoDMPw28x6atRdClMvGIZBaWmpX/5AiI5JbuqRzNQjmanJ7Xb7ugnCS4GemRSmQgghhBDCL0hhKoQQQoigNXjwYB577DGfP4cwSWHqBU3TSEhI8JuVcKJrJDf1SGbqkczUFBIS4usmdJmmaQe8zZkzp1vPu3LlSq666qqebWwvUimz7pBV+V6wWCwkJCT4uhnCS5KbeiQz9Uhm6tE0TakiZ8eOHZ6///e//+Wuu+4iJyfHcywiIsLzd8MwcLvd2Gydlznx8fE929BepFpm3SEjpl5Y/tSVbJo7hp8Xv+frpggvuN1uNm3aFPATxgOJZKYeyUw9LVcQarkCVG1js09uXV0wl5CQ4LlFRUV5RukTEhJYv349kZGRfPrpp4wbNw6Hw8HSpUvZtGkTZ5xxBgMGDCAiIoIjjjiCRYsWtXre/T+G1zSNf//735x11lmEhYUxbNgwPvzwQ6++twUFBZxxxhlERETgcrk477zz2Llzp+f+NWvWcOyxxxIZGYnL5WLcuHH8+OOPAGzZsoWZM2cSExNDeHg4o0eP5pNPPmmTWaCSEVMvhNZuI90oZMXOXF83RXipqqrK100QXpLM1COZqadle6+6JjcZd33ukzasveckwuw9U47cdtttPPLII6SlpRETE0NhYSGnnnoq9913Hw6Hg1deeYWZM2eSk5NzwH13586dy0MPPcTDDz/Mk08+yaxZs9iyZQuxsbGdtkHXdU9R+vXXX9Pc3Mx1113H+eefz5IlSwCYNWsWhx12GM888wxWq5XVq1d7RkKvu+46Ghsb+eabbwgPD2ft2rWtRoMDfUs2KUy90BA+EGrBKC/0dVOEEEIIsZ977rmHE044wfN1bGwshxxyiOfre++9lwULFvDhhx9y/fXXd/g8l156KRdeeCEA999/P0888QQ//PADJ598cqdt+PLLL8nKymLz5s0kJycD8MorrzB69GhWrlzJEUccQUFBAbfccgsjR44EYNiwYZ7HFxQUcM4555CZmQlAWlqaF98B9Ulh6gXDNQh2gb16m6+bIoQQQvSY0BAra+85yWev3VPGjx/f6uvq6mrmzJnDxx9/zI4dO2hubqauro6CgoIDPs/YsWM9fw8PD8flclFcXNylNqxbt47k5GRPUQqQkZFBdHQ069at44gjjuBPf/oTV155Ja+++irTp0/n3HPPJT09HYAbb7yRa665hi+++ILp06dzzjnntGpPoJM5pl6w9xsMQGT9jgOfKPyKpmkkJyfLamGFSGbqkczUZLfbATO/MLvNJ7ee/JkJDw9v9fXNN9/MggULuP/++/n2229ZvXo1mZmZNDY2HvB59l9gpGlaj36EPmfOHLKzs5kxYwZfffUVGRkZLFiwAIArr7ySvLw8LrnkErKyshg/fjxPPvmk57EtmQUqKUy94Eowh9Njm3d2cqbwJxaLhbi4uB67XJrofZKZeiQz9Wiahs3Ws4Whv1m2bBmXXnopZ511FpmZmSQkJJCfn9+rrzlq1CgKCwspLNw77W/t2rWUl5eTkZHhOTZ8+HD++Mc/8sUXX3D22Wfz4osveu5LTk7m97//PfPnz+fPf/4zzz//PBAcmcm/IF6IThwCQDxlNNTX+rg1oqvcbjfr16+X1cIKkczUI5mpxzAM6urqAnqF97Bhw5g/fz6rV69mzZo1XHTRRb2+eGj69OlkZmYya9YsVq1axQ8//MBvf/tbpk6dyvjx46mrq+P6669nyZIlbNmyhWXLlrFy5UpGjRoFwE033cTnn3/O5s2bWbVqFYsXL/bcFwyZSWHqhZi4BOoMcwh919Y8H7dGeKO+vt7XTRBekszUI5mpJ5ALHIBHH32UmJgYjj76aGbOnMlJJ53E4Ycf3quvqWkaH3zwATExMUyZMoXp06eTlpbGf//7XwCsVislJSX89re/Zfjw4Zx33nmccsopzJ07FzB/ybvuuusYNWoUJ598MsOHD+ef//yn5/kDPTPNULiHlZWVREVFUVFRgcvl6vXXc7vdFN6byWC28evxrzBm8hm9/pri4LndbrKyssjMzMRq7blJ9qL3SGbqkczUUV9fz+bNmxk8eDCGYRAaGhrQHw0HkpYRU3/MrOXnasiQITidzlb3eVOvyYipl0pt5hUi6nbl+7YhQgghhBABRgpTL1gsFhoiBgHgLjvwVhPCf1gsFtLS0mRRhkIkM/VIZmpyOBy+boLwUqBnJv+CeEHTNIg2rxRhrdrq49aIrtI0DZfL5Xcfe4iOSWbqkczUo2kaVqtVMlNIMGQmhakX3G43FUYkAGG1233cGtFVLXPfZLWwOiQz9Uhm6jEMg9ra2oBfTBNIgiEzKUy9ZIkcAEB0k+xlqhL5z1I9kpl6JDMhxMGSwtRLjphEAOL13ejyj7AQQgghRI+RwtRLEdH9aTKs2DU3JUWyAEoIIYQQoqdIYeoFi8VCRsZodlniACjZtsnHLRJdYbFYGDFihKwWVohkph7JTE377zcp/F+gZyb/gnjJbrdTZjPnmdYUb/Zxa0RX2e12XzdBeEkyU49kpp5AXt0dqAI9MylMvaDrOllZWdSEmvNMm0q3+LhFoitacuvt6yOLniOZqUcyU1NdXZ2vm9Dnpk2bxk033eT5evDgwTz22GMHfIymabz//vsH/do98TydZTZnzhwOPfTQg3oNX5LCtBuaIs1N9i0VhT5uiRBCCBEcZs6cycknn9zufd9++y2apvHLL794/bwrV67kqquuOtjmtdJRcbhjxw5OOeWUHn2tQCOFaTdYo5MBcMpepkIIIUSfuOKKK1i4cCFbt7a9wM2LL77I+PHjGTt2rNfPGx8fT1hYWE80sVMJCQkBf+WmgyWFaTc4+qUCENWww8ctEUIIIXqAYUBjjW9uXdws/rTTTiM+Pp6XXnqp1fHq6mreeecdrrjiCkpKSrjwwgsZOHAgYWFhZGZm8uabbx7weff/KD83N5cpU6bgdDrJyMhg4cKFbR5z6623Mnz4cMLCwkhLS+POO++kqakJgJdeeom5c+eyZs0aNE1D0zRPm/f/KD8rK4vjjjuO0NBQ4uLiuOqqq6iurvbcf+mll3LmmWfyyCOPkJiYSL9+/fjjH//oea2u0HWde+65h0GDBuFwODj00EP57LPPPPc3NjZy/fXXk5iYiNPpJDU1lXnz5gHmhv5z5swhJSUFh8NBUlISN954Y5dfuztsvfrsAcZisZCZmUnhRnPicby72HxDBfhEZNW15CarhdUhmalHMlNTaGio+ZemWrg/yTeNuH072MM7Pc1ms/Hb3/6Wl156iTvuuMOzCOidd97B7XZz4YUXUl1dzbhx47j11ltxuVx8/PHHXHLJJaSnp3PkkUd2+hq6rnP22WczYMAAvv/+eyoqKlrNR20RGRnJSy+9RFJSEllZWfzud78jMjKSv/zlL5x//vn8+uuvfPbZZyxatAiAqKioNs9RU1PDSSedxMSJE1m5ciXFxcVceeWVXH/99a2K78WLF5OYmMjixYvJzc3lggsuYNy4cV2efvD444/zj3/8g2effZbDDjuMF154gdNPP53s7GyGDRvGE088wYcffsjbb79NSkoKhYWFFBaaUxXfe+89/u///o+33nqL0aNHU1RUxJo1a7r0ut0lhamXGhsbiR+UBkCY1kBlWTGu2AE+bpXoTGNjY8BvsRFoJDP1SGbqMQxDqVXel19+OQ8//DBff/0106ZNA8yP8c855xyioqKIiori5ptv9px/ww038Pnnn/P22293qTBdtGgR69ev5/PPPycpySzU77///jbzQv/2t795/j548GBuvvlm3nrrLf7yl78QGhpKREQENpuNhISEDl/rjTfeoL6+nldeeYXwcLMwf+qpp5g5cyYPPvggAwaYtUVMTAxPPfUUVquVESNGcOqpp/LVV191uTB95JFHuPXWW7ngggsAePDBB1m8eDGPPfYYTz/9NAUFBQwbNoxjjjkGTdNITU31PLagoICEhASmT59OSEgIKSkpXfo+HgwpTL2g6zo5OTlkZmaym2j6Uc7urRulMPVz++ZmtVp93RzRBZKZeiQzNdXX15ujpiFh5silL4R0fX7nyJEjOfroo3nhhReYNm0aGzdu5Ntvv+Wee+4BzMvi3n///bz99tts27aNxsZGGhoaujyHdN26dSQnJ3uKUoCJEye2Oe+///0vTzzxBJs2baK6uprm5mZcLleX+9HyWocccoinKAWYNGmS573UUpiOHj261XsqPj6e9evXd+k1Kisr2b59O5MmTWp1fNKkSZ6Rz0svvZQTTjiBESNGcPLJJ3Paaadx4oknAnDuuefy2GOPkZaWxsknn8ypp57KzJkzsdl6r3z06WcubrebO++8kyFDhhAaGkp6ejr33nsvRhfnm/hSia0/AJVFeT5uiRBCCHGQNM38ON0XNy9HbK+44gree+89qqqqePHFF0lPT2fq1KkAPPzwwzz++OPceuutLF68mNWrV3PSSSfR2NjYY9+qFStWMGvWLE499VT+97//8fPPP3PHHXf06GvsKyQkpNXXmqb16LZshx9+OJs3b+bee++lrq6O8847j9/85jcAJCcnk5OTwz//+U9CQ0O59tprmTJlildzXL3l08L0wQcf5JlnnuGpp55i3bp1PPjggzz00EM8+eSTvmxWl1Q5zL1MG0tkL1MhhBCir5x33nlYLBbeeOMNXnnlFS6//HLPdIRly5ZxxhlncPHFF3PIIYeQlpbGhg0buvzco0aNorCwkB079i5u/u6771qds3z5clJTU7njjjsYP348w4YNY8uW1rWA3W7H7XZ3+lpr1qyhpqbGc2zZsmWeq6j1BJfLRVJSEsuWLWt1fNmyZWRkZLQ67/zzz+f555/nv//9L++99x6lpaWAOQ955syZPPHEEyxZsoQVK1aQlZXVI+1rj08/yl++fDlnnHEGM2bMAMx5Gm+++SY//PCDL5t1QC3D6Y0RA6EGKJe9TFUgHy2qRzJTj2Qm+kJERATnn38+f/3rX6msrOTSSy/13Dds2DDeffddli9fTkxMDI8++ig7d+5sVYQdyPTp0xk+fDizZ8/m4YcfprKykjvuuKPVOcOGDaOgoIC33nqLI444go8//pgFCxa0Omfw4MFs3ryZ1atXM2jQICIjI9tsEzVr1izuvvtuZs+ezZw5c9i1axc33HADl1xyiedj/J5wyy23cPfdd5Oens6hhx7Kiy++yOrVq3n99dcBePTRR0lMTOSwww7DYrHwzjvvkJCQQHR0NC+99BJut5sJEyYQFhbGa6+9RmhoaKt5qD3Np4Xp0UcfzXPPPceGDRsYPnw4a9asYenSpTz66KPtnt/Q0EBDQ4Pn68rKSsCcEtDym4mmaVgsFnRdbzUloOX4/r/BdHTcYrGgaVqb42PGjDH/EjUIdoK9eitut9uzEnX/4XWr1YphGO0e37+NHR3v7T511PZA6lPLP0otrxUIfersuOp9GjNmDLqut3qM6n1q73gg9Wnf//wDpU/7tjFQ+uR2uz2LnloWq+3fnvam1PnqeHsuv/xy/vOf/3DqqaeSmJjoedzf/vY38vLyOOmkkwgLC+N3v/sdZ555JhUVFa2e2zCMVgu/9v16/vz5XHnllRx55JEMHjyYxx9/nFNOOcXz+JkzZ3LTTTdx/fXX09DQwIwZM7jzzjuZM2eO55yzzz6b+fPnc+yxx1JeXs4LL7zgKaBbXis0NJTPPvuMm266iSOOOIKwsDDOPvtsHn300Tbfh32/3nd+54G+Xy333XDDDZSXl/PnP/+Z4uJiMjIy+OCDDxg6dCiGYRAZGclDDz1Ebm4uVqvVU2xbLBaioqJ48MEH+dOf/oTb7SYzM5OPPvqI2NjYdttoGEarmqzlZ6+z0eN9aYYPJ3Tqus7tt9/OQw89hNVqxe12c9999/HXv/613fPnzJnD3Llz2xz/9ttviYiIACA2NpaUlBQKCgo8w9BgbmqbkJDApk2bqKqq8hxPTk4mLi6O9evXU19f7zmelpaGy+UiKyur1Td04MCBxMTEsPCtJzll4xw2aEOoPeMlMjMzaWxsJCcnx3Ou1WolMzOTyspK8vL2zkV1Op2MHDmSkpISz5YMYG4/kZ6eTlFREUVFRZ7jvd2nESNGYLfb2wzNB1KfmpqaCAkJCag+BWJOLcaMGUNZWRnbtm0LmD4FYk7796mpqQmHw8HYsWMDpk8QeDkZhoHFYmH48OFYrVaam5s952qaRmhoKM3Nza3mTFosFpxOJ01NTa3mF1qtVhwOBw0NDa3aEhISQkhICPX19a0KZbvdjs1mo66urlVh43A4sFqt1NbWtmq70+lE07Q2l+EMDQ3FMIxW3xeAsLAw3G53q0GsQOtTS37+1qeGhgY2bNiAruuegr/lZ++nn35i8uTJVFRUdLpIzKeF6VtvvcUtt9zCww8/zOjRo1m9ejU33XQTjz76KLNnz25zfnsjpsnJyZSWlno62pu/kbrdbrKzs8nMzGTTr98z4v1TKSeSyDu3BMVv2ar2qSW30aNHeyaRq96nrhxXuU+GYZCVldVmNarKfQrEnDp6n9nt9oDo0/5tDJQ+1dfXs2XLFoYMGYJhGJ5Cad/X9fcR0474W9t7uk8thavT6cRisfhVn+rr69m8eTOpqamekfiWn73y8nJiY2O7VJj69KP8W265hdtuu82zt1ZmZiZbtmxh3rx57RamDoej3Ut5Wa3WNnObOtrkuaM5UF093nIVh/4p5sTkaKqor6/BGe7q8Hk0TWv3eEdt9Pb4wfapO8dV61NLe1v+8Q2EPnXluKp9crvdnra39x5UsU8HOh4ofdq3H4HSp30FSp/2/bcQ9v6/tq+O9jb11XFv+Fvbe6NPLV/7U59afo7a+3fbm/nnPl2VX1tb2+YN2vJbpr+Ljomj0jD3Rdu9bZOPWyOEEEIIoT6fFqYzZ87kvvvu4+OPPyY/P58FCxbw6KOPctZZZ/myWQfUMjytaRq7rOZephXbpTD1d3I1GvVIZuqRzNTTE6N5om8FemY+/Sj/ySef5M477+Taa6+luLiYpKQkrr76au666y5fNqtDVquVkSNHer6udCRAXT51u/N91yjRqf1zE/5PMlOPZKam0NBQXzdBeKFlIZc/6qklSz4tTCMjI3nsscd47LHHfNmMLtN1nbKyMmJiYrBYLNSFDYQ60MsKfN00cQD75yb8n2SmHslMHS2LQGtqaggJCWkz51T4r5btmPwxs5aV+vtfqcpbPi1MVWMYBoWFhURHR5tfRw2CErBVbTvwA4VP7Z+b8H+SmXokM3VYrVaio6PZtWsXjY2NREdHyy8Tith3Vb6/FKaGYVBbW0txcTHR0dFeLXRqjxSmByEkNhXyILxuu6+bIoQQQnRZQkICuq5TXFxMeXm53xQ54sAMw/Dsy+1vmUVHR5OQkHDQzyOF6UEIj08BwNW828ctEUIIIbpO0zQSEhIoLi4mNTX1oEe5RN9wu91s2LDB7zJrmRLSE6Qw9VJkZKTn72FR8QBEGNW+ao7oon1zE2qQzNQjmanH5XLhdDr9qsgRHXO73QGfmU+v/HSwKisriYqK6tKVBHrDrqJC4v81BgD9byVYbFLnCyGEEELsy5t6TWY7e0HXdYqKijwXAIiMjvfcV1NV4qtmiU7sn5vwf5KZeiQz9Uhm6gmGzKQw9YJhGBQVFXn26nI6ndQY5obS1WUyz9Rf7Z+b8H+SmXokM/VIZuoJhsykMD1IVVoEALWVMmIqhBBCCHEwpDA9SDUWszCtl8JUCCGEEOKgSGHqBU3TiI2NbbV3WJ3NnMTbIHNM/VZ7uQn/JpmpRzJTj2SmnmDITJaRe8FisZCSktLqWKPNBY3QXFPmo1aJzrSXm/Bvkpl6JDP1SGbqCYbMZMTUC7quU1BQ0Go1XJPdHDE16qQw9Vft5Sb8m2SmHslMPZKZeoIhMylMvWAYBqWlpa1Ww7kd0eZfpDD1W+3lJvybZKYeyUw9kpl6giEzKUwPljMaAEtDhW/bIYQQQgihOClMD5IlLAaAkEYpTIUQQgghDoYUpl7QNI2EhIRWq+Gs4bEA2JsqfdUs0Yn2chP+TTJTj2SmHslMPcGQmazK94LFYiEhIaHVsZAIc8TU6ZbC1F+1l5vwb5KZeiQz9Uhm6gmGzGTE1Atut5tNmzbhdrs9x5yRcQCE69W+apboRHu5Cf8mmalHMlOPZKaeYMhMClMvVVVVtfo6NKofABFSmPq1/XMT/k8yU49kph7JTD2BnpkUpgcpck9hGq7V425q9HFrhBBCCCHUJYXpQYqMjvP8vapslw9bIoQQQgihNilMvaBpGsnJya1Ww4WEhFBhhANQXbHbV00TB9BebsK/SWbqkczUI5mpJxgyk1X5XrBYLMTFxbU5Xm2JIMqooVYKU7/UUW7Cf0lm6pHM1COZqScYMpMRUy+43W7Wr1/fZjVcrSUCgIaqUl80S3Sio9yE/5LM1COZqUcyU08wZCaFqZfq6+vbHKuzugBorC7p6+aILmovN+HfJDP1SGbqkczUE+iZSWHaA5pCzMLUXSMjpkIIIYQQ3SWFaQ9otkcBYNSV+7YhQgghhBAKk8LUCxaLhbS0NCyW1t82t9MsTJHC1C91lJvwX5KZeiQz9Uhm6gmGzGRVvhc0TcPlcrW9wxkDgLWhvG8bJLqkw9yE35LM1COZqUcyU08wZBa4JXcvcLvdZGVltVkNZwmLBiCkqdIHrRKd6Sg34b8kM/VIZuqRzNQTDJlJYeql9n4YbOGxANilMPVbgfwmDlSSmXokM/VIZuoJ9MykMO0B9khzs9tQd5WPWyKEEEIIoS4pTHuAM9IcMQ3XpTAVQgghhOguKUy9YLFYGDFiRJvVcGFR/QCINKp90SzRiY5yE/5LMlOPZKYeyUw9wZBZ4Pasl9jt9jbHIqP7A+DUmmiok+LUH7WXm/Bvkpl6JDP1SGbqCfTMpDD1gq7rZGVloet6q+ORrmiaDfNbWVW+2xdNEwfQUW7Cf0lm6pHM1COZqScYMpPCtAdYrBaqtHAAaiukMBVCCCGE6A4pTHtItRYJQG1liY9bIoQQQgihJilMe0itNQKABilMhRBCCCG6RQpTL1gsFjIzM9tdDddgNS8R1lRd2tfNEp04UG7CP0lm6pHM1COZqScYMgvcnvWSxsbGdo832M3C1F1b1pfNEV3UUW7Cf0lm6pHM1COZqSfQM5PC1Au6rpOTk9Puaji3PQoAQwpTv3Og3IR/kszUI5mpRzJTTzBkJoVpD9Gd0QBoDRW+bYgQQgghhKKkMO0pzhgAbFKYCiGEEEJ0ixSmXrJare0fD4sGwNYkhak/6ig34b8kM/VIZuqRzNQT6JnZfN0AlVitVjIzM9u9zxYeC4CzubIvmyS64EC5Cf8kmalHMlOPZKaeYMhMRky9YBgGlZWVGIbR5j6HKw6AUHdVXzdLdOJAuQn/JJmpRzJTj2SmnmDITApTL+i6Tl5eXrur4Zx7CtNwvbqvmyU6caDchH+SzNQjmalHMlNPMGQmhWkPCd9TmLqMaowA/oERQgghhOgtUpj2kMiYeABCNDf1tfJxvhBCCCGEt6Qw9ZLT6Wz3eHh4JA2GuZassnxXXzZJdEFHuQn/JZmpRzJTj2SmnkDPTDMUnkFbWVlJVFQUFRUVuFwuXzeH3XNS6Uc5m3/zOUPGHOXr5gghhBBC+Jw39ZqMmHpB13VKSko6nHRco0UAUFdZ0pfNEp3oLDfhfyQz9Uhm6pHM1BMMmUlh6gXDMCgsLOxwm4ZaayQADdWlfdks0YnOchP+RzJTj2SmHslMPcGQmRSmPajBZg5PN0thKoQQQgjhNSlMe1CT3SxM3bVlPm6JEEIIIYR6pDD1UmRkZIf3NTuizb/USWHqbw6Um/BPkpl6JDP1SGbqCfTMbL5ugEqsVivp6ekd3m84ogDQ6iv6qkmiCzrLTfgfyUw9kpl6JDP1BENmMmLqBV3XKSoq6nA1nBYaA4CtsbwPWyU601luwv9IZuqRzNQjmaknGDKTwtQLhmFQVFTU4Wo4S7hZmNqbKvuyWaITneUm/I9kph7JTD2SmXqCITMpTHtQSHgsAI5muSSpEEIIIYS3pDDtQU5XHADhbhkxFUIIIYTwlhSmXtA0jdjYWDRNa/f+UFc/AMKN6r5sluhEZ7kJ/yOZqUcyU49kpp5gyExW5XvBYrGQkpLS4f3hUWZhGmnUoLvdWKzWvmqaOIDOchP+RzJTj2SmHslMPcGQmYyYekHXdQoKCjpcDeeKMQtTq2ZQXSV7mfqLznIT/kcyU49kph7JTD3BkJkUpl4wDIPS0tIOV8M5Q8OoNRwAVJft7sumiQPoLDfhfyQz9Uhm6pHM1BMMmUlh2sOqtHAAaiulMBVCCCGE8IYUpj2s1mJeKqy+ssTHLRFCCCGEUIsUpl7QNI2EhIQDroarsZmXJW2s2NlXzRKd6Epuwr9IZuqRzNQjmaknGDLzaWE6ePBgNE1rc7vuuut82awOWSwWEhISsFg6/rZVO5MAaC7J76NWic50JTfhXyQz9Uhm6pHM1BMMmfm0ZytXrmTHjh2e28KFCwE499xzfdmsDrndbjZt2oTb7e7wnCaXuY2DtXJLXzVLdKIruQn/IpmpRzJTj2SmnmDIzKf7mMbHx7f6+oEHHiA9PZ2pU6f6qEWdq6o68OVGLbGDYSuE1WztmwaJLuksN+F/JDP1SGbqkczUE+iZ+c0G+42Njbz22mv86U9/6nDuRENDAw0NDZ6vKyvNS3+63W7Pbw+apmGxWNB1vdV2Ci3H9/8to6PjFosFTdNaHXe73RiGgWEY7Z4P4IgfAkBMw3bcbjdWqxXDMNrsOWa1Wtu0saPjvdmnfdu+fxs7Oq5an1pyc7vdAdOnrhxXuU8dvc9U7lMg5tTR+yxQ+rR/GwOtT0CH7zNV+xSIOe3bp/3fZ6r0yZsRXr8pTN9//33Ky8u59NJLOzxn3rx5zJ07t83x7OxsIiIiAIiNjSUlJYWtW7dSWlrqOSchIYGEhATy8/Nb/baRnJxMXFwcubm51NfXe46npaXhcrlYu3at5xva8sOg6zpr165t1YbMzEwaGxsprTOv9hSv7yI7aw1jDz2cqqoq8vLyPOc6nU5GjhxJWVkZhYWFnuORkZGkp6dTXFxMUVGR53hv9glgxIgR2O12srKy2u1TTk6O55jVaiUzM1OpPrXs+5adnc3YsWMDok+BmNO+MjIycLvdZGdne35RVb1PgZjTvn1qeZ+tW7eOQw45JCD6FIg57dunjIwMmpqaWr3PVO9TIOa0b59a3mcbN24kIyNDmT5lZ2fTVZrhJ7u0nnTSSdjtdj766KMOz2lvxDQ5OZnS0lJcLhfQu78V6LpORUUFMTExbdrW8ltBdV0DtgeTCdUaKb/ye6IHjQzo395U6JOu65SXlxMdHY3NZguIPnXluMp9AigrKyMqKqrVJH+V+xSIOXX0PgsJCQmIPu3fxkDrk6ZplJaWtvs+U7VPgZjTvn3a/32mSp/Ky8uJjY2loqLCU691xC8K0y1btpCWlsb8+fM544wzuvy4yspKoqKiutTRvrRpzmjS2Ur+qa8x+MiZvm6OEEIIIYTPeFOv+cV+Ay+++CL9+/dnxowZvm7KAbndbtavX9/pXImSEHPLqJqiTX3RLNGJruYm/Idkph7JTD2SmXqCITOfF6a6rvPiiy8ye/Zsz8es/mzfuRYdqQ4bBIBb9jL1G13JTfgXyUw9kpl6JDP1BHpmPi9MFy1aREFBAZdffrmvm9JjWvYytclepkIIIYQQXebzIcoTTzyxzcRd1VnjhkAhhNfKXqZCCCGEEF3l8xFTlVgsFtLS0lqtXmxP2IB0AGIbt/dFs0Qnupqb8B+SmXokM/VIZuoJhswCt2e9QNM0XC5XhxcAaBGTNAyASKMa6sr6omniALqam/Afkpl6JDP1SGbqCYbMpDD1gtvtJisrq9PVcEn949hlRAFQtyvvgOeK3tfV3IT/kMzUI5mpRzJTTzBkJoWpl7rywxAVGsI2BgBQvjW3t5skuiCQ38SBSjJTj2SmHslMPYGemRSmvaTUnghATbHsZSqEEEII0RVSmPaSmnBzL1O9ZLOPWyKEEEIIoQYpTL1gsVgYMWJEl1bDNbtSAbBVFvR2s0QnvMlN+AfJTD2SmXokM/UEQ2aB27NeYrfbu3SeLW4IABGyl6lf6Gpuwn9IZuqRzNQjmakn0DOTwtQLuq6TlZWFruudnhs2YCgAMU07QQ/sicr+zpvchH+QzNQjmalHMlNPMGQmhWkv6ZeYSqNhJYRmqJSN9oUQQgghOiOFaS8ZGBfBViMegMbdspepEEIIIURnpDDtJXHhdrZp5l6mldtlL1MhhBBCiM5IYeoFi8VCZmZml1bDaZpGqT0JgDrZy9SnvMlN+AfJTD2SmXokM/UEQ2aB27Ne0tjY2OVza8MGAqCXbumt5ogu8iY34R8kM/VIZuqRzNQT6JlJYeoFXdfJycnp8mo4d/RgAEKqZC9TX/I2N+F7kpl6JDP1SGbqCYbMpDDtRSF79jKNlL1MhRBCCCE6JYVpLwpPMPcyjXSXQUO1j1sjhBBCCOHfpDD1ktVq7fK5A/r3p8yIML8ol3mmvuRNbsI/SGbqkczUI5mpJ9Az0wzDMHzdiO6qrKwkKiqKiooKXC6Xr5vTxo6KOnb9YyJjLZtxn/8G1lEzfN0kIYQQQog+5U29JiOmXjAMg8rKSrpay/ePdLLV6A9A9Y6Nvdk0cQDe5iZ8TzJTj2SmHslMPcGQmRSmXtB1nby8vC6vhrNaNEodspepr3mbm/A9yUw9kpl6JDP1BENmUpj2srrwZPMvZfk+bYcQQgghhL+TwrSX6VGpANhlL1MhhBBCiAOSwtRLTqfTq/ND+pl7mUbU7YAAnhPi77zNTfieZKYeyUw9kpl6Aj0zWZXfy979fhNnfzIOi2bAzRshIt7XTRJCCCGE6DOyKr+X6LpOSUmJV5OOk+Ki2EmM+UW5fJzvC93JTfiWZKYeyUw9kpl6giEzKUy9YBgGhYWFXm3TkBQdSqFhjpIassm+T3QnN+Fbkpl6JDP1SGbqCYbMpDDtZYnRTrbuKUxrd8qWUUIIIYQQHZHCtJc5bFbKQhIAqN+V79vGCCGEEEL4MSlMvRQZGen1Y2rDBgKgl8lH+b7SndyEb0lm6pHM1COZqSfQM7P5ugEqsVqtpKene/04PSoFqiGkqrAXWiU6093chO9IZuqRzNQjmaknGDKTEVMv6LpOUVGR16vhbLHmJvvhddtlL1Mf6G5uwnckM/VIZuqRzNQTDJlJYeoFwzAoKiryejVceP9U3IZGiNEI1cW91DrRke7mJnxHMlOPZKYeyUw9wZCZFKZ9ICnWxQ7izC9kyyghhBBCiHZJYdoHkqJD2Wb0M7+QTfaFEEIIIdolhakXNE0jNjYWTdO8etzA6FDPXqZNJfm90DJxIN3NTfiOZKYeyUw9kpl6giEzWZXvBYvFQkpKitePiw4LocgyAIC6XZsJ6emGiQPqbm7CdyQz9Uhm6pHM1BMMmcmIqRd0XaegoMDr1XCaplETau5l6i7N74WWiQPpbm7CdyQz9Uhm6pHM1BMMmUlh6gXDMCgtLe3WarhmVzIAtkrZy7SvHUxuwjckM/VIZuqRzNQTDJlJYdpHrHv2Mg2t3Q4B/JuOEEIIIUR3SWHaRyLik2k2LNiMJqgu8nVzhBBCCCH8jhSmXtA0jYSEhG6thkuMiWSH0bKXqWwZ1ZcOJjfhG5KZeiQz9Uhm6gmGzKQw9YLFYiEhIQGLxftvW1J0KIV7toySwrRvHUxuwjckM/VIZuqRzNQTDJkFbs96gdvtZtOmTbjdbq8fu+9eprqszO9TB5Ob8A3JTD2SmXokM/UEQ2ZSmHqpqqqqW49LiHKyFbMwbdid34MtEl3R3dyE70hm6pHM1COZqSfQM5PCtI+EWC1UOpIAaJYRUyGEEEKINqQw7UMte5laKmSOqRBCCCHE/qQw9YKmaSQnJ3d7NZwW07KX6Q7QA3d+iL852NxE35PM1COZqUcyU08wZCaFqRcsFgtxcXHdXg0X3m8gjYYVi9EMVTt6uHWiIwebm+h7kpl6JDP1SGbqCYbMutWzwsJCtm7d6vn6hx9+4KabbuK5557rsYb5I7fbzfr167u9Gm5gTATbjX7mF7JlVJ852NxE35PM1COZqUcyU08wZNatwvSiiy5i8eLFABQVFXHCCSfwww8/cMcdd3DPPff0aAP9TX19fbcfmxQdytaWwrRsSw+1SHTFweQmfEMyU49kph7JTD2Bnlm3CtNff/2VI488EoC3336bMWPGsHz5cl5//XVeeumlnmxfQEnaZy9TGTEVQgghhGitW4VpU1MTDocDgEWLFnH66acDMHLkSHbskLmTHRkYE0qh0R+QLaOEEEIIIfbXrcJ09OjR/Otf/+Lbb79l4cKFnHzyyQBs376duLi4Hm2gP7FYLKSlpXV70rHLGUKJbQAAjbs392TTxAEcbG6i70lm6pHM1COZqScYMutWzx588EGeffZZpk2bxoUXXsghhxwCwIcffuj5iD8QaZqGy+U6qG0aGiIGAWCpKOypZolO9ERuom9JZuqRzNQjmaknGDLrVmE6bdo0du/eze7du3nhhRc8x6+66ir+9a9/9Vjj/I3b7SYrK+vgVsNFpwBgr90B7uYeapk4kB7JTfQpyUw9kpl6JDP1BENm3SpM6+rqaGhoICYmBoAtW7bw2GOPkZOTQ//+/Xu0gf7mYH8YIvoNpMGwYTHcULmth1olOhPIb+JAJZmpRzJTj2SmnkDPrFuF6RlnnMErr7wCQHl5ORMmTOAf//gHZ555Js8880yPNjDQJMaEs82zZVS+T9sihBBCCOFPulWYrlq1ismTJwPw7rvvMmDAALZs2cIrr7zCE0880aMNDDQDo0PZZAw0v9i13reNEUIIIYTwI90qTGtra4mMjATgiy++4Oyzz8ZisXDUUUexZUvgbhxvsVgYMWLEQa2GGxgdyjoj2fxi56891DJxID2Rm+hbkpl6JDP1SGbqCYbMutWzoUOH8v7771NYWMjnn3/OiSeeCEBxcTEul6tHG+hv7Hb7QT0+KTqUdXoqAEZRdk80SXTBweYm+p5kph7JTD2SmXoCPbNuFaZ33XUXN998M4MHD+bII49k4sSJgDl6ethhh/VoA/2JrutkZWWh63q3n6N/pIMNmIUpxWtBD+xJzP6gJ3ITfUsyU49kph7JTD3BkJmtOw/6zW9+wzHHHMOOHTs8e5gCHH/88Zx11lk91rhAZLNaaIxMoa7eTmhznbkAKi7d180SQgghhPC5bhWmAAkJCSQkJLB161YABg0aFNCb6/eklPhIcgoGcaiWZ84zlcJUCCGEEKJ7H+Xrus4999xDVFQUqamppKamEh0dzb333hvQw8s9ZVSCi/W6udE+RbIASgghhBACujliescdd/Cf//yHBx54gEmTJgGwdOlS5syZQ319Pffdd1+PNtJfWCwWMjMzD3o13KhEF1nGnsJ0pyyA6m09lZvoO5KZeiQz9Uhm6gmGzLpVmL788sv8+9//5vTTT/ccGzt2LAMHDuTaa68N2MIUoLGxEafTeVDPMSrRxTt7ClNj568E7hVv/UdP5Cb6lmSmHslMPZKZegI9s26V3KWlpYwcObLN8ZEjR1JaWnrQjfJXuq6Tk5Nz0NMVhvaPYJNmFqZa+Raor+yJ5okO9FRuou9IZuqRzNQjmaknGDLrVmF6yCGH8NRTT7U5/tRTTzF27NiDblSgs9ssxMUnssOINQ8Ur/Ntg4QQQggh/EC3Psp/6KGHmDFjBosWLfLsYbpixQoKCwv55JNPerSBgSoj0cX6kmQSraXmyvyUCb5ukhBCCCGET3VrxHTq1Kls2LCBs846i/LycsrLyzn77LPJzs7m1Vdf9eq5tm3bxsUXX0xcXByhoaFkZmby448/dqdZfcJqtfbI84xKdLFeFkD1mZ7KTfQdyUw9kpl6JDP1BHpmmmEYRk892Zo1azj88MNxu7t2NaOysjIOO+wwjj32WK655hri4+PJzc0lPT2d9PTO9/asrKwkKiqKiooK5S6FujR3N2+/+ChP2J+G5KPgis993SQhhBBCiB7nTb3W7Q32e8KDDz5IcnIyL774oufYkCFDfNiiAzMMg6qqKiIjI9G0g1tLPyox0jNiauz8Fc0w4CCfU7SvJ3MTfUMyU49kph7JTD3BkJlPC9MPP/yQk046iXPPPZevv/7as93U7373u3bPb2hooKGhwfN1ZaW5mt3tdntGaTVNw2KxoOs6+w4GtxzffzS3o+MWiwVN01odd7vdbNq0iczMzDY/EC17iu2/Us5qtWIYRpvjcREOqsIH09hkxd5Yjbs0H6JTsFqtHba9N/p0oLZ726f22t7R8b7sU0tuo0ePJiQkJCD61JXjKvfJMAxPZvt+bKVynwIxp47eZ3a7PSD6tH8bA61PQIfvM1X7FIg57dun/d9nqvSpq5+kg48L07y8PJ555hn+9Kc/cfvtt7Ny5UpuvPFG7HY7s2fPbnP+vHnzmDt3bpvj2dnZREREABAbG0tKSgpbt25ttXVVyyVU8/Pzqaqq8hxPTk4mLi6O3Nxc6uvrPcfT0tJwuVysXbvW8w01DAO3242u66xdu7ZVGzIzM2lsbCQnJ8dzzGq1kpmZSVVVFXl5eZ7jTqeTkSNHMiQ+go3bBpGhbWHLD59gDD+Z9PR0iouLKSoq8pzfm30CGDFiBHa7naysrIPuU1lZGYWFhZ7jkZGRPu+TYRiUlpaSnZ3N2LFjA6JPgZjTvjIyMnC73WRnZ3t+CVS9T4GY0759anmfrVu3jkMOOSQg+hSIOe3bp4yMDJqamlq9z1TvUyDmtG+fWt5nGzduJCMjQ5k+ZWd3fS2NV3NMzz777APeX15eztdff93lythutzN+/HiWL1/uOXbjjTeycuVKVqxY0eb89kZMk5OTKS0t9cxZ6O0R0+zs7B4ZMbVarcz7ZB3DV9zMOdal6NNux5h8s1/+pqP6b6QtucmIqTp9MgyDrKwsGTFVqE/7vs9kxFSNPgH88ssvMmKqUJ/2f5+p0qfy8nJiY2N7fo5pVFRUp/f/9re/7fLzJSYmkpGR0erYqFGjeO+999o93+Fw4HA42hy3Wq1tVqm1fDPaO/dgjoeGhqJpmlfP09H5GUkuftVTwAqW4rWw55yO2t5bferO8Y765G3b+6pPoaGhWK3WVqNv+1OtT105rmqf3G63J7P971O1Twc6Hih9askMAqdP+wq0Ph3ofaZqnyDwcoLWfdr3faZynzriVWG67yKlnjBp0qRWQ9YAGzZsIDU1tUdfp6dYrdZ2r3jVXRmJLt71LIDKlkuT9pKezk30PslMPZKZeiQz9QRDZt3ax7Sn/PGPf+S7777j/vvvZ+PGjbzxxhs899xzXHfddb5sVod0XaekpKTdj0O6Y0i/cDZZBptflG6CxtoeeV7RWk/nJnqfZKYeyUw9kpl6giEznxamRxxxBAsWLODNN99kzJgx3HvvvTz22GPMmjXLl83qkGEYFBYWtpnP0V02q4W4/oPYbbjQDB12yaVJe0NP5yZ6n2SmHslMPZKZeoIhM5+uygc47bTTOO2003zdDJ8ZleRi/a5kjrFmm1eAGjjO100SQgghhPAJn46YCrk0qRBCCCFECylMvRQZGdmjzzcq0cU6fc9ir60/9uhzi716OjfR+yQz9Uhm6pHM1BPomfn8o3yVWK1W0tPTe/Q5RyW4WKqPMb/Y9iNUbIWoQT36GsGuN3ITvUsyU49kph7JTD3BkJmMmHpB13WKiop6dDVcVFgItuiBrNSHmwfWfdRjzy1MvZGb6F2SmXokM/VIZuoJhsykMPWCYRgUFRX1+Gq4UYkuPnFPML/Ifr9Hn1v0Xm6i90hm6pHM1COZqScYMpPC1A9kJEbyqftI84vC76Byu28bJIQQQgjhA1KY+oExA6MoIo5fLSPMA2s/9G2DhBBCCCF8QApTL2iaRmxsrOd66z1l0tB+OEMsLGg4wjyw9oMeff5g11u5id4jmalHMlOPZKaeYMhMClMvWCwWUlJSsFh69tsW7rAxbXj/vR/nF6yAqqIefY1g1lu5id4jmalHMlOPZKaeYMgscHvWC3Rdp6CgoFdWw506NpHt9GOtZThgyMf5Pag3cxO9QzJTj2SmHslMPcGQmRSmXjAMg9LS0l5ZDXfcyP44bBbmy8f5Pa43cxO9QzJTj2SmHslMPcGQmRSmfiLCYWPq8Pi9H+dvWQZVO33bKCGEEEKIPiSFqR+ZMTaRbcSzzjIMMGCdfJwvhBBCiOAhhakXNE0jISGh11bDHTeyP3abhQUN480D8nF+j+jt3ETPk8zUI5mpRzJTTzBkJoWpFywWCwkJCb22Gi7SGcKUYfF8ou+5CtSWZbI6vwf0dm6i50lm6pHM1COZqScYMgvcnvUCt9vNpk2bcLvdvfYap2YmsNXoz6/WUWDosPT/eu21gkVf5CZ6lmSmHslMPZKZeoIhMylMvVRVVdWrzz89YwAhVo15dWeaB358AcoLevU1g0Fv5yZ6nmSmHslMPZKZegI9MylM/YzLGcLkYfEs0zMpiBoP7kZY8oCvmyWEEEII0eukMPVDp2YmAvBI8/nmgTVvQvF6H7ZICCGEEKL3SWHqBU3TSE5O7vXVcCeMMj/O/7BkIJWDTzLnmi7+e6++ZiDrq9xEz5HM1COZqUcyU08wZCaFqRcsFgtxcXG9vhouKiyEEzMSAPindgGgwbqPYNtPvfq6gaqvchM9RzJTj2SmHslMPcGQWeD2rBe43W7Wr1/fJ6vhrpmWDsBz6x1Uj/yNefDLe3v9dQNRX+YmeoZkph7JTD2SmXqCITMpTL1UX1/fJ68zZmAUU4fHoxvwT84FSwjkLYbN3/TJ6weavspN9BzJTD2SmXokM/UEemZSmPqx644dCsC/s3RqMi8xD356GzQ3+rBVQgghhBC9QwpTP3bkkFiOGBxDo1vnWX4DYXFQnA3f/sPXTRNCCCGE6HFSmHrBYrGQlpbWp5OOW0ZNn19VRfXx88yD3z4CRVl91gbV+SI3cXAkM/VIZuqRzNQTDJkFbs96gaZpuFyuPt2mYerweEYnuahrcvPc7kNg5GmgN8P714K7qc/aoTJf5CYOjmSmHslMPZKZeoIhMylMveB2u8nKyurT1XCapnlGTV9asYXq6Q9CaAwU/QLLHuuzdqjMF7mJgyOZqUcyU49kpp5gyEwKUy/54ofhpNEJpMWHU1nfzKu/NsDJD5p3fP0QFK/r8/aoKJDfxIFKMlOPZKYeyUw9gZ6ZFKYKsFo0rp1mjpo++80mKoadBcNPBnfjno/0m33cQiGEEEKIgyeFqSLOPDSJYf0jKK9t4l/f5MFp/weOKNi+Cr572tfNE0IIIYQ4aFKYesFisTBixAifrIazWS385eSRALywdDM7jBg46T7zzsX3w+6Nfd4mVfgyN9E9kpl6JDP1SGbqCYbMArdnvcRut/vstaeP6s8Rg2NoaNZ5bGEuHHYxpB0LzfXw4fWg6z5rm7/zZW6ieyQz9Uhm6pHM1BPomUlh6gVd18nKykL3UQGoaRq3nWKOmr7zUyG5xdUw83EICYeCFbDy3z5pl7/zdW7Ce5KZeiQz9Uhm6gmGzKQwVcy41FhOzBiAbsCDn+VATCqcMNe8c9EcKNvi0/YJIYQQQnSXFKYK+svJI7FosGjdTlbml8L4KyDlaGiqgY9uBMPwdROFEEIIIbwmhamChvaP4PwjkgGY98k6DE2DM54CmxPylsCPL/i2gUIIIYQQ3aAZhrrDa5WVlURFRVFRUYHL5er11zMMA13XsVgsPr8c2M7KeqY+vJj6Jp2/zRjFlZPTYNkTsPBOsNjg4vmQNtWnbfQX/pSb6BrJTD2SmXokM/Wompk39ZqMmHqpsbHR100AYIDLye2njgLggU/X82N+KUy8HkafDXoz/PcS2JXj41b6D3/JTXSdZKYeyUw9kpl6Aj0zKUy9oOs6OTk5frMa7pKjUpl5SBLNusH1b/xMSW0TnPkMJE+Ahgp4/TdQXezrZvqcv+UmOieZqUcyU49kpp5gyEwKU4Vpmsa8szNJjw+nqLKem/67GrfVARe8CTFDoLwA3rwAGmt93VQhhBBCiE5JYaq4CIeNZy4eR2iIlW9zd/PEl7kQHgez3oXQGNj2Eyy4SjbfF0IIIYTfk8LUS1ar1ddNaGP4gEjuO2sMAE98lcuHa7ZDv6FwwRtgtcO6j2DxfT5upW/5Y27iwCQz9Uhm6pHM1BPomcmq/ABy+4Is3vi+AIAzD01izumjic6dDwuuNk84+98w9lwftlAIIYQQwUZW5fcSwzCorKzEX2v5uaeP5ppp6Vg0eH/1dk78v2/40n4sTLrJPOGD62DrTz5toy/4e26iLclMPZKZeiQz9QRDZlKYekHXdfLy8vx2NVyI1cKtJ4/kvWuOJi0+nOKqBq54+UdurzwbY/jJ4G6Aty6Cyu2+bmqf8vfcRFuSmXokM/VIZuoJhsykMA1Ah6XE8MmNk7lqShqaBm+s3MqCIXOhfwZUF8GbF8pKfSGEEEL4HSlMA5QzxMrtp47i9lPMTfjvWVhA+ZmvQlgc7FgN710J7mbfNlIIIYQQYh9SmHrJ6XT6ugleuWzSYEYluiivbeL+5TVw/utgdUDOx/C/P0AAz1PZl2q5CclMRZKZeiQz9QR6ZrIqPwj8tKWMc55ZDsDbV0/kyIYV8N+LwdBh0h/ghHt83EIhhBBCBCpZld9LdF2npKREuUnH41JjuPDIZAD+9n4WjUNPgdOfNO9c9jgsfcx3jesDquYWzCQz9Uhm6pHM1BMMmUlh6gXDMCgsLFRym4ZbTx5JbLidDTur+c/SzXDYxXDCveadi+6GVa/4toG9SOXcgpVkph7JTD2SmXqCITMpTINEdJid2081F0I9/uUGCktrYdKN5kf5AB/eAF8/HDRzToUQQgjhf6QwDSLnHD6QCUNiqW/Suem/q2ls1mH6XDjqWvOExX+H966QraSEEEII4RNSmHopMjLS103oNk3TePCcsUQ6bfy0pYy7P8wGTYOT58HMx8Fig1/fgxdPgYptvm5uj1I5t2AlmalHMlOPZKaeQM9MVuUHocXri7n85ZUYBtx31hhmTUg178hfBm9fArUlEDEALnwLBh7u28YKIYQQQmmyKr+X6LpOUVGR8qvhjh3Zn1tOGgHAnA+zWZlfat4xeBL87qs9V4jaCS/PhM3f+LClPSNQcgsmkpl6JDP1SGbqCYbMpDD1gmEYFBUVBcRquGumpjMjM5Emt8E1r61iR0WdeUfMYLjiCxgyBRqr4bXfwLr/+bStByuQcgsWkpl6JDP1SGbqCYbMpDANUpqm8fC5YxmZEMnu6gaufPlHdlc3mHc6IuGid2DkaeBuMD/e//l13zZYCCGEEAFPCtMgFma38fxvxxMTFkL29krOfHoZOUVV5p0hTjj3ZTh0zxWiPrgWlj8p20kJIYQQotdIYeoFTdOIjY1F0zRfN6XHJMeG8c7vjyY1LoytZXWc88xyFq8vNu+02uCMp2Di9ebXX/wNPrkF3M2+a3A3BGJugU4yU49kph7JTD3BkJmsyhcAlNU0cs3rP/FdXikWDe6YkcHlkwabP/yGYY6WLrwLMGDoCXDui+ZH/kIIIYQQByCr8nuJrusUFBQE5Gq4mHA7r1w+gQuOSEY34N7/reXpxRvNOzXNvErUea+ALRQ2LoQXToaKrb5tdBcFcm6BSjJTj2SmHslMPcGQmRSmXjAMg9LS0oBdDWe3WZh3dia3nTISgH8s3MBX63fuPSHjdLjsY3OP052/wvPHwcYvfdTargv03AKRZKYeyUw9kpl6giEzKUxFK5qm8fup6VxyVCqGAX94czV5u6r3njBwHFz55d69Tl87G/57MZQX+q7RQgghhAgIUpiKdt15WgbjU2Ooamjm6ld/orphnwVP0cnmXqdHXQeaFdZ9BE8dAd/+A5obfNdoIYQQQihNClMvaJpGQkJCQK+Ga2G3WfjnxYczwOUgt7iam99e0/qjA0cknHw//P5bSJ0EzXXw5T3w7FTYleO7hrcjmHILFJKZeiQz9Uhm6gmGzGRVvjigVQVlnP/sCprcBn84fhg3TR/W9g1hGJD1Dnx+B9QUQ0g4zHwcxp7rm0YLIYQQwm/Iqvxe4na72bRpE26329dN6TOHp8RwzxljAHj8y1x++8IPFJTUtj5J02DseXDNchgyFZpqYP6V8NFN0FTf943eTzDmpjrJTD2SmXokM/UEQ2ZSmHqpqqrK103ocxcemcLfZozCYbPwbe5uTnzsa579ehPN7v22q4iIh0sWwJS/ABr89CK8cKJffLQfjLmpTjJTj2SmHslMPYGemU8L0zlz5qBpWqvbyJEjfdkk0YErJ6fx2U1TmJgWR32TzrxP13Pak0v56/xfmPfpOv65ZCOvf7+Fjbvr4Lg74OJ3ITQWdqyBfx0DXz8EzY2+7oYQQggh/JjN1w0YPXo0ixYt8nxts/m8SaIDQ/qF88bvJvDOT1u57+N1rC+qYn1R69/cXE4bX/55GvFDp8Pvl8L/boLcL2DxfZC9AE5/CgaN800HhBBCCOHXfF4F2mw2EhISunRuQ0MDDQ17tyOqrKwEzDkXLfMtNE3DYrGg63qrVeQtx/efl9HRcYvFgqZprY7rus6gQYM8r7n/+S3n7MtqtWIYRrvH929jR8d7s08HantHx88bn8yxI+L5NGsHpTWNVNQ1UVnfzA/5pRSW1vH3/2Xz6HmHQEQCnP8m1nXvY3z6F7TitRj/mY4x/kqYdhuW8Lg+6ZOu6wwcOBBd14MqJ5X7BDBo0KCA6lMg5tTR+yxQ+rR/GwOtT5qmdfg+U7VPgZjTvn3a/32mSp+8mRPr88I0NzeXpKQknE4nEydOZN68eaSkpLR77rx585g7d26b49nZ2URERAAQGxtLSkoKW7dupbS01HNOQkICCQkJ5Ofnt5qfkZycTFxcHLm5udTX712ok5aWhsvlYu3ata2+oSNGjAAgKyurVRsyMzNpbGwkJ2fvfEqr1UpmZiZVVVXk5eV5jjudTkaOHElZWRmFhXs3po+MjCQ9PZ3i4mKKioo8x/uiT3a73as+OYxGxoZVQNjePl169DjOeHopH6zZweExjRyS4DT7lPkbiiMyCPnyLmK3LkRb+RzuNW/BcXewNeEESsv3tr03+7Rt27agy0nlPoWHh5OdnR1QfQrEnPbvU1FRUcD1KRBzaumT3W5v9T4LhD4FYk7796mkpESpPu37M9YZn24X9emnn1JdXc2IESPYsWMHc+fOZdu2bfz6669ERka2Ob+9EdPk5GRKS0s92w/05m8FLavhhg1ru2VSsP72tv/xuz/4lZdXbGFwXBif3DAJR4i1dZ82f43lizvQitcCYMQNQ59+Dww7EfbMM+7pPrndbjZu3MjQoUMJCQnxuk+BmJO/98kwDHJzc0lPT8dqtQZEnwIxp47eZ3a7PSD6tH8bA61PABs2bGj3faZqnwIxp337tP/7TJU+lZeXExsb26Xtonw6YnrKKad4/j527FgmTJhAamoqb7/9NldccUWb8x0OBw6Ho81xq9Xa6k0Fe78Z7Z17MMcbGhrQNM2r5+no/I7a6O3xg+1Td4531Kc/nzSCT38tIr+klueW5nPT9OGt2z70OEhbCqtega/+jlaSi/W/F0LyBJj2V0ib1ittb2xsxGq1en6hCPac/L1PbrebhoaGdt/bqvbpQMcDpU8t7zMInD7tK9D6dKD3map9gsDLCVr3ad/3mcp96ohfbRcVHR3N8OHD2bhxo6+bIrrJ5Qzh7pmjAfjn4k3k7apue5LFCuMvgxt/hkk3gc0Jhd/Dq2fCi6fC5m/7tM1CCCGE8A9+VZhWV1ezadMmEhMTfd0UcRBOzUxg6vB4Gt06d37wa5uPGTycLjhhLvxhDUz4PVgdULAcXj4NXjsHyvL7tN1CCCGE8C2fFqY333wzX3/9Nfn5+SxfvpyzzjoLq9XKhRde6MtmdchisZCWltbhULgwaZrGvWeMwWGzsGxjCQ9/noOuH2Aqc2QCnPKgOYJ6xJVgCYGNi+Dpo2DZ4+BuPqj2SG7qkczUI5mpRzJTTzBk5tOebd26lQsvvJARI0Zw3nnnERcXx3fffUd8fLwvm9UhTdNwuVxtrxUv2kiJC+O2U8yLJfxzySZufOtn6ps62S4iaiDM+Adc+x0MngzNdbDwLnh+Gmz7qdttkdzUI5mpRzJTj2SmnmDIzKer8g9WZWUlUVFRXVrl1RPcbjdr164lIyPDq4m8weztHwu5Y0EWTW6Dw1Kiee6S8cRHtl3A1oZhwOrX4Yu/QV0ZoMHIGeac1OQjvGqD5KYeyUw9kpl6JDP1qJqZN/Va4I4F9xJvNokV5mb8r1w+gajQEH4uKOfMp5exYWcXrvOraXDYxXDdShh7PmDA+v/Bf6abC6Q2fGEWr10kualHMlOPZKYeyUw9gZ6ZFKai101Mj2PBtUczOC6MbeV1nPfsCnK7UpwCRMTD2c/BdT/AoReb80+3LIM3zoXnpkLuIq8KVCGEEEL4LylMRZ9Ii49gwbWTOGRQFOW1TVz8n+8pLK3t+hPEj4AznzZX8E+8HuwRsGMNvH4OvDQDCr7vvcYLIYQQok/IHFMvGIZBfX09TqczoCce96aymkbOf24FG3ZWkxoXxju/n0j/SKf3T1RTAksfhR+eB/eeq4ENPxmmz4H+o1qdKrmpRzJTj2SmHslMPapmJnNMe5Hdbvd1E5QWE27n1SsmkBwbypaSWn77nx+oqG3y/onC4+Ck++DGVXDYJaBZYMNn8MzR8MH1ULm91emSm3okM/VIZuqRzNQT6JlJYeoFXdfJyspq93rDousGuJy8dsUE4iMdrC+q4rcvfM8b3xewNHc3W0pqaHLrVDc0k1NUxVfrd/Lqinze+2krjc3tfN+jBsEZT5lzUEfNBEOHn1+FJw6HL++F6l2Sm4IkM/VIZuqRzNQTDJnZfN0AEZxS48J59YojOf/Z71iztYI1W7M892la++uZPssu4qmLDsNha2eLjH7D4PzXzLmmC+80L3H67SPw7T+wJB5KgisToi+ElAnmJVGFEEII4XdkxFT4zMgEF+/+fiKzJ6Zy7Ih40uPDcdgsnqI0KjSEjEQX00f1x2GzsHDtTq55bdWBN+pPmQCXfw7nvw5JhwEG2o6fSch5BetLp8DDQ82P+jcuAnc3phAIIYQQotfIiKnwqWEDIpl7xhjP17pusLumgTC7jQjH3h/Ppbm7ueLllXy1vpirX/2JZy8ZhzOkg5FPTYNRp5m3qiL03IVU/PQe0SWr0OpKzY/6f34VnNHmpv2Z58KQKTKSKoQQQviYrMr3gmEY6LqOxWJRajVcoFi+cTdXvPwjdU1uJg/rx3OXjCfU3nkx6ckNHa1gBWS/D+s+gprivSe5BsIhF8AhF0G/ob3XCdEl8l5Tj2SmHslMPapm5k29JoWpF1TdpiGQfJdXwuUvraS20U1qXBhXHDOE34wbRJi948H/dnPT3VDwHfz6Hvz6LtRX7H3AwPGQcTqMPA3i0nu5R6I98l5Tj2SmHslMPapmJttF9RJd18nJyQno1XD+7qi0OF6+/EhiwkLYUlLLXR9kM3HeVzzw6Xp2VNS1+5h2c7NYYfAkOO1R+PMGOPclGHaiue3Uth9h4V3w5OHwz6Nh8TwozeubDgpA3msqkszUI5mpJxgyk8JUKOeIwbEsvfU45p4+msFxYVTUNfGvrzdx7CNL+DG/1PsnDHHC6LNg1jvwp3Vw6iMwZCpoVijOhq8fMLefev1cyF0IAfwPghBCCOFLUpgKJYU7bMw+ejBf/nkaz10yjkMGRVHfpHPN66vYWVnf/SeOTIAjfwezP4RbNsKZz0D68YABuV/A678xR1KX/h+UbOqx/gghhBBCClOvWa2yctufWC0aJ45O4M2rjmJkQiS7qhq45rWfaGhuvaWU1WrFMAya3V6MdobFwqEXwSXz4YZVcNS14IiCss2waM6ej/onwld/h20/QdNBFMSiDXmvqUcyU49kpp5Az0wWP4mAsaWkhplPLqWyvpmLJqRw/1mZnvu+zd3F3/+3jm3ldfxuchpXTh5CuKMbu6U11kDWO5C9APKXgt7c+v6IBIhJhehUcx/VzN9ARP+D7JkQQgihLlmV30sMw6CqqorIyEilVsMFkyU5xVz20koMAx44O5MJaXHc9/FaFq0rbnVevwgHfzh+KBccmUKItZsfHNSVmXNO130EeUugobLtORYbDDsJDptlLq6yhnTvtYKMvNfUI5mpRzJTj6qZSWHaS9xuN1lZWWRmZgb8ULrKnl68kYc/zyHEar5pm9wGVg0umZjKIcnRPLYoly0ltQCkxoUx5/TRHDviIEc1DQNqS6E8H8q2mB/353wKW1fuPSesHyRPgMRDIHEsJIwFV5J5QQDRirzX1COZqUcyU4+qmXlTr8mVn0TAuXZaOllbK/gsuwiAqcP7ce5QC6dMGoXVauW0sUm89UMBj3+5kS0ltVz24kouPXowt50ysuOrSXVG0yA8zrwNHGcem/xnKF4Pq1+HNW+ZG/rnfGzeWjijod8w6Dcc4oZC/EhIOcqc3yqEEEIEGSlMRcDRNI1/nHcImcujGJ3kYvLQOLKysjz3h1gtXDJxMGcfPohHvsjhxWX5vLQ8n+/ySnjiwsMYPiCy5xrTfySceC8cfxcU/gA71kDRL+afu3KgvtwcVd13ZBXNHFEdMhXSppob/odG91ybhBBCCD8lhamXnE6nr5sguiDcYeO6Y81Li7rd7nZzC3fYuHvmaKYMj+eWd9awvqiKmU8u5a+njGTWUandn3vaHmuIuaH/4El7jzXVmVtO7d4AJRvNP4uyYNd6s3DdsQaWP2GeGxoLsWkQOwRi02FABgwYAzFDwBKYm2vIe009kpl6JDP1BHpmMsdUCGBXVQO3vLuGJTm7AEiJDeO6Y9M5+/BBPVugdkVVEWz+BvK+hs1fQ0Vhx+eGhEH/DHPe6sDDzWkE/YabV7YSQggh/IAsfuoluq5TVlZGTEwMlgAdpQpEXc3NMAxe+24Lj3+Zy+7qRgAGxYRy/bFDOWecDwrUFg3V5mKq0jwo3Qy7c2Hnr+bIanM7e6faI/YssNrnFjcMrOp8QCLvNfVIZuqRzNSjamay+KmXGIZBYWEh0dHRvm6K8EJXc9M0jUsmDuaccYN4/bsCnv1mE1vL6rhtfhbPfpPHX04awcljEvp+iw5HBCRkmrd9uZvNYrXoF9ixGratgu2robEatiwzby1sTnMHgNAY8+aMhogBe0daY9P9akqAvNfUI5mpRzJTTzBkJoWpEPsJs9v43ZQ0Lj4qlde/38K/vt7E5t01XPP6Kg5Lieb2U0dxxGA/WDVvtUH8cPOW+RvzmO42F1Vt/3nvIquiLLNYLc3r+LkcLrNIHXSEuaVV8pGyM4AQQog+J4WpEB0ItVu5cnIaFxyZwnPf5PH8N3n8XFDOuf9awSHJ0cRH2Il0hhDptBEdGkJGkovDU2PoH+nDiekW656FURnALPOYrptTAaqLzYsCtNwqCs0Cdscv5sUB8r81by36DYcBo8FqNy8UoFnMRVxxQ825rAljwR7mk24KIYQITFKYeikysge3EhJ95mByi3DY+NMJw7l4QgqPfZnLf1cWsqawvMPzk2NDGZ8aywkZAzjFFx/9789igbh089Yed7M5X3XbT7D1B3Nbq90b9t46olnNhVdJh0L/UeYerP1HQWRij1w0QN5r6pHM1COZqSfQM5PFT0J4aUtJDb9uq6SqvonK+iaq6pvZVdXA6sJycnZWse876qTRA5h39lhiw+2+a3B31JaaBWppHhhu0JvNaQLN9bBzLWz7Eap3tv9YZ5S52KrfsD0F8TCITjbntTqjzJtcmlUIIYKGrMrvJbquU1xcTP/+/ZVaDRfs+jK3qvomVheW823ubl5ctpkmt0F8pINHzj2EqcPje/W1+5RhQOV2c5S1KAt2rTOvctVSyHYmJMy8RGt4PwiPN2/RKeY816RD0cP7y3tNMfLvo3okM/Wompmsyu8lhmFQVFREfHwAFRhBoC9zi3SGMHlYPJOHxXP6IUn88b+ryS2uZvYLP3DxUSmMS41pNaI6OimKEQkKfiyjaRA10LxlnL73eHODuZ1VyUYoyd1zAYFcqNoB9ZXQWGWe11QLFQXmrb2nj0ggNHwIJA0zF2GFxph/hsVBRAJEDjB3FZCRV78h/z6qRzJTTzBkJoWpEL1kzMAoPrrhGB74dD0vLc/nte8KeO271oWYpsFlRw/h5pOGE2YPgLejzQEJY8xbe9zN5kKr+nJzukDNLnNRVk2xWcRu/xl2b0CrLiKqugh2rjjw64XGgj3c3A4rxAm2ULOIjUwwt8eKTADXQHMurCupR+a+CiGE6D0B8D+hEP7LGWJlzumjOW5kf15ZkU9Ds+65r77Jzcr8Ml5Ytpkv1hbxwNljOWZYPx+2tg9YbXtGPvdcYrU9jTW4t69m+88LGRjtxNJQYRaxdXsK2aqdZiGrN5vH6kq79tqhMeZesAMyzZFem9OcUhDiNC9KEN4Pwvub0wpsis0JFkKIACGFqRc0TSM2Ntb3q6yFV/whtynD45nSzhzTrzfs4vb5WWwtq+Pi/3zPOYcPYtqIeJKiQxkUE0p8hAOLxWy3YRg06+Y8AJ9dhaov2MPRUiZiWJJh0KD2N/7XdbMgrS6GpjporoOmenOKQF0pVO4wpw9UFUH5FnM6QV2ZeanXzd903obQmD1TBhLMXQYiE8zC1Wo3t+TSrOY0gpbLwcq/CX7xPhPekczUEwyZyeInIXysuqGZhz9bz8srtrS5L8Sq4bRZaXDrNLl1DANsFo3jRvbn/COSmTo8HlsgF6k9paneXKBV9Kt5Odea3eYOA0115q2xyjxWs8scifWGaxCMnGHeUiaaRXJduTldob5yz0KvPfNjHZFSxAohgo6syu8luq6zdetWBg0apNRquGCnSm4/5pfyxg8FbC2tY1t5HUWV9bj1A7894yMdnHP4IE7I6M/wAZFEOgNjMZDPMtN1c2S1eqd5qyraO/JaW7Jn26w9W2c11cLWleafXWW171nA1d+cNhCx59ayM0HLLgURe0ZoFSpiVXmfib0kM/Wompmsyu8lhmFQWlrKwIEDfd0U4QVVchs/OJbx+1zqtNmtU1zVQGOzTojNgt1q3nZU1vHuj1tZ8PM2dlU18K+vN/GvrzcBMDA6lJEJkWQOiuK3Ewert3/qHj7LzGKB8DjzNiCj8/Ob6iBvCaz/H+R8ahavAFYHhEabl3ptrjePN9WCu3FPobuj8+d2uCB2CMSmQ8xgsyBuqDRHYesrzHPC4yGipajtDzGp5vkR/fu8qFXlfSb2kszUEwyZSWEqhJ+yWS0kRYe2OR4VFsLfTsvgLyeP5Kv1xcxftZVftlZQVFnPtnJztPXL9cW8uCyfm08czkUTUrFa1Bl5U0pIKIw4xbzpbnM6gNNlHt9fY+3eBVzVu8wR2ZriPbsS7NozlWDPdIKaXWYRumONefOWPdIsamNSW4/MhsUB2p6LJuy52cMgMsnctSCivzmHdl/u5j3zauVnSAjR+6QwFUJRdpuFk8ckcPKYBADKaxvJKapifVEVb/5QwPqiKu78IJs3fyhk7hmjOWKf0diuqG5opqS6gdS48N5ofuCxWM39VTtiDzNvUYM6f66meijLNy9YULoJyraY0wCcUWbh63ABxj6F7G6oLjLPr9hqzpkt+sW8eUOzmlMIWq7y1VRnFrEh4eZVvPoN23slr6a6PfNoK6C+Ei0kjH61NrAXQuxg84IJzijvXl8IEfRkjqkXVL3iQrALxtya3Tpv/FDAI5/nUFlvLuaZmBbHlOHxTB7Wj4xEl2e1//7qm9y8siKfpxdvoqKuiRmZidx68khS4sL6rP3BmFmPaW4wC9nSTWaR2rJPbHXxnqkGmllEt+wu0FhtXsWragcYeqdP7xVntDlqGzMYolPNbbkMfe9Nb4LGmj23avNPZxREJZuFbXSKWchHDDD3rJWfhR4l7zP1qJqZLH4SQgBQUt3AI1/k8NbKwlZXnIoLt3NUehyZA6MYneRidFIUUaEhLPh5G49+kcP2ivpWzxNi1Zg9cTA3HDeMqLDAWGAl9uNu3jO9YJc5OhviNPd6tTnNgnZ3rnk1r925ZiHriDBHbp1R5q2hEsoL9t5a5tv2FM26d6FYaMyekeM9r22xmovWWm71FeZxV5K53ZdroLnlV0R/s8gNj2+9Q4JhgLvJ/FquJiZEj5PCtJe43W7y8/MZPHgwVqu18wcIvyC5Qf7uGhbnFLM0dzff5ZVQ09j2evaRDhtVDeboamKUkz+eMJzRSS4e+HQ93+buBiAqNISbTxrBrCNTOhxx7QmSmXraZNZQbRaoZfnmXrLlBeb0AM2yz81qFrj2cHM0NSTMnIdbXggVhebjKrZ1/SIK3rA6zELU3WROV2hhse258EKoWZRbQ8xj2p5RZpvDvC8kfM/0jHCz+I0atOeWbI7u2uzma1hDOp+fq+tmG/q4KJb3mXpUzUxW5feiqqoqXzdBdEOw5za4XziX9RvCZZOG0Niss7qwnJX5pazdXkn29gryS2qpamgmKjSE645N57cTB+MMMf/Re/WKCXy9YRf3f7yOnJ1V3Pn+r3y4ehvzzh7L0P4R3WqPYRidbhAd7JmpqFVmjghzZ4Ou7G7QGXfT3svXVhfvndvaskuB3mzughAaYxaFTpe5l2zLFIXKbeaWXy0LzRqrwd3Q/mu17H7QUHnw7W5htYNlT4Fr2XNxBt1tTrtorjenNIA5AhwRv3dUN7yf2Z+wWPNPR+Se7cqazBFuvYm9UzNsZrFvc+6zDVl/c+T7AOR9pp5Az0wKUyGCjN1m4cghsRw5ZO9iqKr6JjbvrmFwv3Bc7eyFOnV4PJPS43jtuy08/HkOK/PLOPXxb7nhuKFcPTUdu61rc51yd1bx3Dd5fPprEceN7M8tJ40gObbv5q4KRVlDzI/lXUk983yNtWaB2vLclhDzcrmGsc+FF2rNRWievWv37F/rbjAf37Tn1lBlFsAVW/feGvcrHNyN5q0zDRXmrWRjz/QTzGLXEWEWxzaHedOsoDdhaW5kVF01lsWa+T2wOfecs+fPlpHjlpsjcs9tzwI8q23P96IOmmr2LJbb70NYz6K9qH0W70WaI+Qtz2dz9Fx/hfKkMBVCEOkMYeyg6AOeY7NauHTSEE4YncAdC7JYkrOLfyzcwAvLNmO3WWh2GzS6ddy6QXp8BONSYzg8NYZxqTFsK6vj2a838eX6Ys/zfbhmO5/9WsRlkwZz7bFDiQqVuX2ij9jDwJ7ae8+vu81CtLlhb1HqKWybzJFOi21PsbhnHq+m7dlloXjPBR52mfN060qhttT8s6Fq7+Mstr0f/bc8d8uFH2r2bEfmbtxb7LZDA/ymJNQsgLZniofWevqExQYYZv8M3fwTw/y+7Vs8t0wJaZkeYgs1vx+N1ebUksYa83vWUiQ79hTJLSPZLa/V8ouKJWTPVAzLPnk2mX+3WPcW8C1TNizWffpg2WcKh2b+XbPu/eWg5XGatqeYN8w/Na3161ts+ywYdENzE/aaHVAZB/ZQ82fBGmI+t6bt833UlN3iTeaYekHXdcrKyoiJiVFqNVywk9x6nmEYfLhmO3M/WktpTRdGgvbQNDgpI4HTD03ite+2sHyTuUAmOiyEy44ewrjUGMYMdOFy2jyZaZpGUWU9m3fV4AoNYXSSK6CvE60qeZ/5GcMwpzxUF5sF2b6Fst4M1hB0LYSq2noio2OxGLo5WtzcYI4Kt1yut7l+T3HXUuDtmT7RUGUWafYwszi0h5vFobZf9k11ex5TsffWUG0+vrnOJ9+aoNFS2Gt7Cm6b3SzWW0bFQ5xw2ad9MmIti5+EEH2iuqGZjcXV2CwadpsFm0VDN2DtjkpWbSljVUEZ2dsrsVo0zjl8EL+bPIS0eHNeqmEYLMnZxf2frCO3uLrV8w6KCWVY/wh2VjaweXcNdU17F6ckx4YyIzOJ08YmSpEqhMrczWax29yAZ8Sw1XZiLSPB7cyl1TTzcU17phI01prTCRqq9xTQVWZRHRK2Z+pAuHlzN7UulBuqW0/V8Px9n3m8hr5nZHLPdAhLiHmspZBvrjcL/lbt3/NvVstoKJjP29xoFv4tjwM8I5wt5+vNe0fW9ea9CwVbikyMvb9gHKy7yvpkGzYpTHuJ2+0mNzeXYcOGKbUaLthJbr5Vt2cHgFB7+9/7ZrfOgp+3sSRnF79ur2BLSdtrz1stGskxoRRXNVC7z44CQ/qFc9WUNM4dNwibVUbpfEneZ+qRzNTTKjNN21NAN+5TEO9THOvufa7y1mye11S3tyhuboARJ/dJu2VVfi+qr6/v/CThdyQ33+moIG1hs1o4d3wy545PBqCiromsrWUsXZPLYSPTGDogkuSYMOw2C3WNbr5aX8z/ftnOV+uL2by7hr/Oz+L5b/O45cQRnDwmocMR1MZmnRV5JXy7YRcJUU5mTUjttG3CO/I+U49kph5PZhYLWBwBt3hMClMhhF+JCg1hYlocETXbyRzVv9VITqjdyoyxicwYm0hNQzNvrSzkqa9yydtVwzWvr+KQ5GjOHTeIcIcVu9WKw2ahqqGJL9cV83XOLs8+rQDPfZPHTdOHc954GW0VQgh/IYWpEEJJ4Q4bVxwzhPPGD+L5b/L499LNrCksZ01heYeP6RfhYNqIeL7LK2FrWR23L8ji30s7H20VQgjRN2SOqRcMw6CqqorIyEj5D0whkpt6upPZrqoG/v1tHpt21dDQ7KahWaehWUcDJqbHcULGAA4dFI3FotHQ7Ob17wp4avFGz64CoxJd/OH4oZyYkdDpVa103aC8rgmX0yajrXvI+0w9kpl6VM1MFj8JIUQXVNU38fw3efxn6WbPZVpHJkRyw3HDGJkYSf7uGjbvrmFLSS2FZbXsqmpgV1UDJTWNuHWDCIeNCUNiOXpoP44Z2o/hAyKU+s9CCCH6ghSmvcTtdrN27VoyMjJkBaNCJDf19HVm5bWN/GfpZl5alt9qHqq3ElxOfjcljVkTUjyXdA0W8j5Tj2SmHlUzk1X5vcjtdnd+kvA7kpt6+jKz6DA7fz5xBFcek8YLyzbz8op8Gpp0BvcLZ0i/MAbHhZMSG8YAl5P4SAfxkQ5iwuzkFFWxfNNulm0qYeXmUooq67n3f2t59utNXDstnQuOTMFm0VhVUM5X64v5av1ONu+uwWmzEmo3b2F2G5kDXRw7oj+ThvVr95KwqpD3mXokM/UEemZSmAohxB5RYSH88YTh3DR9GECnH8tnDooic1AUV09Np6HZzYJV23jyq41sK69jzkdreXrJJhqbdSrqmlo9rsnd3Gpkdt2OSt7+cSs2i8b4wTEcMiiaZt2gvsmcK+vWDQ5PjWFGZiKx4fae77gQQvgJKUyFEGI/3Zkn6rBZueDIFM4+fBBv/1jI04s3sqPC3G8wOiyEacPjOXZkfw5PiaHJrVPX5Ka+yU1ZTRMr8kpYnFNM3q4avssr5bu80jbPv+Dnbcz9MJspw+M549AkjhvZn8iDHF0tq2lENwziIgJrH0QhhLpkjqkXDMOgvr4ep9MpCxwUIrmpJxAya2h2882G3cSGh3BocgzWTlb6A+TvrmFJTjEFpXU4Qiw4bBYcNiuNzToL1xXx67bKVufHhttJinaSFBXKwJhQMhJdHJocTXp8xAF3FjAMg7dWFnLv/9bS0Kxz/Mj+XDQhhcnD4rvUzo6eU/XMgo1kph5VM5PFT73EMAx0XcdisSj1AxHsJDf1SGbt21hczYert/HBmu3tXrq1RYTDxthBURwxOJYZYxMZPiDSc9+uqgZue+8Xvlxf3OZxA6NDueCIZC6ZmEp0mHdTBiQz9Uhm6lE1MylMe4nb7SYrK4vMzEylVsMFO8lNPZJZ5yrrm9hWVsf2cvO2paSWX7ZVkLW1grqm1osjhvWPYMbYRAZGhzLv0/WU1jRit1q45aQRTB7ej7d+KGT+qq1U1pvzXiMdNq6YPITLjxnSajGWYRjkl9Ty67YKNA1CrBbsVgs2q0Z1fRO/bthMaEw85bXm81xwZApD+0e0abuuG3yctYNftpYzac9WW7IfbN+T95l6VM1MVuULIUSAczlDcCWGMCqx9T/yzW6d3OJqfi4o56v1O/lmw25yi6t5bFGu55xRiS7+7/xDGJlgPnbO6aO57ZSRfJK1g+e+yWN9URWPLcrlpeX5XDUljaHxEXyTu4tvNuymoLTjkVpTuedvLy7P54IjkvnD9GH0j3QC8F1eCfd/so5ftlYA8Py3m4mPdHD6IUmcddhARie5lBoJEkL0LClMhRAigNisFkYluhiV6OKiCSlU1DWxaO1OPsnawbodlZx+6ED+eMIwHLbWoy3OECtnHz6IMw8dyCe/7uD/Fm5g064aHvosp9V5IVaNzIFRhFgtNLl1mtwGTW4dZ4gFm7uB1IQ44iIcbCqu5sv1xbz+fQELft7GZZMGk1NUxaJ15hSCcLuV40cN4NvcXeyqauA/Szfzn6WbCbNbSXA56e9ykOByMjAmlEMGRXN4agz9ZJGWEAFPClMhhAhgUaEhnDNuEOeMG9Sl8y0WjdPGJnHKmEQ+WL2N577Jo6FZZ/KwfkwZFs/E9DjCHW3/62jvI8bv80q4/9P1rCks5+nFmwCwWjQuPDKZPxw/nPhIB43NOt9s2MWCn7excN1Oahvd5O2uIW93TZvXSI4N5fCUGKaNiOeEjAQi2mkHQG1jM6EhVhl5FUJBMsfUC6pOOg52kpt6JDP1dJSZYRh8klXEP5dsJDkmjJtPGtHuvFOA+iY3Oyrq2Vm597apuIafC8vILa5m3/+tnCEWpo8awJmHDmRUkouftpTxfV4J328uZWNxNTFhIYwfHMsRg2M4YnAsafER1DY2U1XfTFV9E5V1zWwrr6OwrJatZXVsLa2l0W2QGOUkMcpJUnQog2JCmTIsnpgA3TtW3mfqUTUzWfzUS1TdpiHYSW7qkczU09uZVdY3sbqgnB82l/K/X7aTf4BdCXqSw2bh9EOS+O3EwWQOivL68RW1TeQWV5EWH+F3F0eQ95l6VM1MCtNeoupquGAnualHMlNPX2ZmGAZZ2yr4YPV2PlqznV3VDWQkupgwJI4JabEclhLN1rI6fswv5YfNZfy4pZTy2iZCrBqRzhAiHDYinTYSo8xR0eTYMAbFhGK3WdhRXs+Oijq2l9eTvb2C9UVVntc9NDmaGZmJxEXYiQ4LISrUTlRoCA6bBbvNQsieHQpyd1bzzYZdfJu7i9WF5eh7/pdNjQvj0ORoDk2O5vCUGDKSXIQcYDeChmY3P+aXsSSnmCU5uyiqrOecwwdx7bHpnsVkB0PeZ+pRNTNZlS+EECJgaZrG2EHRjB0UzR2njqLRreMMaf2fdP9IJ4enxHDVFLOQbWjWcdi8+/jTMAxWFZTz6op8Ps7awerCclYXlnvd3n4RDnZXN7ClpJYtJbV8sHo7AKEhVg5LiWZ8qlmkltc2saNib2G8qqCM2sbWW3+9tDyf/64s5LdHp/L7KeltphkYhkFpTSM7KuopqqhnZ1U9yTFhHDkkts336GAUV9ZT3dBMWnz70zKE6C4pTIUQQijLYtFwWg5ccGma1q2iTNM0xqXGMC41hjtmZPD2j4WsL6qivLaR8tomyusaqahtonHP7gTuPUOjUaEhHDOsH1OG9eOYYfEMjA6loraJ1VvLWV1QzurCMlYVlFNR18TyTSUs31TSYRv6RTiYOjyeaSPiiXDaeHxRLqsLy3n26zxe/66AjEQXNY3N1DQ0U93gprK+icZmvc3zhIZYOTo9jmkjzAVsA1xOQm3efRTs1g2+2bCL178v4Kv1O9ENyBwYxawJKZx+aBJhdu9Kiia3ztrtlWgajE6K6vZVx0RgkcLUSyoNnYu9JDf1SGbqCeTM4iMdXHfs0AOe49bNrbPsVkubS8JGhYUwdXg8U4fHA+ZFBjbuqmZlfik/5pexaVc1ceF2EqJCSYxykhDlJCPRRUaiq9VzTRsez+KcYh75fANrd1TyQ35pu23pF+EgMcpJXISdtdsrKa5q4Mv1xa2u+OWwWYhyaCQureaIwbGccehAxgx0tVm8tmFnNV9kF/HWykK2ldd57rNZNLK2VXDb/Czu+3gdpx+axACXk7omN3WN5s1iMS+dGxvuIC7cTqjdyq/bKvgxv4yfC8uobzKL6JiwECYPMwvwKcPjD2prsPomN098mcuidTu5cnIa544b1OFIua4bB7x8r7cMw+CdH7fy6MINnJqZyJ2njTqouaClNY3EhIW0eo5Afp+BzDEVQgghlKPrBss3lVBR10S4w0q4w0a43Zw7O8DlxG7bO3fVMAzW7qjk6w27WJKzi1+3VbSZItAiLT6cMw8dyLD+EXyTu5uvc4rZXlHvuT8qNIRzDh/ERROSiQ138O5PhbzxfUG3F6NFh4Xg1g2q9lx1bN/jA6NDSYoOZWB0KBZNY1d1A8WV9eyqbqC+0c3xowZwycTUVpfcXZq7mzvez2p1yd5pI+KZd3YmiVGhnu/Hl+uKeXrJRtYUljM6KYpjhvVj8tB+jBsc02aP367aVdXAX+f/4tmrF+DmE4dz/XHDvH6uyvom5n64lvdWbWXysH48deHhRIWFdP5AP6Xk4qcHHniAv/71r/zhD3/gscce69JjfLEqv6qqisjISKVWwwU7yU09kpl6JDO11DY2s7uqgYLiMsoaNT7L3smitTtpaGcagMNmYWJ6HDPHJjFjbGKbaREtRfInv+7AMAxCQ2yE2i2E2W24dXPO6+7qBkprGqmqb2bYgAjGp5pbeaXHR+A2DH4uKPcs8lq7o9KrvhyVFsuFR6bwzYbdvLdqKwAJLienZiby2vdbaGzWiXTYuPO0DELtVp5evLHVorZ9OUMsnDY2iZtPHEFCVNsFZrpu8FNBGU1unfgIB/0iHESHhfB59k5uX5DludzvCaMH8PEvOwB4/IJDOePQgV3uz/KNu7n5nTWtfiEYHBfG878dz9D+EUq+z5QrTFeuXMl5552Hy+Xi2GOP9dvCVNXVcMFOclOPZKYeyUw9+2dWVd/E59k7+WD1NoorGzgqLZZpI/szMS2uRxdOdaaqvolt5XVsL69jW3k928rqMAyD+EgH8ZEO+kc6qW9289YPBSxcu9Oz6wGApsFvj0rl5pNGEOkMYWNxFX9+5xfW7LdoLdxu5eKJqZx92CB+3VbB0o27WbpxN7uqGgBzTu7vp6Zz1ZQ0Qu1W6hrdvLdqKy8s3dzm4g8hVo0mt9mIfS/3e9/Ha3n+283YrRZeu3ICRw6Jbbe/hmHQ5DaobWzmiS838sKyzQCkxIbxh+OH8ejCDWwrryPcbuXR8w5hQPNOMkaPYf3OapZu3M0vhRWE2q17pkzYiQu34wix0NCk09Cs09is06wbDHA5GBRj7kAxwOXs0zm9ShWm1dXVHH744fzzn//k73//O4ceeqgUpqJHSW7qkczUI5mpJxAy215ex5s/FPDflYX0i3Dw97PGcHhKTKtzmt06/166mUcXbiDMbuWyo4dw6dGD23w0bu7CUMa8T9bz45YyABKjnJyYMYAP12ynrLYJgEiHjXiXg91VDVTumYJg0eDqqencNH3v5X513eDa11fxWXYR0WEhvPv7o2ly654LQawqKKOqvpn6Jner4hrgwiNT+NuMUYQ7bJRUN3DdG6v4Lq8UTYOx/R3kV+pU1DV1+/tms2gkRYfy4fWTiA7r/f11lSpMZ8+eTWxsLP/3f//HtGnTDliYNjQ00NDQ4Pm6srKS5ORkSktLPR3VNA2LxYKu6+zbtZbjbnfreTUdHW+5qsK+x91uN9nZ2WRmZrYZQrdYzPk8ut76YxCr1eq5UsP+x/dvY0fHe7NPB2p7oPSpJbfRo0cTEhISEH3qynGV+2QYBllZWYwePbrVf5gq9ykQc+rofWa32wOiT/u3MdD6BPDLL7+0+z5TrU8tr22z2TrMqaymAbu19Q4NHfX10+ydzPtkfavFXoNiQrl80hDOOTzJcznchmad0ppGQu02okNbrye3WCzUN+lc+PwKVhdWoGlgdFJxJUU5uef0DI4d2b9VGxub3dz70Vpe/b7AczzSYeOotFjGD45BN8yFUqU1jZTVNtHY7MZps2Lfs8euRdPYWVXP1tI6tpXX0awb2G0W1s45EZvN2us5lZeXExsb6//7mL711lusWrWKlStXdun8efPmMXfu3DbHs7OziYgw91KLjY0lJSWFrVu3Ulq6d7ViQkICCQkJ5OfnU1W1d25JcnIycXFx5ObmUl+/dz5HWloaLpeLtWvXegIwDIOQkBB0XWft2rWt2pCZmUljYyM5OTmeY1arlczMTKqqqsjLy/McdzqdjBw5krKyMgoLCz3HIyMjSU9Pp7i4mKKiIs/x3uwTwIgRI7Db7WRlZQVknwzDoKKiguzsbMaOHRsQfQrEnPaVkZFBSEgI2dnZnl8CVe9TIOa0b59a3mfr1q3jkEMOCYg+BWJO+/YpIyMDi8XS6n2mep8OlJNeX01uF/t02tgURkQ08ur3hazf3ci0wWGcMW4wA5MS2bRpE5v371NEHOvXr2+3T38+MoKbS6vZWePGaTO3IDsqLY44vYzYUCt2q4bdqnH42DGgN7Nhwwaysna26lN9bQ3nDHEzwBpD/q5qjh7aj9OPOZSK8n1yit7bp6KionZzKigoYNfuEsrqdcrq3OzevatPcsrOzqarfDZiWlhYyPjx41m4cCFjx44F8PsR05bjENy/ZUufpE/SJ+mT9En6JH3qep8q6prYWlbH8AEROEJsAdGnrrS9OyOmPitM33//fc4666xWHx+43W7PN6OhoaHTOS99PcdU13XKysqIiYnxfLOF/5Pc1COZqUcyU49kph5VM/OmXvNZr44//niysrJYvXq15zZ+/HhmzZrF6tWr/XIitmEYFBYWtvntRPg3yU09kpl6JDP1SGbqCYbMfDbHNDIykjFjxrQ6Fh4eTlxcXJvjQgghhBAi8KkzDiyEEEIIIQKaT1fl72/JkiW+bkKnIiMjOz9J+B3JTT2SmXokM/VIZuoJ9Mx8vo/pwejrxU9CCCGEEMI7Six+UpGu6xQVFbW7SbHwX5KbeiQz9Uhm6pHM1BMMmUlh6gXDMCgqKgro1XCBSHJTj2SmHslMPZKZeoIhMylMhRBCCCGEX5DCVAghhBBC+AUpTL2gaRqxsbGeawoLNUhu6pHM1COZqUcyU08wZCar8oUQQgghRK+RVfm9RNd1CgoKAno1XCCS3NQjmalHMlOPZKaeYMhMClMvGIZBaWlpQK+GC0SSm3okM/VIZuqRzNQTDJlJYSqEEEIIIfyCX12S1FstvzFUVlb2yeu53W6qq6uprKzEarX2yWuKgye5qUcyU49kph7JTD2qZtZSp3VlpFfpwrSqqgqA5ORkH7dECCGEEEIcSFVVFVFRUQc8R+lV+bqus337diIjI/tk64TKykqSk5MpLCyUXQAUIrmpRzJTj2SmHslMPapmZhgGVVVVJCUlYbEceBap0iOmFouFQYMG9fnrulwupX4ghElyU49kph7JTD2SmXpUzKyzkdIWsvhJCCGEEEL4BSlMhRBCCCGEX5DC1AsOh4O7774bh8Ph66YIL0hu6pHM1COZqUcyU08wZKb04ichhBBCCBE4ZMRUCCGEEEL4BSlMhRBCCCGEX5DCVAghhBBC+AUpTIUQQgghhF+QwnQ/Tz/9NIMHD8bpdDJhwgR++OGHA57/zjvvMHLkSJxOJ5mZmXzyySd91FLRwpvMnn/+eSZPnkxMTAwxMTFMnz6904xF7/D2vdbirbfeQtM0zjzzzN5toGjD28zKy8u57rrrSExMxOFwMHz4cPk3so95m9ljjz3GiBEjCA0NJTk5mT/+8Y/U19f3UWvFN998w8yZM0lKSkLTNN5///1OH7NkyRIOP/xwHA4HQ4cO5aWXXur1dvYqQ3i89dZbht1uN1544QUjOzvb+N3vfmdER0cbO3fubPf8ZcuWGVar1XjooYeMtWvXGn/729+MkJAQIysrq49bHry8zeyiiy4ynn76aePnn3821q1bZ1x66aVGVFSUsXXr1j5ueXDzNrcWmzdvNgYOHGhMnjzZOOOMM/qmscIwDO8za2hoMMaPH2+ceuqpxtKlS43NmzcbS5YsMVavXt3HLQ9e3mb2+uuvGw6Hw3j99deNzZs3G59//rmRmJho/PGPf+zjlgevTz75xLjjjjuM+fPnG4CxYMGCA56fl5dnhIWFGX/605+MtWvXGk8++aRhtVqNzz77rG8a3AukMN3HkUceaVx33XWer91ut5GUlGTMmzev3fPPO+88Y8aMGa2OTZgwwbj66qt7tZ1iL28z219zc7MRGRlpvPzyy73VRNGO7uTW3NxsHH300ca///1vY/bs2VKY9jFvM3vmmWeMtLQ0o7Gxsa+aKPbjbWbXXXedcdxxx7U69qc//cmYNGlSr7ZTtK8rhelf/vIXY/To0a2OnX/++cZJJ53Uiy3rXfJR/h6NjY389NNPTJ8+3XPMYrEwffp0VqxY0e5jVqxY0ep8gJNOOqnD80XP6k5m+6utraWpqYnY2NjeaqbYT3dzu+eee+jfvz9XXHFFXzRT7KM7mX344YdMnDiR6667jgEDBjBmzBjuv/9+3G53XzU7qHUns6OPPpqffvrJ83F/Xl4en3zyCaeeemqftFl4LxDrEJuvG+Avdu/ejdvtZsCAAa2ODxgwgPXr17f7mKKionbPLyoq6rV2ir26k9n+br31VpKSktq8sUXv6U5uS5cu5T//+Q+rV6/ugxaK/XUns7y8PL766itmzZrFJ598wsaNG7n22mtpamri7rvv7otmB7XuZHbRRRexe/dujjnmGAzDoLm5md///vfcfvvtfdFk0Q0d1SGVlZXU1dURGhrqo5Z1n4yYiqD1wAMP8NZbb7FgwQKcTqevmyM6UFVVxSWXXMLzzz9Pv379fN0c0UW6rtO/f3+ee+45xo0bx/nnn88dd9zBv/71L183TXRgyZIl3H///fzzn/9k1apVzJ8/n48//ph7773X100TQURGTPfo168fVquVnTt3tjq+c+dOEhIS2n1MQkKCV+eLntWdzFo88sgjPPDAAyxatIixY8f2ZjPFfrzNbdOmTeTn5zNz5kzPMV3XAbDZbOTk5JCent67jQ5y3XmvJSYmEhISgtVq9RwbNWoURUVFNDY2Yrfbe7XNwa47md15551ccsklXHnllQBkZmZSU1PDVVddxR133IHFImNZ/qajOsTlcik5WgoyYupht9sZN24cX375peeYrut8+eWXTJw4sd3HTJw4sdX5AAsXLuzwfNGzupMZwEMPPcS9997LZ599xvjx4/uiqWIf3uY2cuRIsrKyWL16ted2+umnc+yxx7J69WqSk5P7svlBqTvvtUmTJrFx40bPLxEAGzZsIDExUYrSPtCdzGpra9sUny2/WBiG0XuNFd0WkHWIr1df+ZO33nrLcDgcxksvvWSsXbvWuOqqq4zo6GijqKjIMAzDuOSSS4zbbrvNc/6yZcsMm81mPPLII8a6deuMu+++W7aL6mPeZvbAAw8YdrvdePfdd40dO3Z4blVVVb7qQlDyNrf9yar8vudtZgUFBUZkZKRx/fXXGzk5Ocb//vc/o3///sbf//53X3Uh6Hib2d13321ERkYab775ppGXl2d88cUXRnp6unHeeef5qgtBp6qqyvj555+Nn3/+2QCMRx991Pj555+NLVu2GIZhGLfddptxySWXeM5v2S7qlltuMdatW2c8/fTTsl1UoHnyySeNlJQUw263G0ceeaTx3Xffee6bOnWqMXv27Fbnv/3228bw4cMNu91ujB492vj444/7uMXCm8xSU1MNoM3t7rvv7vuGBzlv32v7ksLUN7zNbPny5caECRMMh8NhpKWlGffdd5/R3Nzcx60Obt5k1tTUZMyZM8dIT083nE6nkZycbFx77bVGWVlZ3zc8SC1evLjd/6Nacpo9e7YxderUNo859NBDDbvdbqSlpRkvvvhin7e7J2mGIePzQgghhBDC92SOqRBCCCGE8AtSmAohhBBCCL8ghakQQgghhPALUpiK/2/n/mOqrv44jj8vfPn9yzQokMGMUNShXqhZAVsDxy9/gIgmYwMz/cMsc4q1TKlL4GpC6Vq1yu3i/DH6sTDnAnSmwMgxtS7SIIcEaouGLUvBSXL5fP/qs91EB9Xqmq/H9tnuOed9znnz+eOz9879XERERETcggpTEREREXELKkxFRERExC2oMBURERERt6DCVEREROQu1tTUxMKFC4mIiMBisXDgwIFxr9HQ0MAjjzxCUFAQoaGhLFmyhN7e3nGvo8JURGQMent7sVgsOByOfzsVEZG/1eDgILNnz+btt9/+U/N7enrIyckhNTUVh8NBQ0MDP/30E3l5eeNeS4WpiNw1LBbLba9XXnnl305RROQfl5WVRXl5OYsXLx51fGhoiJKSEiZPnkxAQABz587l+PHj5vjp06dxOp2Ul5cTExNDQkICJSUlOBwObty4Ma5c/vdX/hARkTtJX1+f+fnDDz+ktLSUs2fPmn2BgYH/RloiIm7tmWeeoaOjg5qaGiIiIqitrSUzM5P29nZiY2NJTEzEw8MDu93OihUrGBgYYM+ePcybNw8vL69x7aUTUxG5a9x///3mFRISgsViMdthYWG88cYbREZG4uPjw5w5c6ivr7/lWk6nk5UrVxIXF8eFCxcA+Oyzz0hISMDX15cHHngAm83G8PCwOcdisbBr1y4WL16Mv78/sbGxHDx40By/fPkyhYWFhIaG4ufnR2xsLHa7/ZY5fPLJJ8THx+Pn58ekSZOYN28eg4OD5viuXbuYPn06vr6+xMXF8c4777jMv3jxIsuWLWPChAlMnDiRnJwcl3fCVqxYQW5uLpWVlYSHhzNp0iTWrl077hMQEblzXbhwAbvdzscff0xKSgoxMTGUlJSQnJxsPp+mTJnC4cOH2bx5Mz4+PkyYMIHvv/+ejz76aNz7qTAVEQF27txJVVUVlZWVnDlzhoyMDBYtWkRXV9dNsUNDQyxduhSHw0FzczNRUVE0NzdTVFTEc889R0dHB++99x7V1dVUVFS4zLXZbCxbtowzZ86QnZ1NYWEhP//8MwBbt26lo6ODuro6Ojs7effdd7n33ntHzbevr4+CggJWrlxJZ2cnx48fJy8vD8MwANi3bx+lpaVUVFTQ2dnJtm3b2Lp1K7t37wbgxo0bZGRkEBQURHNzMy0tLQQGBpKZmclvv/1m7nPs2DG6u7s5duwYu3fvprq6murq6r/jlovIHaC9vR2n08nUqVMJDAw0r8bGRrq7uwH48ccfWb16NcXFxZw8eZLGxka8vb3Jz883n0ljZoiI3IXsdrsREhJitiMiIoyKigqXmIcffth4+umnDcMwjJ6eHgMwmpubjbS0NCM5Odn45ZdfzNi0tDRj27ZtLvP37NljhIeHm23A2LJli9keGBgwAKOurs4wDMNYuHCh8eSTT44p/9OnTxuA0dvbO+p4TEyMsX//fpe+V1991Xj00UfN3KZNm2aMjIyY40NDQ4afn5/R0NBgGIZhFBcXG9HR0cbw8LAZs3TpUuOJJ54YU44icucBjNraWrNdU1NjeHp6Gt9++63R1dXlcvX19RmGYRhbtmwxHnroIZd1Ll68aADGiRMnxrW/3jEVkbvelStX+OGHH0hKSnLpT0pKoq2tzaWvoKCAyMhIvvjiC/z8/Mz+trY2WlpaXE5InU4n169f59q1a/j7+wMwa9YsczwgIIDg4GD6+/sBWLNmDUuWLOGrr74iPT2d3NxcHnvssVFznj17NmlpacTHx5ORkUF6ejr5+fncc889DA4O0t3dzVNPPcXq1avNOcPDw4SEhJj5njt3jqCgIJd1r1+/bp6CAMycORNPT0+zHR4eTnt7+23upoj8l1itVpxOJ/39/aSkpIwac+3aNTw8XL+E//25MTIyMq79VJiKiIxDdnY2e/fu5cSJE6Smppr9AwMD2Gy2Uf89iq+vr/n5jz8EsFgs5oM7KyuL8+fP8/nnn3PkyBHS0tJYu3YtlZWVN63p6enJkSNH+PLLLzl8+DBvvfUWL730Eq2trWYR/MEHHzB37tyb5v2eb2JiIvv27btp7dDQ0DHlKyL/DQMDA5w7d85s9/T04HA4mDhxIlOnTqWwsJCioiKqqqqwWq1cunSJo0ePMmvWLObPn8/8+fN58803KSsro6CggKtXr7J582aio6OxWq3jykXvmIrIXS84OJiIiAhaWlpc+ltaWpgxY4ZL35o1a3jttddYtGgRjY2NZn9CQgJnz57lwQcfvOn640nC7YSGhlJcXMzevXvZsWMH77///i1jLRYLSUlJ2Gw2vv76a7y9vamtreW+++4jIiKC77777qZcpkyZYubb1dVFWFjYTTG/n6qKyN3h1KlTWK1Ws4jcsGEDVquV0tJSAOx2O0VFRWzcuJFp06aRm5vLyZMniYqKAiA1NZX9+/dz4MABrFYrmZmZ+Pj4UF9f7/LN0ljoxFREBNi0aRMvv/wyMTExzJkzB7vdjsPhGPVE8dlnn8XpdLJgwQLq6upITk6mtLSUBQsWEBUVRX5+Ph4eHrS1tfHNN99QXl4+phxKS0tJTExk5syZDA0NcejQIaZPnz5qbGtrK0ePHiU9PZ2wsDBaW1u5dOmSGW+z2Vi3bh0hISFkZmYyNDTEqVOnuHz5Mhs2bKCwsJDt27eTk5NDWVkZkZGRnD9/nk8//ZTnn3+eyMjIP38zReSO8vjjj9/2R0peXl7YbDZsNtstY5YvX87y5cv/ci4qTEVEgHXr1vHrr7+yceNG+vv7mTFjBgcPHiQ2NnbU+PXr1zMyMkJ2djb19fVkZGRw6NAhysrKeP311/Hy8iIuLo5Vq1aNOQdvb29efPFFent78fPzIyUlhZqamlFjg4ODaWpqYseOHVy5coXo6GiqqqrIysoCYNWqVfj7+7N9+3Y2bdpEQEAA8fHxrF+/HgB/f3+ampp44YUXyMvL4+rVq0yePJm0tDSCg4PHd/NERP4mFuN2JbKIiIiIyD9E75iKiIiIiFtQYSoiIiIibkGFqYiIiIi4BRWmIiIiIuIWVJiKiIiIiFtQYSoiIiIibkGFqYiIiIi4BRWmIiIiIuIWVJiKiIiIiFtQYSoiIiIibkGFqYiIiIi4hf8DW7X8Zy8QcnEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(train_losses, val_losses, tokens_seen_track):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(tokens_seen_track, train_losses, label=\"Train loss\", )\n",
    "    plt.plot(tokens_seen_track, val_losses, label=\"Validation loss\")\n",
    "    plt.xlabel(\"Tokens seen\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "# Example:\n",
    "plot_losses(train_losses, val_losses, tokens_seen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573baf2",
   "metadata": {},
   "source": [
    "Finetune On ColdPlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3cea514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in fine-tune train dataloader: 219\n",
      "Number of batches in fine-tune val dataloader: 25\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_model() got multiple values for argument 'settings'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     63\u001b[39m scheduler = CosineWithWarmup(\n\u001b[32m     64\u001b[39m     optimizer,\n\u001b[32m     65\u001b[39m     warmup_steps=settings_ft[\u001b[33m\"\u001b[39m\u001b[33mwarmup_steps\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m     min_lr=settings_ft[\u001b[33m\"\u001b[39m\u001b[33mmin_lr\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     69\u001b[39m )\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Fine-tune using the same training loop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m train_losses_ft, val_losses_ft, tokens_seen_ft = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader_ft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloader_ft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m=\u001b[49m\u001b[43msettings_ft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcheckpoints/gpt_512_512_8_8_finetuned_coldplay.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLook at the star look how they  \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     82\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFine-tuning complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: train_model() got multiple values for argument 'settings'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Coldplay data (from your earlier processing)\n",
    "# train_lyrics = joined_train_lyrics (first 90 songs)\n",
    "# test_lyrics = joined_test_lyrics (remaining songs)\n",
    "\n",
    "# Settings for fine-tuning (adjusted for smaller dataset and to avoid overfitting)\n",
    "settings_ft = {\n",
    "    \"learning_rate\": 1e-5,          # Lower LR for fine-tuning to preserve pretrained weights\n",
    "    \"weight_decay\": 0.01,           # Reduced weight decay\n",
    "    \"num_epochs\": 5,                # Fewer epochs since Coldplay dataset is small\n",
    "    \"batch_size\": 4,                # Smaller batch size for small dataset\n",
    "    \"warmup_steps\": 100,            # Shorter warmup\n",
    "    \"max_lr\": 1e-5,\n",
    "    \"min_lr\": 1e-6,\n",
    "    \"eval_freq\": 50,                # Evaluate more frequently\n",
    "    \"eval_iter\": 5,                 # Smaller eval batches\n",
    "    \"gradient_clip\": 0.5,           # gentler clipping\n",
    "    \"patience\": 3,                  # Earlier stopping if no improvement\n",
    "    \"min_improvement\": 1e-4,\n",
    "    \"print_interval\": 1,\n",
    "    \"generate_interval\": 2\n",
    "}\n",
    "\n",
    "# Create dataloaders for Coldplay (use smaller stride for more samples from small data)\n",
    "context_length = 128  # Same as pretraining\n",
    "train_dataloader_ft = create_encoded_dataloader(\n",
    "    joined_train_lyrics,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=settings_ft[\"batch_size\"],\n",
    "    max_length=context_length,\n",
    "    stride=32,  # Smaller stride for overlapping windows to increase effective samples\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_dataloader_ft = create_encoded_dataloader(\n",
    "    joined_test_lyrics,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=settings_ft[\"batch_size\"],\n",
    "    max_length=context_length,\n",
    "    stride=32,\n",
    "    shuffle=False,\n",
    "    drop_last=False  # Don't drop last for small val set\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in fine-tune train dataloader: {len(train_dataloader_ft)}\")\n",
    "print(f\"Number of batches in fine-tune val dataloader: {len(val_dataloader_ft)}\")\n",
    "\n",
    "# Reinitialize optimizer and scheduler for fine-tuning (don't load pretrained optimizer)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=settings_ft[\"learning_rate\"],\n",
    "    weight_decay=settings_ft[\"weight_decay\"],\n",
    "    betas=(0.9, 0.95)\n",
    ")\n",
    "\n",
    "total_steps_ft = settings_ft[\"num_epochs\"] * len(train_dataloader_ft)\n",
    "scheduler = CosineWithWarmup(\n",
    "    optimizer,\n",
    "    warmup_steps=settings_ft[\"warmup_steps\"],\n",
    "    total_steps=total_steps_ft,\n",
    "    base_lr=settings_ft[\"max_lr\"],\n",
    "    min_lr=settings_ft[\"min_lr\"]\n",
    ")\n",
    "\n",
    "# Fine-tune using the same training loop\n",
    "train_losses_ft, val_losses_ft, tokens_seen_ft = train_model(\n",
    "    model,\n",
    "    train_dataloader_ft,\n",
    "    val_dataloader_ft,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    settings=settings_ft,\n",
    "    context_length=context_length,\n",
    "    save_path=\"checkpoints/gpt_512_512_8_8_finetuned_coldplay.pt\",\n",
    "    sample_prompt=\"Look at the star look how they  \"\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afca470a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " thought it was actually entertaining. i am both gay, but by god it's not the dumbest weight of your life. I heard that of the family was the two previous movie were bad, and the story was that that I knew they had to handle out of a film. It was just the first time I'd have to worry but I can't move for a lot more interesting. This movie has some bad moments, but the directing, the characters were very very bad, and the don't be sure it's not that bad.<br /><br />The only thing I watched that was because I didn't give this movie\n"
     ]
    }
   ],
   "source": [
    "# Generate text using the trained model and the generate_text function\n",
    "start_context = \"I want something\"\n",
    "num_chars_to_generate = 500 # You can adjust this number\n",
    "generated_tokens = generate(\n",
    "    model=model,\n",
    "    max_new_tokens=num_chars_to_generate,\n",
    "    context=text_to_token_ids(start_context, tokenizer, device),\n",
    "    context_length=context_length,\n",
    "    temperature=0.8, # Adjust temperature for creativity\n",
    "    top_k=None # Or specify a top_k value\n",
    ")\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "decoded_text = token_ids_to_text(generated_tokens, tokenizer)\n",
    "\n",
    "print(decoded_text.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea6343",
   "metadata": {},
   "source": [
    "Character Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c596d24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'T', 1: '!', 2: '9', 3: 'N', 4: '|', 5: 'D', 6: ')', 7: ']', 8: 'Y', 9: '0', 10: '8', 11: 's', 12: 'R', 13: '^', 14: '?', 15: '~', 16: 'u', 17: '{', 18: '5', 19: '[', 20: 'k', 21: ':', 22: '\\x10', 23: 'G', 24: 'f', 25: 'S', 26: '(', 27: 'b', 28: 'E', 29: 'H', 30: 'h', 31: 'o', 32: ';', 33: 'p', 34: 'n', 35: 'X', 36: '<', 37: \"'\", 38: 'B', 39: 'q', 40: 't', 41: 'd', 42: '}', 43: '1', 44: '`', 45: 'r', 46: '\"', 47: 'V', 48: '#', 49: '7', 50: '2', 51: '$', 52: 'W', 53: '.', 54: 'z', 55: 'C', 56: 'I', 57: 'v', 58: 'K', 59: 'a', 60: 'c', 61: 'g', 62: 'U', 63: '6', 64: '\\n', 65: ' ', 66: '*', 67: '@', 68: 'J', 69: '%', 70: 'y', 71: 'x', 72: '=', 73: '\\\\', 74: '-', 75: 'L', 76: '+', 77: '4', 78: '3', 79: 'F', 80: '_', 81: 'O', 82: 'l', 83: '>', 84: '/', 85: '&', 86: 'Z', 87: 'Q', 88: 'w', 89: ',', 90: 'A', 91: 'm', 92: 'i', 93: 'e', 94: 'M', 95: 'j', 96: 'P'} {'T': 0, '!': 1, '9': 2, 'N': 3, '|': 4, 'D': 5, ')': 6, ']': 7, 'Y': 8, '0': 9, '8': 10, 's': 11, 'R': 12, '^': 13, '?': 14, '~': 15, 'u': 16, '{': 17, '5': 18, '[': 19, 'k': 20, ':': 21, '\\x10': 22, 'G': 23, 'f': 24, 'S': 25, '(': 26, 'b': 27, 'E': 28, 'H': 29, 'h': 30, 'o': 31, ';': 32, 'p': 33, 'n': 34, 'X': 35, '<': 36, \"'\": 37, 'B': 38, 'q': 39, 't': 40, 'd': 41, '}': 42, '1': 43, '`': 44, 'r': 45, '\"': 46, 'V': 47, '#': 48, '7': 49, '2': 50, '$': 51, 'W': 52, '.': 53, 'z': 54, 'C': 55, 'I': 56, 'v': 57, 'K': 58, 'a': 59, 'c': 60, 'g': 61, 'U': 62, '6': 63, '\\n': 64, ' ': 65, '*': 66, '@': 67, 'J': 68, '%': 69, 'y': 70, 'x': 71, '=': 72, '\\\\': 73, '-': 74, 'L': 75, '+': 76, '4': 77, '3': 78, 'F': 79, '_': 80, 'O': 81, 'l': 82, '>': 83, '/': 84, '&': 85, 'Z': 86, 'Q': 87, 'w': 88, ',': 89, 'A': 90, 'm': 91, 'i': 92, 'e': 93, 'M': 94, 'j': 95, 'P': 96}\t"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "import os\n",
    "\n",
    "full_text = train_text_data + '\\n' + test_text_data\n",
    "\n",
    "characters = list(set(full_text))\n",
    "\n",
    "int_to_char = {}\n",
    "for i, char in enumerate(characters):\n",
    "    int_to_char[i] = char\n",
    "\n",
    "char_to_int = {}\n",
    "\n",
    "for value, char  in int_to_char.items():\n",
    "    char_to_int[char] = value\n",
    "\n",
    "print(int_to_char,char_to_int,end=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebfdb83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13090602 12929994\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "encoded_train = [char_to_int[c] for c in train_text_data if c in char_to_int]\n",
    "\n",
    "encoded_test = [char_to_int[c] for c in test_text_data if c in char_to_int]\n",
    "\n",
    "print(len(encoded_train),len(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac78b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# context_length=3\n",
    "# encoded= [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\n",
    "\n",
    "def create_dataset(encoded, context_length=128):\n",
    "  inputs, targets = [], []\n",
    "  for i in range(len(encoded) - context_length):\n",
    "    inputs.append(encoded[i:i+context_length])\n",
    "    targets.append(encoded[i+1:i+context_length+1])\n",
    "  return torch.tensor(inputs), torch.tensor(targets)\n",
    "\n",
    "train_inputs, train_targets = create_dataset(encoded_train)\n",
    "test_inputs, test_targets = create_dataset(encoded_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    logits = model(input_batch)  # [B, T, V]\n",
    "    B, T, V = logits.shape\n",
    "    loss = criterion(logits.view(B*T, V), target_batch.view(B*T))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    total_loss = 0.0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    model.eval()\n",
    "    for i, (inp, tgt) in enumerate(data_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        loss = calc_loss_batch(inp, tgt, model, device)\n",
    "        total_loss += loss.item()\n",
    "    model.train()\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter=1):\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "    val_loss   = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "# ---------- Cosine LR with Warmup ----------\n",
    "class CosineWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, base_lr, min_lr, last_epoch=-1):\n",
    "        self.warmup_steps = max(1, warmup_steps)\n",
    "        self.total_steps = max(self.warmup_steps+1, total_steps)\n",
    "        self.base_lr = base_lr\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch + 1\n",
    "        lrs = []\n",
    "        for _ in self.base_lrs:\n",
    "            if step <= self.warmup_steps:\n",
    "                lr = self.base_lr * step / self.warmup_steps\n",
    "            else:\n",
    "                progress = (step - self.warmup_steps) / max(1, self.total_steps - self.warmup_steps)\n",
    "                lr = self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + math.cos(math.pi * progress))\n",
    "            lrs.append(lr)\n",
    "        return lrs\n",
    "\n",
    "\n",
    "# ---------- Training Loop ----------\n",
    "def train_model_char(\n",
    "    model,\n",
    "    train_inputs,\n",
    "    train_targets,\n",
    "    test_inputs,\n",
    "    test_targets,\n",
    "    device,\n",
    "    settings,\n",
    "    context_length,\n",
    "    save_path=\"checkpoints/char_gpt.pt\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Character-level GPT training loop using FP32.\n",
    "    \"\"\"\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    vocab_size = model.vocab_size  # assume defined in model\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=settings[\"learning_rate\"],\n",
    "        weight_decay=settings[\"weight_decay\"],\n",
    "        betas=(0.9, 0.95)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(train_inputs, train_targets),\n",
    "        batch_size=settings[\"batch_size\"], shuffle=True, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        TensorDataset(test_inputs, test_targets),\n",
    "        batch_size=settings[\"batch_size\"], shuffle=True, pin_memory=True\n",
    "    )\n",
    "\n",
    "    total_steps = settings[\"num_epochs\"] * len(train_loader)\n",
    "    scheduler = CosineWithWarmup(\n",
    "        optimizer,\n",
    "        warmup_steps=settings[\"warmup_steps\"],\n",
    "        total_steps=total_steps,\n",
    "        base_lr=settings[\"max_lr\"],\n",
    "        min_lr=settings[\"min_lr\"]\n",
    "    )\n",
    "\n",
    "    train_losses, val_losses, tokens_seen_track = [], [], []\n",
    "\n",
    "    tokens_seen = 0\n",
    "    global_step = -1\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(settings[\"num_epochs\"]):\n",
    "        for step, (inp, tgt) in enumerate(train_loader):\n",
    "            loss = calc_loss_batch(inp, tgt, model, device)\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), settings[\"gradient_clip\"])\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "            tokens_seen += inp.numel()\n",
    "\n",
    "            # periodic evaluation\n",
    "            if (global_step % settings[\"eval_freq\"]) == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device,\n",
    "                    eval_iter=settings[\"eval_iter\"]\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                tokens_seen_track.append(tokens_seen)\n",
    "                lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "                print(f\"Ep {epoch+1} | step {global_step:06d} | lr {lr_now:.3e} \"\n",
    "                      f\"| train {train_loss:.3f} | val {val_loss:.3f}\")\n",
    "\n",
    "                # early stopping\n",
    "                if val_loss + settings[\"min_improvement\"] < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    os.makedirs(os.path.dirname(save_path) or \".\", exist_ok=True)\n",
    "                    torch.save({\"model_state\": model.state_dict(),\n",
    "                                \"optimizer_state\": optimizer.state_dict(),\n",
    "                                \"epoch\": epoch,\n",
    "                                \"global_step\": global_step}, save_path)\n",
    "                    print(f\"[Checkpoint saved at step {global_step}]\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= settings[\"patience\"]:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        return train_losses, val_losses, tokens_seen_track\n",
    "\n",
    "    return train_losses, val_losses, tokens_seen_track\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca6df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 83          # GPT-2 tokenizer vocab size\n",
    "context_length = 256                     # Reduced context length for faster training\n",
    "embed_dim = 512                        # Smaller embedding dimension\n",
    "attention_dim = 512                     # Keep same as embed_dim\n",
    "num_heads = 8                           # Divisible by attention_dim\n",
    "num_blocks = 8                          # Fewer blocks for smaller model\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# Initial context (starting token or BOS)\n",
    "context = torch.zeros(1, 1, dtype=torch.int64).to(device)\n",
    "\n",
    "model = GPT(num_heads,vocab_size,embed_dim,attention_dim,num_blocks,context_length, dropout_rate).to(device)\n",
    "#model.load_state_dict(torch.load(\"/content/trained_model-128-85.pth\", weights_only=False))\n",
    "\n",
    "train_losses, val_losses, tokens_seen_track = train_model_char(\n",
    "    model,\n",
    "    train_inputs,\n",
    "    train_targets,\n",
    "    test_inputs,\n",
    "    test_targets,\n",
    "    device,\n",
    "    settings,\n",
    "    settings[\"context_length\"],\n",
    "    save_path=\"checkpoints/char_gpt.pt\",\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model_path = \"trained_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "\n",
    "from google.colab import files\n",
    "files.download('trained_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c045dc",
   "metadata": {},
   "source": [
    "Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5029ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1, epochs+1), losses, marker='o')\n",
    "plt.title('Epoch vs Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(1, epochs+1), accuracies, marker='o', color='green')\n",
    "plt.title('Epoch vs Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e19cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 10\n",
    "eval_freq = 50  # Evaluate every 50 batches\n",
    "eval_iter = 5   # Evaluate on 5 batches for training and validation loss calculation\n",
    "learning_rate = 0.001\n",
    "start_context = \"The\" # Starting text for generation\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start training\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_dataloader, test_dataloader, optimizer, device,\n",
    "    num_epochs, eval_freq, eval_iter, start_context, tokenizer, context_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecdc729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GROK CODE\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Assuming tokenizer is defined as tiktoken.get_encoding(\"gpt2\")\n",
    "# and device is defined, e.g., device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model, train_dataloader, test_dataloader are already defined as per your code\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device, non_blocking=True)\n",
    "    target_batch = target_batch.to(device, non_blocking=True)\n",
    "    logits = model(input_batch)\n",
    "    loss = F.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "# Adjusted generate_text to use tokenizer for decoding\n",
    "def generate_text(model, new_chars, context, context_length, tokenizer, temperature=1.0, top_k=None):\n",
    "    generated_tokens = []\n",
    "    for _ in range(new_chars):\n",
    "        if context.shape[1] > context_length:\n",
    "            context = context[:, -context_length:]\n",
    "        logits = model(context)[:, -1, :]\n",
    "        logits = logits / max(temperature, 1e-3)\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "            raise ValueError(\"Logits contain NaN or Inf\")\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        probabilities = torch.clamp(probabilities, min=1e-9, max=1.0)\n",
    "        next_token = torch.multinomial(probabilities, 1)\n",
    "        context = torch.cat((context, next_token), dim=1)\n",
    "        generated_tokens.append(next_token.item())\n",
    "    return tokenizer.decode(generated_tokens)\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context = text_to_token_ids(start_context, tokenizer).to(device, non_blocking=True)\n",
    "    context_length = model.positional_embedding.num_embeddings\n",
    "    generated = generate_text(\n",
    "        model=model,\n",
    "        new_chars=50,\n",
    "        context=context,\n",
    "        context_length=context_length,\n",
    "        tokenizer=tokenizer,\n",
    "        temperature=0.5,\n",
    "        top_k=10\n",
    "    )\n",
    "    print(start_context + generated)\n",
    "    model.train()\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen = 0\n",
    "    global_step = -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "# Example usage:\n",
    "optimizer = AdamW(model.parameters(),\n",
    "        lr=settings[\"learning_rate\"],\n",
    "        weight_decay=settings[\"weight_decay\"],\n",
    "        betas=(0.9, 0.95))\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_dataloader, test_dataloader, optimizer, device,\n",
    "    num_epochs=10, eval_freq=100, eval_iter=5,\n",
    "    start_context=\"Every movie is a\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74818ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def debug_model_and_data(model, train_dataloader, tokenizer, device):\n",
    "    \"\"\"Comprehensive debugging of model and data\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"DEBUGGING MODEL AND DATA\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. Check data\n",
    "    print(\"\\n1. DATA ANALYSIS:\")\n",
    "    for batch_idx, (input_batch, target_batch) in enumerate(train_dataloader):\n",
    "        if batch_idx == 0:  # Check first batch\n",
    "            print(f\"Input shape: {input_batch.shape}\")\n",
    "            print(f\"Target shape: {target_batch.shape}\")\n",
    "            print(f\"Input sample: {input_batch[0][:20]}\")  # First 20 tokens\n",
    "            print(f\"Target sample: {target_batch[0][:20]}\")\n",
    "\n",
    "            # Decode to check if data makes sense\n",
    "            sample_text = tokenizer.decode(input_batch[0][:50].tolist())\n",
    "            print(f\"Decoded sample: '{sample_text}'\")\n",
    "\n",
    "            # Check for data issues\n",
    "            print(f\"Input min/max: {input_batch.min()}/{input_batch.max()}\")\n",
    "            print(f\"Vocab size: {tokenizer.n_vocab}\")\n",
    "            if input_batch.max() >= tokenizer.n_vocab:\n",
    "                print(\"❌ ERROR: Token IDs exceed vocab size!\")\n",
    "                return False\n",
    "            break\n",
    "\n",
    "    # 2. Check model architecture\n",
    "    print(\"\\n2. MODEL ARCHITECTURE:\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    # 3. Check model forward pass\n",
    "    print(\"\\n3. FORWARD PASS CHECK:\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_input = input_batch[:1].to(device, non_blocking=True)  # Single sample\n",
    "        try:\n",
    "            output = model(sample_input)\n",
    "            print(f\"Model output shape: {output.shape}\")\n",
    "            print(f\"Output range: {output.min():.4f} to {output.max():.4f}\")\n",
    "\n",
    "            # Check for NaN/Inf\n",
    "            if torch.isnan(output).any():\n",
    "                print(\"❌ ERROR: Model output contains NaN!\")\n",
    "                return False\n",
    "            if torch.isinf(output).any():\n",
    "                print(\"❌ ERROR: Model output contains Inf!\")\n",
    "                return False\n",
    "            print(\"✅ Forward pass successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR in forward pass: {e}\")\n",
    "            return False\n",
    "\n",
    "    # 4. Check loss calculation\n",
    "    print(\"\\n4. LOSS CALCULATION:\")\n",
    "    model.train()\n",
    "    sample_input, sample_target = input_batch[:1].to(device, non_blocking=True), target_batch[:1].to(device, non_blocking=True)\n",
    "\n",
    "    # Manual loss calculation\n",
    "    logits = model(sample_input)\n",
    "    loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), sample_target.view(-1))\n",
    "    print(f\"Sample loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Check what random guessing would give\n",
    "    random_loss = np.log(tokenizer.n_vocab)\n",
    "    print(f\"Random guessing loss: {random_loss:.4f}\")\n",
    "\n",
    "    if loss.item() > random_loss + 1:\n",
    "        print(\"⚠️  WARNING: Loss much higher than random guessing!\")\n",
    "        print(\"This suggests the model isn't learning properly\")\n",
    "\n",
    "    # 5. Check gradients\n",
    "    print(\"\\n5. GRADIENT CHECK:\")\n",
    "    loss.backward()\n",
    "\n",
    "    grad_norms = []\n",
    "    zero_grads = 0\n",
    "    total_grads = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            grad_norms.append(grad_norm)\n",
    "            if grad_norm == 0:\n",
    "                zero_grads += 1\n",
    "            total_grads += 1\n",
    "\n",
    "    if grad_norms:\n",
    "        print(f\"Average gradient norm: {np.mean(grad_norms):.6f}\")\n",
    "        print(f\"Max gradient norm: {np.max(grad_norms):.6f}\")\n",
    "        print(f\"Min gradient norm: {np.min(grad_norms):.6f}\")\n",
    "        print(f\"Zero gradients: {zero_grads}/{total_grads}\")\n",
    "\n",
    "        if np.mean(grad_norms) < 1e-6:\n",
    "            print(\"❌ ERROR: Gradients too small (vanishing gradient problem)\")\n",
    "            return False\n",
    "        elif np.max(grad_norms) > 100:\n",
    "            print(\"❌ ERROR: Gradients too large (exploding gradient problem)\")\n",
    "            return False\n",
    "\n",
    "    print(\"✅ All checks passed!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_success = debug_model_and_data(model, train_dataloader, tokenizer, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
