{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca067d95",
   "metadata": {},
   "source": [
    "No Libraries, No Shortcuts: LLM from Scratch with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711034fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59592a",
   "metadata": {},
   "source": [
    "Self -Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b9cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, attention_dim, bias=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_key = nn.Linear(embed_dim, attention_dim, bias=bias)\n",
    "        self.w_query = nn.Linear(embed_dim, attention_dim, bias=bias)\n",
    "        self.w_value = nn.Linear(embed_dim, attention_dim, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.size()\n",
    "        \"\"\"\n",
    "        [\n",
    "            [write vector],\n",
    "            [me vector],\n",
    "            [a vector],\n",
    "            [poem vector],\n",
    "        ]\n",
    "        \"\"\"\n",
    "\n",
    "        k = self.w_key(x)   # (B, T, A)\n",
    "        q = self.w_query(x) # (B, T, A)\n",
    "        v = self.w_value(x) # (B, T, A)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = (q @ k.transpose(-2, -1)) / (k.size(-1) ** 0.5)  # (B, T, T)\n",
    "\n",
    "        # Causal mask (future positions masked)\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
    "        scores = scores.masked_fill(mask, float('-1e10'))\n",
    "\n",
    "        attn = scores.softmax(dim=-1)  # (B, T, T)\n",
    "\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        return attn @ v  # (B, T, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb330013",
   "metadata": {},
   "source": [
    "multiple attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77cc992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, attention_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.head_size = attention_dim//num_heads\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(SelfAttention(embed_dim=embed_dim, attention_dim=self.head_size,dropout=dropout))\n",
    "\n",
    "    def forward(self,x):\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head(x)) #B x T x A//num_heads\n",
    "        concatenated = torch.cat(head_outputs, dim = 2)\n",
    "        return concatenated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a5fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,attention_dim):\n",
    "        super().__init__()\n",
    "        self.up = nn.Linear(attention_dim,attention_dim*4)\n",
    "        self.relu = nn.GELU()\n",
    "        self.down = nn.Linear(attention_dim*4,attention_dim)\n",
    "    def forward(self,x):\n",
    "        return self.down(self.relu(self.up(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b83d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,num_heads,embed_dim,attention_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_multihead = MultiHeadAttention(num_heads, embed_dim, attention_dim, dropout)\n",
    "        self.feed_forward = FeedForward(attention_dim)\n",
    "        self.n1 = nn.LayerNorm(attention_dim)\n",
    "        self.n2 = nn.LayerNorm(attention_dim)\n",
    "    def forward(self,x):\n",
    "        e = self.masked_multihead(self.n1(x))\n",
    "        e =  e + x\n",
    "        e = self.feed_forward(self.n2(e))\n",
    "        return e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bc13de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, num_heads, vocab_size, embed_dim, attention_dim, num_blocks, context_length, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, attention_dim)\n",
    "        self.positional_embedding = nn.Embedding(context_length, attention_dim)\n",
    "\n",
    "        self.decoders = nn.ModuleList([\n",
    "            Decoder(num_heads, attention_dim, attention_dim, dropout_rate) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.exit_norm = nn.LayerNorm(attention_dim)\n",
    "        self.linear = nn.Linear(attention_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        embeddings = self.embedding(context)\n",
    "        context_len = context.shape[1]\n",
    "        position = torch.arange(context_len, device=context.device).unsqueeze(0)\n",
    "        position_embeddings = self.positional_embedding(position)\n",
    "\n",
    "        e = embeddings + position_embeddings\n",
    "\n",
    "        for decoder in self.decoders:\n",
    "            e = decoder(e)\n",
    "\n",
    "        return self.linear(self.exit_norm(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fb092c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: '*', 11: '+', 12: ',', 13: '-', 14: '.', 15: '/', 16: '0', 17: '1', 18: '2', 19: '3', 20: '4', 21: '5', 22: '6', 23: '7', 24: '8', 25: '9', 26: ':', 27: ';', 28: '?', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '{', 85: '|', 86: '}', 87: 'à', 88: 'á', 89: 'è', 90: 'é', 91: 'ë', 92: 'ñ', 93: 'ó', 94: 'ú', 95: '\\u2005', 96: '–', 97: '—', 98: '‘', 99: '’', 100: '“', 101: '”', 102: '…', 103: '\\u205f'}\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = float('-inf')\n",
    "    return out\n",
    "\n",
    "\n",
    "def generate_text(model, new_chars, context, context_length, int_to_char, temperature=1.0, top_k=None):\n",
    "    res = []\n",
    "    for _ in range(new_chars):\n",
    "        if context.shape[1] > context_length:\n",
    "            context = context[:, -context_length:]\n",
    "\n",
    "        logits = model(context)  # [B, T, V]\n",
    "        logits = logits[:, -1, :]  # [B, V]\n",
    "        logits = logits / max(temperature, 1e-3)\n",
    "\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "\n",
    "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "            raise ValueError(\"Logits contain NaN or Inf\")\n",
    "\n",
    "        probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "        probabilities = torch.clamp(probabilities, min=1e-9, max=1.0)\n",
    "\n",
    "        next_token = torch.multinomial(probabilities, 1)  # [B, 1]\n",
    "        context = torch.cat((context, next_token), dim=1)\n",
    "        res.append(int_to_char[next_token.item()])\n",
    "\n",
    "    return ''.join(res)\n",
    "\n",
    "def generate(model, max_new_tokens, context, context_length, temperature=1.0, top_k=None):\n",
    "    res = []\n",
    "    for _ in range(max_new_tokens):\n",
    "        if context.shape[1] > context_length:\n",
    "            context = context[:, -context_length:]\n",
    "\n",
    "        logits = model(context)  # [B, T, V]\n",
    "        logits = logits[:, -1, :]  # [B, V]\n",
    "        logits = logits / max(temperature, 1e-3)\n",
    "\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "\n",
    "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "            raise ValueError(\"Logits contain NaN or Inf\")\n",
    "\n",
    "        probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "        probabilities = torch.clamp(probabilities, min=1e-9, max=1.0)\n",
    "\n",
    "        next_token = torch.multinomial(probabilities, 1)  # [B, 1]\n",
    "        context = torch.cat((context, next_token), dim=1)\n",
    "\n",
    "\n",
    "    return context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3639633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = 104             # Number of tokens in your vocabulary\n",
    "# context_length = 128         # Maximum sequence length (tokens)\n",
    "# embed_dim = 256              # Token embedding size\n",
    "# attention_dim = 256          # Attention projection dimension (keep same as embed_dim)\n",
    "# num_heads = 4                # Number of attention heads (must divide attention_dim)\n",
    "# num_blocks = 6               # Number of decoder blocks (layers)\n",
    "# num_words = 5000\n",
    "# dropout_rate=0.1           # Number of new tokens to generate (optional tuning)\n",
    "\n",
    "# # Initial context (starting token or BOS)\n",
    "# context = torch.zeros(1, 1, dtype=torch.int64).to(device)\n",
    "\n",
    "# model = GPT(num_heads,vocab_size,embed_dim,attention_dim,num_blocks,context_length,dropout_rate).to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037b655",
   "metadata": {},
   "source": [
    "Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732a822",
   "metadata": {},
   "source": [
    "Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4dd762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from datasets) (2.3.5)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Collecting fsspec<=2025.10.0,>=2023.1.0 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets)\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting certifi (from httpx<1.0.0->datasets)\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1.0.0->datasets)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading typer_slim-0.20.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Downloading urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading datasets-4.4.2-py3-none-any.whl (512 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-1.2.3-py3-none-any.whl (520 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "Using cached aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "Using cached multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Using cached yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Using cached pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.20.1-py3-none-any.whl (47 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: xxhash, urllib3, tqdm, shellingham, pyyaml, pyarrow, propcache, multidict, idna, hf-xet, h11, fsspec, frozenlist, dill, click, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, typer-slim, requests, multiprocess, httpcore, anyio, aiosignal, httpx, aiohttp, huggingface-hub, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/30\u001b[0m [idna]ow]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.12.0━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/30\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling fsspec-2025.12.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/30\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.12.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/30\u001b[0m [idna]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30/30\u001b[0m [datasets]/30\u001b[0m [datasets]ce-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.12.0 attrs-25.4.0 certifi-2025.11.12 charset_normalizer-3.4.4 click-8.3.1 datasets-4.4.2 dill-0.4.0 frozenlist-1.8.0 fsspec-2025.10.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-1.2.3 idna-3.11 multidict-6.7.0 multiprocess-0.70.18 propcache-0.4.1 pyarrow-22.0.0 pyyaml-6.0.3 requests-2.32.5 shellingham-1.5.4 tqdm-4.67.1 typer-slim-0.20.1 urllib3-2.6.2 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f18035b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  100k  100  100k    0     0  85540      0  0:00:01  0:00:01 --:--:-- 85603\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_idx</th>\n",
       "      <th>row</th>\n",
       "      <th>truncated_cells</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{'text': 'Ive been reading books of old\n",
       "The le...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{'text': 'Come up to meet you, tell you Im sor...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>{'text': 'I used to rule the world\n",
       "Seas would ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>{'text': 'When you try your best, but you dont...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>{'text': 'Look at the stars\n",
       "Look how they shin...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_idx                                                row truncated_cells\n",
       "0        0  {'text': 'Ive been reading books of old\n",
       "The le...              []\n",
       "1        1  {'text': 'Come up to meet you, tell you Im sor...              []\n",
       "2        2  {'text': 'I used to rule the world\n",
       "Seas would ...              []\n",
       "3        3  {'text': 'When you try your best, but you dont...              []\n",
       "4        4  {'text': 'Look at the stars\n",
       "Look how they shin...              []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!curl -X GET \"https://datasets-server.huggingface.co/rows?dataset=huggingartists%2Fcoldplay&config=default&split=train&offset=0&length=100\" -o coldplay_data.json\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('coldplay_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data['rows'])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5227496c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ive been reading books of old\\nThe legends and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Come up to meet you, tell you Im sorry\\nYou do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I used to rule the world\\nSeas would rise when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>When you try your best, but you dont succeed\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Look at the stars\\nLook how they shine for you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_idx                                               text\n",
       "0        0  Ive been reading books of old\\nThe legends and...\n",
       "1        1  Come up to meet you, tell you Im sorry\\nYou do...\n",
       "2        2  I used to rule the world\\nSeas would rise when...\n",
       "3        3  When you try your best, but you dont succeed\\n...\n",
       "4        4  Look at the stars\\nLook how they shine for you..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.drop(columns=['truncated_cells'])\n",
    "df['text'] = df['row'].apply(lambda x: x['text'])\n",
    "df = df.drop(columns=['row'])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d890377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Select subsets\n",
    "train_subset = df.iloc[:90]\n",
    "test_subset = df.iloc[90:]\n",
    "\n",
    "# Extract and clean lyrics\n",
    "def keep_english_only(text):\n",
    "    return re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "\n",
    "# Process training lyrics\n",
    "train_lyrics = [keep_english_only(row[\"text\"]) for index, row in train_subset.iterrows()]\n",
    "joined_train_lyrics = '\\n'.join(train_lyrics)\n",
    "\n",
    "# Process test lyrics\n",
    "test_lyrics = [keep_english_only(row[\"text\"]) for index, row in test_subset.iterrows()]\n",
    "joined_test_lyrics = '\\n'.join(test_lyrics)\n",
    "\n",
    "full_lyrics = joined_train_lyrics + '\\n' + joined_test_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c317cfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ive been reading books of old\\nThe legends and the myths\\nAchilles and his gold\\nHercules and his gifts\\nSpider-Mans control\\nAnd Batman with his fists\\nAnd clearly I dont see myself upon that list\\nBut she '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_lyrics[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e391f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = list(set(full_lyrics))\n",
    "len(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535bbdda",
   "metadata": {},
   "source": [
    "Stanford IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2c43617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 292468.61 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 398017.08 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 309098.81 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train text length: 13090602 characters\n",
      "Test text length: 12929994 characters\n",
      "\n",
      "Train preview: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really h\n",
      "\n",
      "Test preview: I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, C\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "# Function to keep only English (ASCII) characters\n",
    "def keep_english_only(text):\n",
    "    return re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "\n",
    "# Function to clean and combine a list of texts\n",
    "def combine_and_clean(text_list):\n",
    "    # Keep only English\n",
    "    cleaned_list = [keep_english_only(t) for t in text_list]\n",
    "    # Combine into one string\n",
    "    combined = \" \".join(cleaned_list)\n",
    "    # Remove extra spaces/newlines\n",
    "    combined = re.sub(r'\\s+', ' ', combined).strip()\n",
    "    return combined\n",
    "\n",
    "# Create separate combined strings\n",
    "train_text_data = combine_and_clean(ds['train']['text'][:10000])\n",
    "test_text_data = combine_and_clean(ds['test']['text'][:10000])\n",
    "\n",
    "print(f\"Train text length: {len(train_text_data)} characters\")\n",
    "print(f\"Test text length: {len(test_text_data)} characters\")\n",
    "\n",
    "# Preview first 300 chars from each\n",
    "print(\"\\nTrain preview:\", train_text_data[:300])\n",
    "print(\"\\nTest preview:\", test_text_data[:300])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa4d652",
   "metadata": {},
   "source": [
    "Shakespeare Karpathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9be42250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import re\n",
    "\n",
    "# # Load dataset\n",
    "# ds = load_dataset(\"SamuelYang/bookcorpus\")\n",
    "\n",
    "# # Function to keep only English (ASCII) characters\n",
    "# def keep_english_only(text):\n",
    "#     return re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "\n",
    "# # Function to clean and combine a list of texts\n",
    "# def combine_and_clean(text_list):\n",
    "#     # Keep only English\n",
    "#     cleaned_list = [keep_english_only(t) for t in text_list]\n",
    "#     # Combine into one string\n",
    "#     combined = \" \".join(cleaned_list)\n",
    "#     # Remove extra spaces/newlines\n",
    "#     combined = re.sub(r'\\s+', ' ', combined).strip()\n",
    "#     return combined\n",
    "\n",
    "# # Since bookcorpus has only a train split, create synthetic validation and test splits\n",
    "# # Use 80% for train, 10% for validation, 10% for test\n",
    "# train_size = int(0.8 * len(ds['train']['text']))\n",
    "# val_size = int(0.1 * len(ds['train']['text']))\n",
    "# train_text = ds['train']['text'][:train_size]\n",
    "# validation_text = ds['train']['text'][train_size:train_size + val_size]\n",
    "# test_text = ds['train']['text'][train_size + val_size:train_size + 2 * val_size]\n",
    "\n",
    "# # Create combined strings for train, validation, and test\n",
    "# train_text_data = combine_and_clean(train_text)\n",
    "# validation_text_data = combine_and_clean(validation_text)\n",
    "# test_text_data = combine_and_clean(test_text)\n",
    "\n",
    "# # Print lengths\n",
    "# print(f\"Train text length: {len(train_text_data)} characters\")\n",
    "# print(f\"Validation text length: {len(validation_text_data)} characters\")\n",
    "# print(f\"Test text length: {len(test_text_data)} characters\")\n",
    "\n",
    "# # Preview first 300 chars from each\n",
    "# print(\"\\nTrain preview:\", train_text_data[:300])\n",
    "# print(\"\\nValidation preview:\", validation_text_data[:300])\n",
    "# print(\"\\nTest preview:\", test_text_data[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94b765",
   "metadata": {},
   "source": [
    "Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ffc1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "\n",
    "# # Alternatively:\n",
    "# # from llms_from_scratch.ch04 import generate_text_simple\n",
    "\n",
    "# def text_to_token_ids(text, tokenizer, device):\n",
    "#     encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "#     encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(device, non_blocking=True) # add batch dimension and move to device\n",
    "#     return encoded_tensor\n",
    "\n",
    "# def token_ids_to_text(token_ids, tokenizer):\n",
    "#     flat = token_ids.squeeze(0) # remove batch dimension\n",
    "#     return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "# start_context = \"I want something\"\n",
    "# tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6e959",
   "metadata": {},
   "source": [
    "Custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e5dcc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.venv/lib/python3.12/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n",
      "Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m36m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Installing collected packages: safetensors, regex, tiktoken, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub\n",
      "\u001b[2K    Found existing installation: huggingface_hub 1.2.3\n",
      "\u001b[2K    Uninstalling huggingface_hub-1.2.3:\n",
      "\u001b[2K      Successfully uninstalled huggingface_hub-1.2.3\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed huggingface-hub-0.36.0 regex-2025.11.3 safetensors-0.7.0 tiktoken-0.12.0 tokenizers-0.22.1 transformers-4.57.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61eae432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_training_corpus(batch_size=1000):\n",
    "    \"\"\"\n",
    "    Yields batches of raw IMDB reviews + Coldplay lyrics\n",
    "    for tokenizer training.\n",
    "    \"\"\"\n",
    "    # Combine IMDB train + test + Coldplay lyrics\n",
    "    texts = ds[\"train\"][\"text\"][:] + ds[\"test\"][\"text\"][:] + \\\n",
    "            [joined_train_lyrics, joined_test_lyrics]\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        yield [keep_english_only(t) for t in batch]  # clean before yielding\n",
    "\n",
    "\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", local_files_only=True)\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c043f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def text_to_token_ids(text, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Convert raw text → token IDs tensor (batch size = 1).\n",
    "    \"\"\"\n",
    "    encoded = tokenizer.encode(text, add_special_tokens=False)\n",
    "    encoded_tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0)  # (1, T)\n",
    "    return encoded_tensor.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert token IDs tensor → decoded text.\n",
    "    \"\"\"\n",
    "    flat = token_ids.squeeze(0).tolist()  # remove batch dimension\n",
    "    return tokenizer.decode(flat, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f31dc9",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36b52eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embedding): Embedding(5000, 128)\n",
       "  (positional_embedding): Embedding(128, 128)\n",
       "  (decoders): ModuleList(\n",
       "    (0-3): 4 x Decoder(\n",
       "      (masked_multihead): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-1): 2 x SelfAttention(\n",
       "            (w_key): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (w_query): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (w_value): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (up): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (relu): GELU(approximate='none')\n",
       "        (down): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (n1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (n2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (exit_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear): Linear(in_features=128, out_features=5000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size           # GPT-2 tokenizer vocab size\n",
    "context_length = 128                     # Reduced context length for faster training\n",
    "embed_dim = 128                        # Smaller embedding dimension\n",
    "attention_dim = 128                     # Keep same as embed_dim\n",
    "num_heads = 2                           # Divisible by attention_dim\n",
    "num_blocks = 4                          # Fewer blocks for smaller model\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Initial context (starting token or BOS)\n",
    "context = torch.zeros(1, 1, dtype=torch.int64).to(device, non_blocking=True)\n",
    "\n",
    "model = GPT(num_heads,vocab_size,embed_dim,attention_dim,num_blocks,context_length, dropout_rate).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92544513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Once upon a time in hollywood,don rarely socrough Naz rest memory commentsonedole\n"
     ]
    }
   ],
   "source": [
    "start_context=\"Once upon a time in hollywood,\"\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    context=text_to_token_ids(start_context, tokenizer, device),\n",
    "    max_new_tokens=10,\n",
    "    context_length=context_length\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407d51c",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "821c7486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embedding): Embedding(5000, 128)\n",
       "  (positional_embedding): Embedding(128, 128)\n",
       "  (decoders): ModuleList(\n",
       "    (0-3): 4 x Decoder(\n",
       "      (masked_multihead): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-1): 2 x SelfAttention(\n",
       "            (w_key): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (w_query): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (w_value): Linear(in_features=128, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (up): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (relu): GELU(approximate='none')\n",
       "        (down): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (n1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (n2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (exit_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear): Linear(in_features=128, out_features=5000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        torch.nn.init.ones_(module.weight)\n",
    "        torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "# Apply initialization\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "483e1f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3467525 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 13090602\n",
      "Tokens: 3467525\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(train_text_data)\n",
    "total_tokens = len(tokenizer.encode(train_text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)\n",
    "\n",
    "# Sanity check\n",
    "\n",
    "if total_tokens * (0.95) < context_length:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the context_length or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-0.95) <context_length:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the context_length or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac212cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "\n",
    "\n",
    "        # Use a sliding window to chunk the data into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_encoded_dataloader(txt, tokenizer, batch_size=4, max_length=128,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = CustomDataset(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8968ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(model, new_chars, context, context_length, temperature=1.0, top_k=None):\n",
    "#     res = []\n",
    "#     for _ in range(new_chars):\n",
    "#         if context.shape[1] > context_length:\n",
    "#             context = context[:, -context_length:]\n",
    "\n",
    "#         logits = model(context)  # [B, T, V]\n",
    "#         logits = logits[:, -1, :]  # [B, V]\n",
    "#         logits = logits / max(temperature, 1e-3)\n",
    "\n",
    "#         if top_k is not None:\n",
    "#             logits = top_k_logits(logits, top_k)\n",
    "\n",
    "#         if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "#             raise ValueError(\"Logits contain NaN or Inf\")\n",
    "\n",
    "#         probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "#         probabilities = torch.clamp(probabilities, min=1e-9, max=1.0)\n",
    "\n",
    "#         next_token = torch.multinomial(probabilities, 1)  # [B, 1]\n",
    "#         context = torch.cat((context, next_token), dim=1)\n",
    "#         res.append(next_token.item())\n",
    "\n",
    "#     return tokenizer.decode(res)\n",
    "\n",
    "# # Function to compute gradient norm and max gradient\n",
    "# def compute_gradient_stats(model):\n",
    "#     total_norm = 0.0\n",
    "#     max_grad = 0.0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is not None:\n",
    "#             param_norm = p.grad.data.norm(2).item()\n",
    "#             total_norm += param_norm ** 2\n",
    "#             max_grad = max(max_grad, p.grad.data.abs().max().item())\n",
    "#     total_norm = total_norm ** 0.5\n",
    "#     return total_norm, max_grad\n",
    "\n",
    "# # Loss calculation function for a single batch\n",
    "# def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "#     input_batch = input_batch.to(device, non_blocking=True)\n",
    "#     target_batch = target_batch.to(device, non_blocking=True)\n",
    "\n",
    "#     logits = model(input_batch)\n",
    "#     loss = torch.nn.functional.cross_entropy(\n",
    "#         logits.view(-1, vocab_size), target_batch.view(-1), ignore_index=-100\n",
    "#     )\n",
    "#     return loss\n",
    "\n",
    "# # Training loop for one epoch\n",
    "# def train_epoch(model, dataloader, optimizer, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     total_grad_norm = 0\n",
    "#     total_max_grad = 0\n",
    "#     num_batches = len(dataloader)\n",
    "\n",
    "#     for batch_idx, batch in enumerate(dataloader):\n",
    "#         loss = calc_loss_batch(batch[0], batch[1], model, device)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Compute gradient statistics\n",
    "#         grad_norm, max_grad = compute_gradient_stats(model)\n",
    "#         total_grad_norm += grad_norm\n",
    "#         total_max_grad = max(total_max_grad, max_grad)\n",
    "\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Print batch-level stats every 100 batches for debugging\n",
    "#         if (batch_idx + 1) % 100 == 0:\n",
    "#             print(f\"Batch {batch_idx + 1}/{num_batches}: \"\n",
    "#                   f\"Loss = {loss.item():.4f}, \"\n",
    "#                   f\"Grad Norm = {grad_norm:.4f}, \"\n",
    "#                   f\"Max Grad = {max_grad:.4f}\")\n",
    "\n",
    "#     avg_loss = total_loss / num_batches\n",
    "#     avg_grad_norm = total_grad_norm / num_batches\n",
    "#     avg_max_grad = total_max_grad / num_batches\n",
    "#     return avg_loss, avg_grad_norm, avg_max_grad\n",
    "\n",
    "# # Evaluation loop for one epoch\n",
    "# def eval_epoch(model, dataloader, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     num_batches = len(dataloader)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "#             loss = calc_loss_batch(batch[0], batch[1], model, device)\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#     return total_loss / num_batches\n",
    "\n",
    "# # Main training function with early stopping\n",
    "# def main(settings):\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=settings[\"learning_rate\"], weight_decay=settings[\"weight_decay\"], betas=(0.9, 0.95))\n",
    "\n",
    "#     best_val_loss = float('inf')\n",
    "#     patience_counter = 0\n",
    "#     best_model_path = \"gpt_model_best.pth\"\n",
    "\n",
    "#     for epoch in range(settings[\"num_epochs\"]):\n",
    "#         train_loss, train_grad_norm, train_max_grad = train_epoch(model, train_dataloader, optimizer, device)\n",
    "#         val_loss = eval_epoch(model, test_dataloader, device)\n",
    "\n",
    "#         if (epoch + 1) % settings[\"print_interval\"] == 0:\n",
    "#             print(f\"Epoch {epoch + 1}/{settings['num_epochs']}: \"\n",
    "#                   f\"Train Loss = {train_loss:.4f}, \"\n",
    "#                   f\"Val Loss = {val_loss:.4f}, \"\n",
    "#                   f\"Avg Grad Norm = {train_grad_norm:.4f}, \"\n",
    "#                   f\"Max Grad = {train_max_grad:.4f}, \"\n",
    "#                   f\"Learning Rate = {settings['learning_rate']:.6f}\")\n",
    "\n",
    "#         # Early stopping check\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "#             patience_counter = 0\n",
    "#             torch.save(model.state_dict(), best_model_path)\n",
    "#             print(f\"Saved best model with validation loss {best_val_loss:.4f}\")\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "#             print(f\"No improvement in validation loss. Patience counter: {patience_counter}/{settings['patience']}\")\n",
    "#             if patience_counter >= settings[\"patience\"]:\n",
    "#                 print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "#                 break\n",
    "\n",
    "#         if (epoch + 1) % settings[\"generate_interval\"] == 0:\n",
    "#             start_context = \"This is a test generation: \"\n",
    "#             context = text_to_token_ids(start_context, tokenizer, device)\n",
    "#             generated = generate_text(model, 100, context, context_length, temperature=0.7)\n",
    "#             print(f\"Generated text after epoch {epoch + 1}:\\n{start_context + generated}\\n\")\n",
    "\n",
    "#     # Load the best model\n",
    "#     model.load_state_dict(torch.load(best_model_path))\n",
    "#     print(f\"Training completed. Best model loaded from '{best_model_path}' with validation loss {best_val_loss:.4f}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7237bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# ---------- Loss helpers ----------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device, non_blocking=True)\n",
    "    target_batch = target_batch.to(device, non_blocking=True)\n",
    "\n",
    "    logits = model(input_batch)  # [B, T, V]\n",
    "    B, T, V = logits.shape\n",
    "    loss = criterion(logits.view(B * T, V), target_batch.view(B * T))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(data_loader) if num_batches is None else min(num_batches, len(data_loader))\n",
    "\n",
    "    for i, (inp, tgt) in enumerate(data_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        loss = calc_loss_batch(inp, tgt, model, device)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.train()\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter=1):\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "    val_loss   = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "# ---------- Cosine LR with Warmup ----------\n",
    "class CosineWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, base_lr, min_lr, last_epoch=-1):\n",
    "        self.warmup_steps = max(1, warmup_steps)\n",
    "        self.total_steps = max(self.warmup_steps + 1, total_steps)\n",
    "        self.base_lr = base_lr\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch + 1\n",
    "        lrs = []\n",
    "        for _ in self.base_lrs:\n",
    "            if step <= self.warmup_steps:\n",
    "                lr = self.base_lr * step / self.warmup_steps\n",
    "            else:\n",
    "                progress = (step - self.warmup_steps) / max(1, self.total_steps - self.warmup_steps)\n",
    "                lr = self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + math.cos(math.pi * progress))\n",
    "            lrs.append(lr)\n",
    "        return lrs\n",
    "\n",
    "\n",
    "# ---------- Training Loop ----------\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    settings,\n",
    "    save_path=\"checkpoints/gpt_256_256_8_8.pt\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Pure FP32 training loop — no autocast, no GradScaler.\n",
    "    \"\"\"\n",
    "\n",
    "    # Seeding\n",
    "    torch.manual_seed(123)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(123)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=settings[\"learning_rate\"],\n",
    "        weight_decay=settings[\"weight_decay\"],\n",
    "        betas=(0.9, 0.95),\n",
    "    )\n",
    "\n",
    "    total_steps = settings[\"num_epochs\"] * len(train_loader)\n",
    "    scheduler = CosineWithWarmup(\n",
    "        optimizer,\n",
    "        warmup_steps=settings[\"warmup_steps\"],\n",
    "        total_steps=total_steps,\n",
    "        base_lr=settings[\"max_lr\"],\n",
    "        min_lr=settings[\"min_lr\"],\n",
    "    )\n",
    "\n",
    "    train_losses, val_losses, tokens_seen_track = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    best_val_loss, patience_counter = float(\"inf\"), 0\n",
    "\n",
    "    for epoch in range(settings[\"num_epochs\"]):\n",
    "        model.train()  # ensure training mode at start of each epoch\n",
    "        for step, (inp, tgt) in enumerate(train_loader):\n",
    "            loss = calc_loss_batch(inp, tgt, model, device)\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), settings[\"gradient_clip\"])\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "            tokens_seen += inp.numel()\n",
    "\n",
    "            # evaluation\n",
    "            if global_step % settings[\"eval_freq\"] == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device,\n",
    "                    eval_iter=settings[\"eval_iter\"],\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                tokens_seen_track.append(tokens_seen)\n",
    "                lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "                print(f\"Ep {epoch+1} | step {global_step:06d} | lr {lr_now:.3e} \"\n",
    "                      f\"| train {train_loss:.3f} | val {val_loss:.3f}\")\n",
    "\n",
    "                # early stopping\n",
    "                if val_loss + settings[\"min_improvement\"] < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    os.makedirs(os.path.dirname(save_path) or \".\", exist_ok=True)\n",
    "                    torch.save({\n",
    "                        \"model_state\": model.state_dict(),\n",
    "                        \"optimizer_state\": optimizer.state_dict(),\n",
    "                        \"epoch\": epoch,\n",
    "                        \"global_step\": global_step,\n",
    "                    }, save_path)\n",
    "                    print(f\"[Checkpoint saved at step {global_step}]\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= settings[\"patience\"]:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        return train_losses, val_losses, tokens_seen_track\n",
    "\n",
    "    return train_losses, val_losses, tokens_seen_track\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from torch import nn\n",
    "\n",
    "# # ---------- Loss helpers ----------\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "#     input_batch = input_batch.to(device, non_blocking=True)\n",
    "#     target_batch = target_batch.to(device, non_blocking=True)\n",
    "\n",
    "#     logits = model(input_batch)  # [B, T, V]\n",
    "#     B, T, V = logits.shape\n",
    "#     loss = criterion(logits.view(B * T, V), target_batch.view(B * T))\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "#     if len(data_loader) == 0:\n",
    "#         return float(\"nan\")\n",
    "\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     num_batches = len(data_loader) if num_batches is None else min(num_batches, len(data_loader))\n",
    "\n",
    "#     for i, (inp, tgt) in enumerate(data_loader):\n",
    "#         if i >= num_batches:\n",
    "#             break\n",
    "#         loss = calc_loss_batch(inp, tgt, model, device)\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     model.train()\n",
    "#     return total_loss / num_batches\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def evaluate_model(model, train_loader, val_loader, device, eval_iter=1):\n",
    "#     train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "#     val_loss   = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "#     return train_loss, val_loss\n",
    "\n",
    "\n",
    "# # ---------- Training Loop ----------\n",
    "# def train_model(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     device,\n",
    "#     settings,\n",
    "#     save_path=\"checkpoints/gpt_256_256_8_8.pt\",\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Pure FP32 training loop — no autocast, no GradScaler.\n",
    "#     Uses CosineAnnealingWarmRestarts scheduler for periodic LR resets.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Seeding\n",
    "#     torch.manual_seed(123)\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.manual_seed_all(123)\n",
    "\n",
    "#     model.to(device)\n",
    "\n",
    "#     optimizer = torch.optim.AdamW(\n",
    "#         model.parameters(),\n",
    "#         lr=settings[\"max_lr\"],  # start with max LR\n",
    "#         weight_decay=settings[\"weight_decay\"],\n",
    "#         betas=(0.9, 0.95),\n",
    "#     )\n",
    "\n",
    "#     # CosineAnnealingWarmRestarts:\n",
    "#     # T_0 = number of epochs before the first restart\n",
    "#     # T_mult = factor to increase T_i after a restart\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "#         optimizer,\n",
    "#         T_0=settings.get(\"restart_period\", 10),   # default: restart every 10 epochs\n",
    "#         T_mult=settings.get(\"restart_mult\", 2),   # how restart period grows\n",
    "#         eta_min=settings[\"min_lr\"],               # minimum LR\n",
    "#     )\n",
    "\n",
    "#     train_losses, val_losses, tokens_seen_track = [], [], []\n",
    "#     tokens_seen, global_step = 0, -1\n",
    "#     best_val_loss, patience_counter = float(\"inf\"), 0\n",
    "\n",
    "#     for epoch in range(settings[\"num_epochs\"]):\n",
    "#         model.train()  # ensure training mode at start of each epoch\n",
    "#         for step, (inp, tgt) in enumerate(train_loader):\n",
    "#             loss = calc_loss_batch(inp, tgt, model, device)\n",
    "#             loss.backward()\n",
    "\n",
    "#             # gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), settings[\"gradient_clip\"])\n",
    "\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "#             # step scheduler with fractional epoch progress\n",
    "#             scheduler.step(epoch + step / len(train_loader))\n",
    "\n",
    "#             global_step += 1\n",
    "#             tokens_seen += inp.numel()\n",
    "\n",
    "#             # evaluation\n",
    "#             if global_step % settings[\"eval_freq\"] == 0:\n",
    "#                 train_loss, val_loss = evaluate_model(\n",
    "#                     model, train_loader, val_loader, device,\n",
    "#                     eval_iter=settings[\"eval_iter\"],\n",
    "#                 )\n",
    "#                 train_losses.append(train_loss)\n",
    "#                 val_losses.append(val_loss)\n",
    "#                 tokens_seen_track.append(tokens_seen)\n",
    "#                 lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "#                 print(f\"Ep {epoch+1} | step {global_step:06d} | lr {lr_now:.3e} \"\n",
    "#                       f\"| train {train_loss:.3f} | val {val_loss:.3f}\")\n",
    "\n",
    "#                 # early stopping\n",
    "#                 if val_loss + settings[\"min_improvement\"] < best_val_loss:\n",
    "#                     best_val_loss = val_loss\n",
    "#                     patience_counter = 0\n",
    "#                     os.makedirs(os.path.dirname(save_path) or \".\", exist_ok=True)\n",
    "#                     torch.save({\n",
    "#                         \"model_state\": model.state_dict(),\n",
    "#                         \"optimizer_state\": optimizer.state_dict(),\n",
    "#                         \"epoch\": epoch,\n",
    "#                         \"global_step\": global_step,\n",
    "#                     }, save_path)\n",
    "#                     print(f\"[Checkpoint saved at step {global_step}]\")\n",
    "#                 else:\n",
    "#                     patience_counter += 1\n",
    "#                     if patience_counter >= settings[\"patience\"]:\n",
    "#                         print(\"Early stopping triggered.\")\n",
    "#                         return train_losses, val_losses, tokens_seen_track\n",
    "\n",
    "#     return train_losses, val_losses, tokens_seen_track\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f3943b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in training dataloader: 846\n",
      "Number of batches in validation dataloader: 837\n",
      "Train loader:\n",
      "torch.Size([32, 128]) torch.Size([32, 128])\n",
      "\n",
      "test loader:\n",
      "torch.Size([32, 128]) torch.Size([32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/hackathons/52_Weeks_Challenges/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "settings = {\n",
    "    \"learning_rate\": 3e-4,          # Slightly higher start (AdamW with warmup will handle it)\n",
    "    \"weight_decay\": 0.1,            # Standard for GPT-style training\n",
    "    \"num_epochs\": 300,               # Enough for IMDb text size (~40M tokens) with small model\n",
    "    \"batch_size\": 32,               # Balanced for GPU memory vs convergence\n",
    "    \"warmup_steps\": 1500,            # Warmup helps avoid divergence early\n",
    "    \"max_lr\": 3e-4,                 # Same as base LR\n",
    "    \"min_lr\": 3e-5,                 # Don't decay too low (keeps learning alive)\n",
    "    \"eval_freq\": 200,               # Every ~200 steps = less overhead\n",
    "    \"eval_iter\": 20,                # Average over more batches for stability\n",
    "    \"gradient_clip\": 1.0,           # Safe norm clipping\n",
    "    \"patience\": 50,                 # Stop early if val loss stagnates\n",
    "    \"min_improvement\": 1e-4,\n",
    "    \"print_interval\": 1,            # Print progress every epoch\n",
    "    \"generate_interval\": 5          # Require meaningful improvement\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create training and validation dataloaders\n",
    "train_dataloader = create_encoded_dataloader(\n",
    "    train_text_data,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=settings[\"batch_size\"],\n",
    "    max_length=context_length,\n",
    "    stride=context_length,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_dataloader = create_encoded_dataloader(\n",
    "    test_text_data,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=settings[\"batch_size\"],\n",
    "    max_length=context_length,\n",
    "    stride=context_length,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in training dataloader: {len(train_dataloader)}\")\n",
    "print(f\"Number of batches in validation dataloader: {len(test_dataloader)}\")\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "\n",
    "print(\"\\ntest loader:\")\n",
    "for x, y in test_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3d2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 | step 000000 | lr 4.000e-07 | train 8.545 | val 8.545\n",
      "[Checkpoint saved at step 0]\n",
      "Ep 1 | step 000200 | lr 4.040e-05 | train 7.949 | val 7.953\n",
      "[Checkpoint saved at step 200]\n",
      "Ep 1 | step 000400 | lr 8.040e-05 | train 7.093 | val 7.095\n",
      "[Checkpoint saved at step 400]\n",
      "Ep 1 | step 000600 | lr 1.204e-04 | train 6.610 | val 6.646\n",
      "[Checkpoint saved at step 600]\n",
      "Ep 1 | step 000800 | lr 1.604e-04 | train 6.243 | val 6.304\n",
      "[Checkpoint saved at step 800]\n",
      "Ep 2 | step 001000 | lr 2.004e-04 | train 5.943 | val 5.998\n",
      "[Checkpoint saved at step 1000]\n",
      "Ep 2 | step 001200 | lr 2.404e-04 | train 5.743 | val 5.798\n",
      "[Checkpoint saved at step 1200]\n",
      "Ep 2 | step 001400 | lr 2.804e-04 | train 5.605 | val 5.669\n",
      "[Checkpoint saved at step 1400]\n",
      "Ep 2 | step 001600 | lr 3.000e-04 | train 5.487 | val 5.569\n",
      "[Checkpoint saved at step 1600]\n",
      "Ep 3 | step 001800 | lr 3.000e-04 | train 5.407 | val 5.482\n",
      "[Checkpoint saved at step 1800]\n",
      "Ep 3 | step 002000 | lr 3.000e-04 | train 5.365 | val 5.410\n",
      "[Checkpoint saved at step 2000]\n",
      "Ep 3 | step 002200 | lr 3.000e-04 | train 5.267 | val 5.336\n",
      "[Checkpoint saved at step 2200]\n",
      "Ep 3 | step 002400 | lr 3.000e-04 | train 5.190 | val 5.268\n",
      "[Checkpoint saved at step 2400]\n",
      "Ep 4 | step 002600 | lr 3.000e-04 | train 5.142 | val 5.221\n",
      "[Checkpoint saved at step 2600]\n",
      "Ep 4 | step 002800 | lr 3.000e-04 | train 5.069 | val 5.167\n",
      "[Checkpoint saved at step 2800]\n",
      "Ep 4 | step 003000 | lr 3.000e-04 | train 5.036 | val 5.124\n",
      "[Checkpoint saved at step 3000]\n",
      "Ep 4 | step 003200 | lr 3.000e-04 | train 5.043 | val 5.095\n",
      "[Checkpoint saved at step 3200]\n",
      "Ep 5 | step 003400 | lr 3.000e-04 | train 4.943 | val 5.050\n",
      "[Checkpoint saved at step 3400]\n",
      "Ep 5 | step 003600 | lr 3.000e-04 | train 4.901 | val 5.018\n",
      "[Checkpoint saved at step 3600]\n",
      "Ep 5 | step 003800 | lr 2.999e-04 | train 4.845 | val 4.987\n",
      "[Checkpoint saved at step 3800]\n",
      "Ep 5 | step 004000 | lr 2.999e-04 | train 4.839 | val 4.959\n",
      "[Checkpoint saved at step 4000]\n",
      "Ep 5 | step 004200 | lr 2.999e-04 | train 4.849 | val 4.928\n",
      "[Checkpoint saved at step 4200]\n",
      "Ep 6 | step 004400 | lr 2.999e-04 | train 4.792 | val 4.902\n",
      "[Checkpoint saved at step 4400]\n",
      "Ep 6 | step 004600 | lr 2.999e-04 | train 4.769 | val 4.874\n",
      "[Checkpoint saved at step 4600]\n",
      "Ep 6 | step 004800 | lr 2.999e-04 | train 4.713 | val 4.856\n",
      "[Checkpoint saved at step 4800]\n",
      "Ep 6 | step 005000 | lr 2.999e-04 | train 4.731 | val 4.830\n",
      "[Checkpoint saved at step 5000]\n",
      "Ep 7 | step 005200 | lr 2.999e-04 | train 4.705 | val 4.810\n",
      "[Checkpoint saved at step 5200]\n",
      "Ep 7 | step 005400 | lr 2.998e-04 | train 4.663 | val 4.791\n",
      "[Checkpoint saved at step 5400]\n",
      "Ep 7 | step 005600 | lr 2.998e-04 | train 4.634 | val 4.768\n",
      "[Checkpoint saved at step 5600]\n",
      "Ep 7 | step 005800 | lr 2.998e-04 | train 4.611 | val 4.755\n",
      "[Checkpoint saved at step 5800]\n",
      "Ep 8 | step 006000 | lr 2.998e-04 | train 4.598 | val 4.731\n",
      "[Checkpoint saved at step 6000]\n",
      "Ep 8 | step 006200 | lr 2.998e-04 | train 4.590 | val 4.726\n",
      "[Checkpoint saved at step 6200]\n",
      "Ep 8 | step 006400 | lr 2.997e-04 | train 4.550 | val 4.701\n",
      "[Checkpoint saved at step 6400]\n",
      "Ep 8 | step 006600 | lr 2.997e-04 | train 4.552 | val 4.688\n",
      "[Checkpoint saved at step 6600]\n",
      "Ep 9 | step 006800 | lr 2.997e-04 | train 4.506 | val 4.678\n",
      "[Checkpoint saved at step 6800]\n",
      "Ep 9 | step 007000 | lr 2.997e-04 | train 4.506 | val 4.668\n",
      "[Checkpoint saved at step 7000]\n",
      "Ep 9 | step 007200 | lr 2.997e-04 | train 4.513 | val 4.663\n",
      "[Checkpoint saved at step 7200]\n",
      "Ep 9 | step 007400 | lr 2.996e-04 | train 4.481 | val 4.646\n",
      "[Checkpoint saved at step 7400]\n",
      "Ep 9 | step 007600 | lr 2.996e-04 | train 4.473 | val 4.637\n",
      "[Checkpoint saved at step 7600]\n",
      "Ep 10 | step 007800 | lr 2.996e-04 | train 4.439 | val 4.624\n",
      "[Checkpoint saved at step 7800]\n",
      "Ep 10 | step 008000 | lr 2.996e-04 | train 4.432 | val 4.626\n",
      "Ep 10 | step 008200 | lr 2.995e-04 | train 4.418 | val 4.611\n",
      "[Checkpoint saved at step 8200]\n",
      "Ep 10 | step 008400 | lr 2.995e-04 | train 4.444 | val 4.594\n",
      "[Checkpoint saved at step 8400]\n",
      "Ep 11 | step 008600 | lr 2.995e-04 | train 4.416 | val 4.596\n",
      "Ep 11 | step 008800 | lr 2.994e-04 | train 4.380 | val 4.588\n",
      "[Checkpoint saved at step 8800]\n",
      "Ep 11 | step 009000 | lr 2.994e-04 | train 4.377 | val 4.569\n",
      "[Checkpoint saved at step 9000]\n",
      "Ep 11 | step 009200 | lr 2.994e-04 | train 4.375 | val 4.567\n",
      "[Checkpoint saved at step 9200]\n",
      "Ep 12 | step 009400 | lr 2.993e-04 | train 4.388 | val 4.564\n",
      "[Checkpoint saved at step 9400]\n",
      "Ep 12 | step 009600 | lr 2.993e-04 | train 4.363 | val 4.555\n",
      "[Checkpoint saved at step 9600]\n",
      "Ep 12 | step 009800 | lr 2.993e-04 | train 4.349 | val 4.543\n",
      "[Checkpoint saved at step 9800]\n",
      "Ep 12 | step 010000 | lr 2.992e-04 | train 4.354 | val 4.536\n",
      "[Checkpoint saved at step 10000]\n",
      "Ep 13 | step 010200 | lr 2.992e-04 | train 4.339 | val 4.534\n",
      "[Checkpoint saved at step 10200]\n",
      "Ep 13 | step 010400 | lr 2.992e-04 | train 4.308 | val 4.531\n",
      "[Checkpoint saved at step 10400]\n",
      "Ep 13 | step 010600 | lr 2.991e-04 | train 4.305 | val 4.522\n",
      "[Checkpoint saved at step 10600]\n",
      "Ep 13 | step 010800 | lr 2.991e-04 | train 4.321 | val 4.515\n",
      "[Checkpoint saved at step 10800]\n",
      "Ep 14 | step 011000 | lr 2.991e-04 | train 4.287 | val 4.505\n",
      "[Checkpoint saved at step 11000]\n",
      "Ep 14 | step 011200 | lr 2.990e-04 | train 4.262 | val 4.509\n",
      "Ep 14 | step 011400 | lr 2.990e-04 | train 4.272 | val 4.507\n",
      "Ep 14 | step 011600 | lr 2.989e-04 | train 4.269 | val 4.494\n",
      "[Checkpoint saved at step 11600]\n",
      "Ep 14 | step 011800 | lr 2.989e-04 | train 4.268 | val 4.487\n",
      "[Checkpoint saved at step 11800]\n",
      "Ep 15 | step 012000 | lr 2.988e-04 | train 4.241 | val 4.489\n",
      "Ep 15 | step 012200 | lr 2.988e-04 | train 4.254 | val 4.482\n",
      "[Checkpoint saved at step 12200]\n",
      "Ep 15 | step 012400 | lr 2.988e-04 | train 4.242 | val 4.470\n",
      "[Checkpoint saved at step 12400]\n",
      "Ep 15 | step 012600 | lr 2.987e-04 | train 4.234 | val 4.474\n",
      "Ep 16 | step 012800 | lr 2.987e-04 | train 4.239 | val 4.468\n",
      "[Checkpoint saved at step 12800]\n",
      "Ep 16 | step 013000 | lr 2.986e-04 | train 4.213 | val 4.462\n",
      "[Checkpoint saved at step 13000]\n",
      "Ep 16 | step 013200 | lr 2.986e-04 | train 4.211 | val 4.459\n",
      "[Checkpoint saved at step 13200]\n",
      "Ep 16 | step 013400 | lr 2.985e-04 | train 4.200 | val 4.453\n",
      "[Checkpoint saved at step 13400]\n"
     ]
    }
   ],
   "source": [
    "#main(settings)\n",
    "train_losses, val_losses, tokens_seen = train_model(\n",
    "    model, train_dataloader, test_dataloader, device,\n",
    "    settings,\n",
    "    save_path=\"checkpoints/gpt_128_128_8_8.pt\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc144fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(train_losses, val_losses, tokens_seen_track):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(tokens_seen_track, train_losses, label=\"Train loss\", )\n",
    "    plt.plot(tokens_seen_track, val_losses, label=\"Validation loss\")\n",
    "    plt.xlabel(\"Tokens seen\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "# Example:\n",
    "plot_losses(train_losses, val_losses, tokens_seen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573baf2",
   "metadata": {},
   "source": [
    "Finetune On ColdPlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cea514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Coldplay data (from your earlier processing)\n",
    "# train_lyrics = joined_train_lyrics (first 90 songs)\n",
    "# test_lyrics = joined_test_lyrics (remaining songs)\n",
    "\n",
    "# Settings for fine-tuning (adjusted for smaller dataset and to avoid overfitting)\n",
    "settings_ft = {\n",
    "    \"learning_rate\": 1e-5,          # Lower LR for fine-tuning to preserve pretrained weights\n",
    "    \"weight_decay\": 0.01,           # Reduced weight decay\n",
    "    \"num_epochs\": 5,                # Fewer epochs since Coldplay dataset is small\n",
    "    \"batch_size\": 4,                # Smaller batch size for small dataset\n",
    "    \"warmup_steps\": 100,            # Shorter warmup\n",
    "    \"max_lr\": 1e-5,\n",
    "    \"min_lr\": 1e-6,\n",
    "    \"eval_freq\": 50,                # Evaluate more frequently\n",
    "    \"eval_iter\": 5,                 # Smaller eval batches\n",
    "    \"gradient_clip\": 0.5,           # gentler clipping\n",
    "    \"patience\": 3,                  # Earlier stopping if no improvement\n",
    "    \"min_improvement\": 1e-4,\n",
    "    \"print_interval\": 1,\n",
    "    \"generate_interval\": 2\n",
    "}\n",
    "\n",
    "# Create dataloaders for Coldplay (use smaller stride for more samples from small data)\n",
    "context_length = 128  # Same as pretraining\n",
    "train_dataloader_ft = create_encoded_dataloader(\n",
    "    joined_train_lyrics,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=settings_ft[\"batch_size\"],\n",
    "    max_length=context_length,\n",
    "    stride=32,  # Smaller stride for overlapping windows to increase effective samples\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_dataloader_ft = create_encoded_dataloader(\n",
    "    joined_test_lyrics,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=settings_ft[\"batch_size\"],\n",
    "    max_length=context_length,\n",
    "    stride=32,\n",
    "    shuffle=False,\n",
    "    drop_last=False  # Don't drop last for small val set\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in fine-tune train dataloader: {len(train_dataloader_ft)}\")\n",
    "print(f\"Number of batches in fine-tune val dataloader: {len(val_dataloader_ft)}\")\n",
    "\n",
    "# Reinitialize optimizer and scheduler for fine-tuning (don't load pretrained optimizer)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=settings_ft[\"learning_rate\"],\n",
    "    weight_decay=settings_ft[\"weight_decay\"],\n",
    "    betas=(0.9, 0.95)\n",
    ")\n",
    "\n",
    "total_steps_ft = settings_ft[\"num_epochs\"] * len(train_dataloader_ft)\n",
    "scheduler = CosineWithWarmup(\n",
    "    optimizer,\n",
    "    warmup_steps=settings_ft[\"warmup_steps\"],\n",
    "    total_steps=total_steps_ft,\n",
    "    base_lr=settings_ft[\"max_lr\"],\n",
    "    min_lr=settings_ft[\"min_lr\"]\n",
    ")\n",
    "\n",
    "# Fine-tune using the same training loop\n",
    "train_losses_ft, val_losses_ft, tokens_seen_ft = train_model(\n",
    "    model,\n",
    "    train_dataloader_ft,\n",
    "    val_dataloader_ft,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    settings=settings_ft,\n",
    "    context_length=context_length,\n",
    "    save_path=\"checkpoints/gpt_512_512_8_8_finetuned_coldplay.pt\",\n",
    "    sample_prompt=\"Look at the star look how they  \"\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using the trained model and the generate_text function\n",
    "start_context = \"I want something\"\n",
    "num_chars_to_generate = 500 # You can adjust this number\n",
    "generated_tokens = generate(\n",
    "    model=model,\n",
    "    max_new_tokens=num_chars_to_generate,\n",
    "    context=text_to_token_ids(start_context, tokenizer, device),\n",
    "    context_length=context_length,\n",
    "    temperature=0.8, # Adjust temperature for creativity\n",
    "    top_k=None # Or specify a top_k value\n",
    ")\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "decoded_text = token_ids_to_text(generated_tokens, tokenizer)\n",
    "\n",
    "print(decoded_text.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea6343",
   "metadata": {},
   "source": [
    "Character Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c596d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "import os\n",
    "\n",
    "full_text = train_text_data + '\\n' + test_text_data\n",
    "\n",
    "characters = list(set(full_text))\n",
    "\n",
    "int_to_char = {}\n",
    "for i, char in enumerate(characters):\n",
    "    int_to_char[i] = char\n",
    "\n",
    "char_to_int = {}\n",
    "\n",
    "for value, char  in int_to_char.items():\n",
    "    char_to_int[char] = value\n",
    "\n",
    "print(int_to_char,char_to_int,end=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfdb83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "encoded_train = [char_to_int[c] for c in train_text_data if c in char_to_int]\n",
    "\n",
    "encoded_test = [char_to_int[c] for c in test_text_data if c in char_to_int]\n",
    "\n",
    "print(len(encoded_train),len(encoded_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac78b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# context_length=3\n",
    "# encoded= [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\n",
    "\n",
    "def create_dataset(encoded, context_length=128):\n",
    "  inputs, targets = [], []\n",
    "  for i in range(len(encoded) - context_length):\n",
    "    inputs.append(encoded[i:i+context_length])\n",
    "    targets.append(encoded[i+1:i+context_length+1])\n",
    "  return torch.tensor(inputs), torch.tensor(targets)\n",
    "\n",
    "train_inputs, train_targets = create_dataset(encoded_train)\n",
    "test_inputs, test_targets = create_dataset(encoded_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    logits = model(input_batch)  # [B, T, V]\n",
    "    B, T, V = logits.shape\n",
    "    loss = criterion(logits.view(B*T, V), target_batch.view(B*T))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    total_loss = 0.0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    model.eval()\n",
    "    for i, (inp, tgt) in enumerate(data_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        loss = calc_loss_batch(inp, tgt, model, device)\n",
    "        total_loss += loss.item()\n",
    "    model.train()\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter=1):\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "    val_loss   = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "# ---------- Cosine LR with Warmup ----------\n",
    "class CosineWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, base_lr, min_lr, last_epoch=-1):\n",
    "        self.warmup_steps = max(1, warmup_steps)\n",
    "        self.total_steps = max(self.warmup_steps+1, total_steps)\n",
    "        self.base_lr = base_lr\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch + 1\n",
    "        lrs = []\n",
    "        for _ in self.base_lrs:\n",
    "            if step <= self.warmup_steps:\n",
    "                lr = self.base_lr * step / self.warmup_steps\n",
    "            else:\n",
    "                progress = (step - self.warmup_steps) / max(1, self.total_steps - self.warmup_steps)\n",
    "                lr = self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + math.cos(math.pi * progress))\n",
    "            lrs.append(lr)\n",
    "        return lrs\n",
    "\n",
    "\n",
    "# ---------- Training Loop ----------\n",
    "def train_model_char(\n",
    "    model,\n",
    "    train_inputs,\n",
    "    train_targets,\n",
    "    test_inputs,\n",
    "    test_targets,\n",
    "    device,\n",
    "    settings,\n",
    "    context_length,\n",
    "    save_path=\"checkpoints/char_gpt.pt\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Character-level GPT training loop using FP32.\n",
    "    \"\"\"\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    vocab_size = model.vocab_size  # assume defined in model\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=settings[\"learning_rate\"],\n",
    "        weight_decay=settings[\"weight_decay\"],\n",
    "        betas=(0.9, 0.95)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(train_inputs, train_targets),\n",
    "        batch_size=settings[\"batch_size\"], shuffle=True, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        TensorDataset(test_inputs, test_targets),\n",
    "        batch_size=settings[\"batch_size\"], shuffle=True, pin_memory=True\n",
    "    )\n",
    "\n",
    "    total_steps = settings[\"num_epochs\"] * len(train_loader)\n",
    "    scheduler = CosineWithWarmup(\n",
    "        optimizer,\n",
    "        warmup_steps=settings[\"warmup_steps\"],\n",
    "        total_steps=total_steps,\n",
    "        base_lr=settings[\"max_lr\"],\n",
    "        min_lr=settings[\"min_lr\"]\n",
    "    )\n",
    "\n",
    "    train_losses, val_losses, tokens_seen_track = [], [], []\n",
    "\n",
    "    tokens_seen = 0\n",
    "    global_step = -1\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(settings[\"num_epochs\"]):\n",
    "        for step, (inp, tgt) in enumerate(train_loader):\n",
    "            loss = calc_loss_batch(inp, tgt, model, device)\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), settings[\"gradient_clip\"])\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "            tokens_seen += inp.numel()\n",
    "\n",
    "            # periodic evaluation\n",
    "            if (global_step % settings[\"eval_freq\"]) == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device,\n",
    "                    eval_iter=settings[\"eval_iter\"]\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                tokens_seen_track.append(tokens_seen)\n",
    "                lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "                print(f\"Ep {epoch+1} | step {global_step:06d} | lr {lr_now:.3e} \"\n",
    "                      f\"| train {train_loss:.3f} | val {val_loss:.3f}\")\n",
    "\n",
    "                # early stopping\n",
    "                if val_loss + settings[\"min_improvement\"] < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    os.makedirs(os.path.dirname(save_path) or \".\", exist_ok=True)\n",
    "                    torch.save({\"model_state\": model.state_dict(),\n",
    "                                \"optimizer_state\": optimizer.state_dict(),\n",
    "                                \"epoch\": epoch,\n",
    "                                \"global_step\": global_step}, save_path)\n",
    "                    print(f\"[Checkpoint saved at step {global_step}]\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= settings[\"patience\"]:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        return train_losses, val_losses, tokens_seen_track\n",
    "\n",
    "    return train_losses, val_losses, tokens_seen_track\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca6df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 83          # GPT-2 tokenizer vocab size\n",
    "context_length = 256                     # Reduced context length for faster training\n",
    "embed_dim = 512                        # Smaller embedding dimension\n",
    "attention_dim = 512                     # Keep same as embed_dim\n",
    "num_heads = 8                           # Divisible by attention_dim\n",
    "num_blocks = 8                          # Fewer blocks for smaller model\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# Initial context (starting token or BOS)\n",
    "context = torch.zeros(1, 1, dtype=torch.int64).to(device)\n",
    "\n",
    "model = GPT(num_heads,vocab_size,embed_dim,attention_dim,num_blocks,context_length, dropout_rate).to(device)\n",
    "#model.load_state_dict(torch.load(\"/content/trained_model-128-85.pth\", weights_only=False))\n",
    "\n",
    "train_losses, val_losses, tokens_seen_track = train_model_char(\n",
    "    model,\n",
    "    train_inputs,\n",
    "    train_targets,\n",
    "    test_inputs,\n",
    "    test_targets,\n",
    "    device,\n",
    "    settings,\n",
    "    settings[\"context_length\"],\n",
    "    save_path=\"checkpoints/char_gpt.pt\",\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model_path = \"trained_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "\n",
    "from google.colab import files\n",
    "files.download('trained_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c045dc",
   "metadata": {},
   "source": [
    "Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5029ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1, epochs+1), losses, marker='o')\n",
    "plt.title('Epoch vs Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(1, epochs+1), accuracies, marker='o', color='green')\n",
    "plt.title('Epoch vs Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e19cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 10\n",
    "eval_freq = 50  # Evaluate every 50 batches\n",
    "eval_iter = 5   # Evaluate on 5 batches for training and validation loss calculation\n",
    "learning_rate = 0.001\n",
    "start_context = \"The\" # Starting text for generation\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start training\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_dataloader, test_dataloader, optimizer, device,\n",
    "    num_epochs, eval_freq, eval_iter, start_context, tokenizer, context_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecdc729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GROK CODE\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Assuming tokenizer is defined as tiktoken.get_encoding(\"gpt2\")\n",
    "# and device is defined, e.g., device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model, train_dataloader, test_dataloader are already defined as per your code\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device, non_blocking=True)\n",
    "    target_batch = target_batch.to(device, non_blocking=True)\n",
    "    logits = model(input_batch)\n",
    "    loss = F.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "# Adjusted generate_text to use tokenizer for decoding\n",
    "def generate_text(model, new_chars, context, context_length, tokenizer, temperature=1.0, top_k=None):\n",
    "    generated_tokens = []\n",
    "    for _ in range(new_chars):\n",
    "        if context.shape[1] > context_length:\n",
    "            context = context[:, -context_length:]\n",
    "        logits = model(context)[:, -1, :]\n",
    "        logits = logits / max(temperature, 1e-3)\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "            raise ValueError(\"Logits contain NaN or Inf\")\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        probabilities = torch.clamp(probabilities, min=1e-9, max=1.0)\n",
    "        next_token = torch.multinomial(probabilities, 1)\n",
    "        context = torch.cat((context, next_token), dim=1)\n",
    "        generated_tokens.append(next_token.item())\n",
    "    return tokenizer.decode(generated_tokens)\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context = text_to_token_ids(start_context, tokenizer).to(device, non_blocking=True)\n",
    "    context_length = model.positional_embedding.num_embeddings\n",
    "    generated = generate_text(\n",
    "        model=model,\n",
    "        new_chars=50,\n",
    "        context=context,\n",
    "        context_length=context_length,\n",
    "        tokenizer=tokenizer,\n",
    "        temperature=0.5,\n",
    "        top_k=10\n",
    "    )\n",
    "    print(start_context + generated)\n",
    "    model.train()\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen = 0\n",
    "    global_step = -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "# Example usage:\n",
    "optimizer = AdamW(model.parameters(),\n",
    "        lr=settings[\"learning_rate\"],\n",
    "        weight_decay=settings[\"weight_decay\"],\n",
    "        betas=(0.9, 0.95))\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_dataloader, test_dataloader, optimizer, device,\n",
    "    num_epochs=10, eval_freq=100, eval_iter=5,\n",
    "    start_context=\"Every movie is a\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74818ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def debug_model_and_data(model, train_dataloader, tokenizer, device):\n",
    "    \"\"\"Comprehensive debugging of model and data\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"DEBUGGING MODEL AND DATA\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. Check data\n",
    "    print(\"\\n1. DATA ANALYSIS:\")\n",
    "    for batch_idx, (input_batch, target_batch) in enumerate(train_dataloader):\n",
    "        if batch_idx == 0:  # Check first batch\n",
    "            print(f\"Input shape: {input_batch.shape}\")\n",
    "            print(f\"Target shape: {target_batch.shape}\")\n",
    "            print(f\"Input sample: {input_batch[0][:20]}\")  # First 20 tokens\n",
    "            print(f\"Target sample: {target_batch[0][:20]}\")\n",
    "\n",
    "            # Decode to check if data makes sense\n",
    "            sample_text = tokenizer.decode(input_batch[0][:50].tolist())\n",
    "            print(f\"Decoded sample: '{sample_text}'\")\n",
    "\n",
    "            # Check for data issues\n",
    "            print(f\"Input min/max: {input_batch.min()}/{input_batch.max()}\")\n",
    "            print(f\"Vocab size: {tokenizer.n_vocab}\")\n",
    "            if input_batch.max() >= tokenizer.n_vocab:\n",
    "                print(\"❌ ERROR: Token IDs exceed vocab size!\")\n",
    "                return False\n",
    "            break\n",
    "\n",
    "    # 2. Check model architecture\n",
    "    print(\"\\n2. MODEL ARCHITECTURE:\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    # 3. Check model forward pass\n",
    "    print(\"\\n3. FORWARD PASS CHECK:\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_input = input_batch[:1].to(device, non_blocking=True)  # Single sample\n",
    "        try:\n",
    "            output = model(sample_input)\n",
    "            print(f\"Model output shape: {output.shape}\")\n",
    "            print(f\"Output range: {output.min():.4f} to {output.max():.4f}\")\n",
    "\n",
    "            # Check for NaN/Inf\n",
    "            if torch.isnan(output).any():\n",
    "                print(\"❌ ERROR: Model output contains NaN!\")\n",
    "                return False\n",
    "            if torch.isinf(output).any():\n",
    "                print(\"❌ ERROR: Model output contains Inf!\")\n",
    "                return False\n",
    "            print(\"✅ Forward pass successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR in forward pass: {e}\")\n",
    "            return False\n",
    "\n",
    "    # 4. Check loss calculation\n",
    "    print(\"\\n4. LOSS CALCULATION:\")\n",
    "    model.train()\n",
    "    sample_input, sample_target = input_batch[:1].to(device, non_blocking=True), target_batch[:1].to(device, non_blocking=True)\n",
    "\n",
    "    # Manual loss calculation\n",
    "    logits = model(sample_input)\n",
    "    loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), sample_target.view(-1))\n",
    "    print(f\"Sample loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Check what random guessing would give\n",
    "    random_loss = np.log(tokenizer.n_vocab)\n",
    "    print(f\"Random guessing loss: {random_loss:.4f}\")\n",
    "\n",
    "    if loss.item() > random_loss + 1:\n",
    "        print(\"⚠️  WARNING: Loss much higher than random guessing!\")\n",
    "        print(\"This suggests the model isn't learning properly\")\n",
    "\n",
    "    # 5. Check gradients\n",
    "    print(\"\\n5. GRADIENT CHECK:\")\n",
    "    loss.backward()\n",
    "\n",
    "    grad_norms = []\n",
    "    zero_grads = 0\n",
    "    total_grads = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            grad_norms.append(grad_norm)\n",
    "            if grad_norm == 0:\n",
    "                zero_grads += 1\n",
    "            total_grads += 1\n",
    "\n",
    "    if grad_norms:\n",
    "        print(f\"Average gradient norm: {np.mean(grad_norms):.6f}\")\n",
    "        print(f\"Max gradient norm: {np.max(grad_norms):.6f}\")\n",
    "        print(f\"Min gradient norm: {np.min(grad_norms):.6f}\")\n",
    "        print(f\"Zero gradients: {zero_grads}/{total_grads}\")\n",
    "\n",
    "        if np.mean(grad_norms) < 1e-6:\n",
    "            print(\"❌ ERROR: Gradients too small (vanishing gradient problem)\")\n",
    "            return False\n",
    "        elif np.max(grad_norms) > 100:\n",
    "            print(\"❌ ERROR: Gradients too large (exploding gradient problem)\")\n",
    "            return False\n",
    "\n",
    "    print(\"✅ All checks passed!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_success = debug_model_and_data(model, train_dataloader, tokenizer, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
